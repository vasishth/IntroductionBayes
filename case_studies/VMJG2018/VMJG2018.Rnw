\documentclass[man,floatsintext]{apa6}

\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[backend=biber,useprefix=true,style=apa,url=false,doi=false,sorting=nyt,eprint=false]{biblatex}

\usepackage{fancyvrb}

\usepackage{newfloat}
\DeclareFloatingEnvironment[
%    fileext=los,
%    listname=List of Schemes,
%    name=Listing,
%    placement=!htbp,
%    within=section,
]{listing}

\usepackage{lscape} % landscape table
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow} % multirows in tables 
\usepackage{bigdelim} % curly braces in table
\usepackage{xcolor,colortbl}

\usepackage{mathtools}
\makeatletter
 
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
 
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{microtype}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{geometry}
%\usepackage{lineno,clipboard}
%\newclipboard{reviews}
%\openclipboard{reviews}

%\newcommand{\revised}[1]{{\color{black}{#1}}}

\usepackage{tikz}

\usepackage{gb4e}


\DeclareLanguageMapping{american}{american-apa}
\addbibresource{StatSigFilter.bib}


\leftheader{Vasishth,Mertzen,J\"ager, Gelman}
\title{The statistical significance filter leads to overoptimistic expectations of replicability}
\shorttitle{The statistical significance filter}

\fourauthors{Shravan Vasishth}{Daniela Mertzen}{Lena A.\ J\"ager}{Andrew Gelman}



\fouraffiliations{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Statistics, Columbia University, New York, USA}


\authornote{Please send correspondence to vasishth@uni-potsdam.de.}
\note{\today}

\journal{Accepted in the Journal of Memory and Language} 
\volume{} 

\keywords{Type M error; replicability; surprisal; expectation; locality; Bayesian data analysis; parameter estimation}

\abstract{It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than $0.05$ leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.} 

\begin{document}

\maketitle

<<setup,include=FALSE,cache=FALSE,echo=FALSE>>=
library(knitr)
library(coda)
library(plyr)
library(ggplot2)
library(xtable)
library(dplyr)
library(SIN)
library(papaja)

library(rstan)
library(parallel)
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())

library(loo)
library(lme4)

library(rstantools)

opts_chunk$set(fig.path='figures/fig-', 
               fig.align='center', 
               fig.show='hold',warning=FALSE,include=FALSE,message=FALSE)
options(replace.assign=TRUE,show.signif.stars=FALSE)
options(replace.assign=TRUE,width=75)
#opts_chunk$set(dev='postscript')

library(brms)
@

<<loadfunctions, include=TRUE, echo=FALSE, warning=FALSE, error=TRUE, message=TRUE>>=
source("../R/createStanDat.R")
source("../R/createStanDatAcc.R")
source("../R/magnifytext.R")
source("../R/multiplot.R")
source("../R/plotresults.R")
source("../R/plotpredictions.R")
source("../R/stan_results.R")
source("../R/plotmeanSE.R")
@

\section{Introduction}


Imagine that a reading study shows a difference between two means that has an estimate of $77$ ms, with standard error 30, that is, with $p=0.01$. Now suppose instead that the same study had shown an estimate of $40$ ms, also with a standard error of 30; this time $p=0.18$. The usual reporting of these two types of results---either as significant and therefore ``reliable'' and publishable, or not significant and therefore either not publishable, or seen as showing that the null hypothesis is true---is misleading because it implies an inappropriate level of certainty in rejecting or accepting the null.  Indeed, it has been argued that this routine attribution of certainty to noisy data is a major contributor to the current replication crisis in psychology and other sciences \parencites{open2015estimating,amrhein2017earth}. For recent examples from psycholinguistics of replication difficulties, see \textcite{nieuwland2017limits}, and \textcite{kochari2018lexical}. The issue is not just the high frequency of failed replications, but also that these failed replications arise in an environment where routine success (defined as $p<0.05$) is expected.
We will refer to this $p<0.05$ decision criterion for publication-worthiness as the \textit{statistical significance filter}. We will demonstrate through direct replication attempts 
one well-known adverse consequence of the statistical significance filter \parencites{gelman2017failure,lane1978estimating},  that it leads to findings that are positively biased. We want to stress that none of the statistical points made in this paper are new \parencite[for similar arguments, see][among others]{hedges1984estimation,button2013power,dumas2017low,goodman1992comment,ioannidis2008most,frank2017collaborative}. However, we feel it is necessary to demonstrate through direct replication attempts why significance yields no useful information when statistical power is low. The fact that underpowered studies continue to be treated as informative suggests that such a demonstration is needed.

We assume here that the reader is familiar with the null hypothesis significance testing (NHST) procedure as it is used in psychology today. 
NHST can work well when power is relatively high.  But when power is low, published studies that show statistical significance will have exaggerated estimates (see Appendix~\ref{app:a} for a formal argument). The effect of low power is demonstrated in Figure~\ref{fig:typemdemo} using simulated data: for a low-power scenario, the estimates from repeated samples fluctuate wildly around the true value, and can also have the wrong sign. Whenever an effect is significant, it is necessarily an overestimate. \textcite{gelmancarlin} refer to these overestimates as Type M(agnitude) errors (when the sign of the effect is incorrect, Gelman and Carlin call this Type S(ign) error).  
These  overestimates occur because the standard error is relatively large in low-power situations; the wider the sampling distribution of the mean, the greater the probability of obtaining extreme values. 
By contrast, when power is high, the estimates under repeated sampling tend to be close to the true value because the standard error is relatively small.

\begin{figure}[!htbp]
\centering
<<demotypeM,echo=FALSE,fig.width=7,fig.height=7,include=TRUE>>=
set.seed(4321)
d<-15
sd<-100
lown<-power.t.test(d=d,sd=sd,power=.10,type="one.sample",alternative="two.sided",strict=TRUE)$n
highn<-power.t.test(d=d,sd=sd,power=.80,type="one.sample",alternative="two.sided",strict=TRUE)$n
nsim<-50
tlow<-thigh<-meanslow<-meanshigh<-CIuplow<-CIlwlow<-CIuphigh<-CIlwhigh<-NULL
critlow<-abs(qt(0.025,df=lown-1))
crithigh<-abs(qt(0.025,df=highn-1))

for(i in 1:nsim){
  x<-rnorm(lown,mean=d,sd=sd)
  meanslow[i]<-mean(x)
  tlow[i]<-t.test(x)$statistic
  CIuplow[i]<-mean(x)+critlow*sd(x)/sqrt(length(x))
  CIlwlow[i]<-mean(x)-critlow*sd(x)/sqrt(length(x))
  x<-rnorm(highn,mean=d,sd=sd)
  meanshigh[i]<-mean(x)
  thigh[i]<-t.test(x)$statistic
  CIuphigh[i]<-mean(x)+crithigh*sd(x)/sqrt(length(x))
  CIlwhigh[i]<-mean(x)-crithigh*sd(x)/sqrt(length(x))
}

 
siglow<-ifelse(abs(tlow)>abs(critlow),"p<0.05","p>0.05")
sighigh<-ifelse(abs(thigh)>abs(crithigh),"p<0.05","p>0.05")

summarylow<-data.frame(means=meanslow,significance=siglow, CIupper=CIuplow, CIlower=CIlwlow)
summaryhigh<-data.frame(index=1:nsim,means=meanshigh,significance=sighigh, CIupper=CIuphigh, CIlower=CIlwhigh)


# re-order data by mean effect size
summarylow<-summarylow[order(summarylow$means), ]
summarylow$index<-1:nrow(summarylow)
summaryhigh<-summaryhigh[order(summaryhigh$means), ]
summaryhigh$index<-1:nrow(summaryhigh)

p_low<-ggplot(summarylow, aes(y=means, x=index,
                              shape=significance,  ymax=CIupper, ymin=CIlower)) + 
  geom_pointrange()+
#  coord_flip()+
  geom_point(size=2.5)+
  scale_shape_manual(values=c(1, 17))+
  magnifytext(sze=22)+ 
  geom_hline(yintercept=15) +
  theme_bw() + 
    scale_x_continuous(name = "")+
  scale_y_continuous(name = "means",limits=c(-100,110))+
  labs(title="Effect 15 ms, SD 100, \n n=20, power=0.10")+
  theme(legend.position="none")+geom_hline(yintercept=0, linetype="dotted")

p_hi<-ggplot(summaryhigh, aes(y=means, x=index,
                              shape=significance, ymax=CIupper, ymin=CIlower)) + 
  geom_pointrange()+
#  coord_flip()+
  geom_point(size=2.5)+
  scale_shape_manual(values=c(1, 17))+
    scale_x_continuous(name = "Sample id")+
  magnifytext(sze=22)+ 
  geom_hline(yintercept=15) +
  theme_bw() + 
  scale_y_continuous(name = "means",limits=c(-100,110))+
  labs(title="Effect 15 ms, SD 100, \n n=350, power=0.80")+
  theme(legend.position=c(0.8,0.3))+geom_hline(yintercept=0, linetype="dotted")

multiplot(p_low,p_hi,cols=1)
@
\caption{A demonstration of Type M error using simulated data. We assume that the data are generated from a normal distribution with mean 15 ms and standard deviation 100.  The true mean is shown in each plot as a solid horizontal line. When power is low, under repeated sampling, whenever the estimates of an effect come out significant, the values are overestimates and can even have the wrong sign. When power is high, significant and non-significant effects will be tightly clustered near the true mean.}\label{fig:typemdemo}
\end{figure}

Figure~\ref{fig:typemdemo} illustrates another important point: when power is high, the estimates have much narrower 95\% confidence intervals. We will express this by saying that high-powered studies have higher \textit{precision} than low-powered studies. We borrow the term precision from Bayesian statistics, where it has a specific meaning: the inverse of the variance. Here, we are using the term precision to stand for the uncertainty about our estimate of interest (the sample mean, or a difference in sample means). This uncertainty is expressed in frequentist statistics in terms of the standard error of the sample mean. The standard error decreases as a function of the square root of the sample size; hence, if power is increased by increasing sample size, standard error will decrease.

Many researchers, such as \textcite{cohen1962statistical}, and 
\textcite{gelmancarlin}, have pointed out that a prospective power analysis  should be conducted before we run a study; after all, why would one want to spend money and time running an experiment where the probability of detecting an effect is 30\% or less? In medical statistics, prospective power analyses are quite common; not so in psycholinguistics. 
Suppose that we were to follow this practice from medical statistics and conduct a prospective power analysis based on the effect sizes reported in the literature. \textcite{gelmancarlin}, and many others before them, have pointed out that this can lead to an interesting problem.
Whenever an effect in an underpowered study comes out significant, it is \textit{necessarily} an overestimate.
In fields where power tends to be low, 
these overestimates will fill the literature. 
If we base the power analysis on  the published literature, we would conclude that the effects are large. A formal power analysis based on such  exaggerated estimates is bound to yield an overestimate of power, and we can incorrectly convince ourselves that we have an appropriately powered study. 

In psycholinguistics, usually we do no power analyses at all. We just rely on the informal observation that most of the previously published results had a significant 
effect. From this we conclude that the effect must be ``reliable,'' and therefore replicable. 

Although the above observations about power and replications are well-known in statistics and psychology \parencites[see the discussion in][]{pvals,chambers2017seven}, they are not widely appreciated in psycholinguistics. Our goal in this paper is to demonstrate---not via simulation but through actual replication attempts of a published empirical result---that relying exclusively on statistical significance to decide whether or not a result is newsworthy leads to misleading conclusions. 

We show through a case study that small-sample experiments can easily deliver statistically significant results that overestimate the true effect and are non-replicable. For this case study, we chose a paper by \textcite{levy2013expectation} that investigated expectation and locality effects in sentence comprehension. We selected this particular paper because there are no \textit{a priori} reasons to doubt the results in the paper, as they are theoretically well-founded and have plenty of independent empirical support. 

Anticipating our conclusions, we suggest that researchers and journals should avoid focusing exclusively on statistical significance to evaluate the validity and reliability of studies. Validity should be established by running as high-precision a study as possible (we explain this later in the paper); and reliability should be established through direct replication using pre-registration.

\section{Case study: The effects of expectation vs.\ memory retrieval in sentence processing}

\subsection{Background} 

\textcite{levy2013expectation} published two eyetracking studies in  the \textit{Journal of Memory and Language} in which they tested the   
predictions of two well-established theoretical proposals in sentence processing research: the expectation-based account \parencites{haleearley01, levy08} and the memory-based retrieval accounts \parencites{gibson98,gibson00,lewisvasishth:cogsci05}. 

The expectation-based account, as developed by \textcite{levy08}, predicts that intervening material between, for example, a subject and its verb, facilitates processing at the verb. To illustrate this point, consider the discussion by \textcite{levy08} of the following sentences from an eyetracking (reading) study conducted by \textcite{konieczny2003anticipation}.

\begin{exe}
\ex \label{ex:lars}
\begin{xlist}
\item
\gll Die Einsicht, dass [$_{NOM}$ der Freund] [$_{DAT}$ dem Kunden] [$_{ACC}$ das Auto aus Plastik] verkaufte,\dots\\
The insight, that {} the friend {} the client {} the car from plastic sold,\dots\\
\glt `The insight that the friend sold the client the plastic car\dots.' 
\item
\gll Die Einsicht, dass [$_{NOM}$ [der Freund] [$_{GEN}$ des Kunden]] [$_{ACC}$ das Auto aus Plastik] verkaufte,\dots\\
The insight, that {} the friend {} of~the client {} the car from plastic sold,\dots\\
\glt `The insight that the friend of the client sold  the plastic car\dots.' 
\end{xlist}
\end{exe}

Konieczny and D\"oring found that regression path durations at the verb \textit{verkaufte} in (\ref{ex:lars}a) were shorter than in (\ref{ex:lars}b) (555 vs.\ 793 ms). Levy's explanation for this facilitation is that the dative noun phrase (NP) in (\ref{ex:lars}a) sharpens the expectation for the verb to a greater degree than in (\ref{ex:lars}b): in the former,  nominative, accusative, and dative NPs narrow the range of possible upcoming verb phrases more than in the latter, where only  nominative and accusative NPs have been seen. 
Levy formalizes this idea in terms of surprisal \parencite{haleearley01}, which essentially states that the conditional probability of the verb phrase appearing given the preceding context determines processing difficulty: the more predictable the verb phrase, the easier it is to process. Using a probabilistic context-free grammar of German, Levy shows that syntactic surprisal is lower in 
(\ref{ex:lars}a) than (\ref{ex:lars}b) (23.51 vs.\ 23.91 bits); this suggests that surprisal may be a good explanation for the facilitation effect seen in \textcite{konieczny2003anticipation}.\footnote{A reviewer, Roger Levy, points out that these values are almost certainly overestimates of ``true'' comprehender surprisal for these cases, because the probabilistic context free grammar used for the calculations encodes much less information than human comprehenders would deploy.}

A competing class of theories of sentence processing difficulty makes the incorrect prediction for the reading time pattern observed at the verb in the Konieczny and D\"oring study. For example,  the Dependency Locality Theory or DLT \parencite{gibson00} assumes that processing difficulty (and therefore reading time) at a verb is a linear function of the distance between the verb and its arguments; distance here is measured in terms of the number of new discourse referents intervening between co-dependents. Under such an account, no difference is predicted between the two sentences above, because the same number of new discourse referents intervenes between the subject and verb in (\ref{ex:lars}a) and (\ref{ex:lars}b).
A closely related account is a computational model of cue-based retrieval \parencite{lewisvasishth:cogsci05,EngelmannJaegerVasishthSubmitted2018,NicenboimRetrieval2018}.  The Lewis \& Vasishth 2005 (LV05) model assumes that completing argument-verb dependencies is affected by similarity-based interference arising from distractor nouns in memory \parencite[a related model is by][]{vandykemcelree06}. Like the DLT, the LV05 model predicts that interposing nouns between the argument(s) and verb in grammatical sentences will increase processing difficulty at the verb. For the Konieczny and D\"oring data, this model also predicts no difference in processing difficulty between the two conditions (\ref{ex:lars}a) and (\ref{ex:lars}b). We will treat the DLT and the cue-based retrieval theories as specific instantiations of the memory-based account.

Levy and Keller (hereafter, LK) built on the work of Konieczny and D\"oring by developing a novel experimental design that cleverly pits  the expectation-based and memory-based accounts against each other. LK's studies are described next, as they form the basis for our replication attempts.

\subsection{The experiment design by Levy and Keller (2013)}

As shown in Table \ref{itemsE1LK},
in their sentences for their Experiment 1, a dative NP and a prepositional adjunct either appeared in a subordinate clause or a main clause.  
The critical region in this experiment was the verb \textit{versteckt}; the post-critical region was defined as the two words following the matrix verb (\textit{und somit}, `and thus,'  in the example shown in Table \ref{itemsE1LK}).

Their Experiment 2 had a design similar to Experiment 1, with one difference: syntactic complexity was increased by embedding the main clause of Experiment 1 within a relative clause (see Table \ref{itemsE2LK}). 
Here, the critical region was the head verb of the relative clause and the auxiliary (\textit{versteckt hat}, `hidden had', in Table \ref{itemsE2LK}) and the post-critical region was the noun phrase (here, \textit{die Sache}, `the affair'). Note that the two experiments take advantage of the head-final property of German: the verb always appears clause-finally in these constructions. Since all the arguments precede the verb, it is easy to investigate the effect of verb predictability conditional on having seen all the arguments.

\subsection{Predictions for the LK study} \label{LKpredictions}

LK lay out the predictions of 
the expectation-based account as follows \parencite{levy2013expectation}:

\begin{quote}
\dots [condition (a)] (neither dative nor adjunct in the main clause) should be hardest to process, while [condition (d)] should be easiest (both dative and adjunct in the main clause). [Conditions (b) and (c)] should be in between (one phrase in the main clause).  (p.\ 202)
\end{quote}

The reasoning behind these predictions is that interposing material  sharpens the expectation for a participial verb. For a graphical summary of the predictions, see Figure~\ref{pred}, left panel; this figure is a reproduction of LK's Figure 1. As mentioned above, \textcite{levy08} and others refer to such predicted speedups as expectation effects.\footnote{LK showed in a corpus analysis  \parencite[summarized in their Table 1][p.\ 204]{levy2013expectation} that if the dative NP or both the dative NP and the adjunct phrase appeared in the main clause, the main clause verb phrase (the verb phrase that the verb \textit{versteckt}, `hidden') heads had lower surprisal values. Thus, according to the corpus analysis, conditions (a) and (c) would be predicted to be read slower than conditions (b) and (d). In the present paper, we follow the predictions laid out in LK's Figure 1.}

The memory-based account makes different predictions. Because intervening discourse referents between the subject and the verb should generally lead to greater processing difficulty, placing the dative NP or the adjunct in the main clause should lead to a slowdown at the verb, and placing both the dative NP and the adjunct in the main clause should lead to an even greater slowdown at the verb.
This means that reading time at the critical verb in condition (b) should be slower than (a), and condition (d) should be slower than (c); in fact, (d) should show the greatest slowdown in reading time, because it is associated with the highest processing cost (see Figure \ref{pred}, right panel). \textcite{gibson00} and others often refer to these slowdowns as  locality effects.

One nice property of the LK design is that the verb position is always constant across conditions being compared: the intervening phrases (dative NP and adjunct) always appear in the sentence, either intervening between the subject and verb or at the beginning of the sentence. This resolves a potentially serious confound in such studies; many of the previous studies \parencites{lars00,grodner,vasishthlewisLanguage05} had the verb further downstream in the sentence whenever an additional intervener was present. This positional confound makes comparisons across conditions difficult to interpret: if a verb appears later in the sentence, this alone may lead to slowdowns or speedups compared to a baseline condition \parencite[for discussion, see][]{fh93}.


\begin{figure}[!htbp]
  \centering
<<LKpredictions,cache=FALSE,echo=FALSE,include=TRUE,fig.width=8,fig.height=4>>=
LKsurprisal <- data.frame(
  pred = c(700,600,600,500),
  cond = c("a", "b", "c", "d")
)

LKmemory <- data.frame(
  pred = c(500,600,600,700),
  cond = c("a", "b", "c", "d")
)

plot1<-plotpredictions(dat=LKsurprisal,maintitle="Predictions of \n the expectation-based account")
plot2<-plotpredictions(dat=LKmemory,maintitle="Predictions of \n the memory account",ylabel="")
multiplot(plot1,plot2,cols=2)
@
\caption{\label{pred} Predictions for the Levy and Keller Experiments 1 and 2: The left panel shows the speedup  predicted by the expectation account. The  right panel shows the slowdown predicted by memory-based accounts. This figure is based on Figure 1 of Levy and Keller (2013).}
\end{figure}

\doublespacing
\begin{table}[htbp]
\caption{Example sentences for LK's Experiment 1 (simplified). The abbreviations mean the following: ADJ: adjunct; DAT: dative; PP: prepositional phrase; NP: noun phrase.}
\setlength{\extrarowheight}{2pt}
{\scriptsize
\setlength\tabcolsep{1.5pt}
\begin{tabular}{l l l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}[3pt][0pt]}l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}[3pt][0pt]}l l l >{\columncolor[gray]{.9}}l l l l l l l l l}
\multicolumn{11}{l}{\textbf{a.  PP adjunct in subordinate clause, dative NP in subordinate clause}}\\
Nachdem & der & Lehrer & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & \dots,\\ 
 \textit{After} & \textit{the} & \textit{teacher} & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} &  &  &  &  & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}}, & und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}} & \textcolor{black}{\textit{Gerstner}} &  &  &  &  & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & \textit{and} & \textit{thus}\dots\\
\multicolumn{11}{l}{\textbf{b. PP adjunct in main clause, dative NP in subordinate clause}}\\
Nachdem & der & Lehrer & & & [\textbf{DAT} dem & Sohn] & \dots,\\ 
\textit{After} & \textit{the} & \textit{teacher} & & & [\textbf{DAT} \textit{the} & \textit{son}] & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} & [\textbf{ADJ} zur & Ahndung] &  &   & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}}, & und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}} & \textcolor{black}{\textit{Gerstner}}& [\textbf{ADJ} \textit{as} & \textit{payback}] &  &  & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & \textit{and} & \textit{thus}\dots\\
\multicolumn{11}{l}{\textbf{c.  PP adjunct in subordinate clause, dative NP in main clause}}\\
Nachdem & der & Lehrer & [\textbf{ADJ} zur & Ahndung] & & & \dots,\\ 
\textit{After} & \textit{the} & \textit{teacher} & [\textbf{ADJ} \textit{as} & \textit{payback}] &  & & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} &     & & [\textbf{DAT} dem & Sohn] &  den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}},& und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}}& \textcolor{black}{\textit{Gerstner}} &   & & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & \textit{and} & \textit{thus}\dots\\
\multicolumn{11}{l}{\textbf{d. PP adjunct in main clause, dative NP in main clause}}\\
Nachdem & der & Lehrer & &  &  & & \dots,\\ 
 \textit{After} & \textit{the} & \textit{teacher} &  &  & & & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}}, & und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}}& \textcolor{black}{\textit{Gerstner}} & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & \textit{and} & \textit{thus}\dots
\end{tabular}}
\label{itemsE1LK}
\begin{footnotesize}
\bigskip
\begin{tablenotes}
\item \textit{`After the teacher imposed detention classes, Hans Gerstner hid the football from the naughty son of the industrious janitor as additional payback for the multiple wrongdoings, and thus corrected the affair.'}
\end{tablenotes}
\end{footnotesize}

\bigskip
\bigskip

\noindent
\caption{Example sentences for LK's Experiment 2 (simplified). The abbreviations mean the following: ADJ: adjunct; DAT: dative; PP: prepositional phrase; NP: noun phrase.}
{\scriptsize
\setlength\tabcolsep{1.5pt}
\begin{tabular}{l l l l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}[3pt][0pt]}l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}[3pt][0pt]}l l l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}}l l l l l l l l l l l}
\multicolumn{11}{l}{\textbf{a.  PP adjunct in subordinate clause, dative NP in subordinate clause}}\\
Nachdem & der & Lehrer & & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & \dots,\\
 \textit{After} & \textit{the} & \textit{teacher} & & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{der} &  \textcolor{black}{Mitsch\"uler}, & der  &  &  & &  & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}}, & die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}} & \textcolor{black}{\textit{classmate}}, & \textit{who}  &  &  & & & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots\\
\multicolumn{11}{l}{\textbf{b. PP adjunct in relative clause, dative NP in subordinate clause}}\\
Nachdem & der & Lehrer & & & & [\textbf{DAT} dem & Sohn] & \dots,\\
\textit{After} & \textit{the} & \textit{teacher} & & & & [\textbf{DAT} \textit{the} & \textit{son}] & \dots, \\ \addlinespace[2pt]
hat & \textcolor{black}{der} & \textcolor{black}{Mitsch\"uler}, & der &[\textbf{ADJ} zur & Ahndung] & &  & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}}, & die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}}  & \textcolor{black}{\textit{classmate}}, & \textit{who} & [\textbf{ADJ} \textit{as} & \textit{payback}] & & & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots\\
\multicolumn{11}{l}{\textbf{c.  PP adjunct in subordinate clause, dative NP in relative clause}}\\
Nachdem & der & Lehrer & & [\textbf{ADJ} zur & Ahndung] & & & \dots,\\
\textit{After} & \textit{the} & \textit{teacher} & & [\textbf{ADJ} \textit{as} & \textit{payback}] &  & & \dots, \\ \addlinespace[2pt]
hat & \textcolor{black}{der} & \textcolor{black}{Mitsch\"uler}, & der &    & & [\textbf{DAT} dem & Sohn] &  den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}},& die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}} & \textcolor{black}{\textit{classmate}}, & \textit{who} & & & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots\\
\multicolumn{11}{l}{\textbf{d. PP adjunct in relative clause, dative NP in relative clause}}\\
Nachdem & der & Lehrer & & &  & & & \dots,\\
 \textit{After} & \textit{the} & \textit{teacher} & &  & & & & \dots, \\ \addlinespace[2pt]
 hat & \textcolor{black}{der} & \textcolor{black}{Mitsch\"uler}, & der & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}}, & die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}} & \textcolor{black}{\textit{classmate}},  & \textit{who} & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots
\end{tabular}}
\label{itemsE2LK}
\begin{footnotesize}
\bigskip
\begin{tablenotes}
\item \textit{`After the teacher imposed detention classes, the classmate who hid the football from the naughty son of the industrious janitor as additional payback for the multiple wrongdoings corrected the affair.'}
\end{tablenotes}
\end{footnotesize}
\end{table}


\restoregeometry

\subsection{A re-analysis of the LK data}

The two studies by LK had 28 participants  and 24 items each. 
In their paper, statistical summaries and analyses for the critical and post-critical regions were prepared using the \texttt{lme4} package \parencite{lme4new} in R \parencite{R}. 
They released their data to us, which allowed us to carry out the same analyses as they did, but within a Bayesian framework \parencite{Gelman14} using the probabilistic programming language Stan \parencite{stan:2017}. Below, we explain our reasons for using the Bayesian data-analytic approach. Briefly, our main interest here is in quantifying uncertainty about the parameter estimates of interest. We elaborate on this point next.

\subsubsection{Motivation for using Bayesian data analysis} \label{motivation}

In the Bayesian framework, all the  parameters in the model, which can be represented as a vector $\theta$,  are assumed to have some prior distribution of plausible values, $p(\theta)$. Given the prior, and a likelihood function for the data $p(data\mid \theta)$, Bayes' rule is used to compute the posterior distribution of the parameters: $p(\theta\mid data)$. 
Bayes' rule states that the posterior is proportional to the prior multiplied by the likelihood: $p(\theta\mid data) \propto p(\theta) p(data\mid \theta)$.
Thus, the application of Bayes' rule furnishes a posterior distribution representing plausible values of a parameter given the data and model (the model subsumes the likelihood function and the priors). Technically, this cannot be done with the frequentist approach, where each parameter is assumed to be an unknown point value. Such a point value may represent an invariant number in some fields (e.g., in physics, the speed of light in a vacuum), but is a fictional construct in areas like psychology and psycholinguistics. For example, there exists no single number representing the increase in reading time in object vs.\ subject relatives in English. 
The Bayesian approach allows us to focus on  the uncertainty of the estimates of interest.
A further, although more peripheral, advantage is that we can always fit so-called ``maximal'' models with full covariance matrices for by-participant and by-item variance components \parencite{schielzeth2008conclusions,barr2013}. Such maximal models often fail to converge in \texttt{lme4} for small data sets and yield unrealistic estimates of the variance components \parencite[see][for an example]{VasishthBeckmanetal}. Fitting a maximal model has the advantage that we can make the most conservative possible claim about the parameters given the data and model. The reason that Bayesian methods allow us to fit essentially arbitrarily complex random effects variance components is the involvement of prior information in the model. We discuss priors next. 

\paragraph{Prior specification in Bayesian models}

In the Bayesian approach, it is common to use so-called mildly and weakly informative priors that have a regularizing effect on the posteriors.\footnote{Ideally, one should use a ``community of priors'' to conduct an analysis, so that all opinions on a topic are taken into account. This approach is used sometimes in areas like medicine \parencite{spiegelhalter2004bayesian}.} A weakly informative prior allows a wide range of plausible values; regularizing means that we downweight extreme parameter values that are a priori unlikely to occur. A simple example is a prior on correlations or correlation matrices; Stan allows us to define a so-called LKJ prior \parencite{lewandowski2009generating} on even large correlation matrices such that the prior downweights $-1$ and $+1$ as possible values. This is illustrated in Figure~\ref{fig:prior}. When the nu ($\nu$) parameter in the built-in Stan function for an  LKJ prior  is less than 1, intermediate values are downweighted; such a situation is the opposite of what we mean here by regularizing priors. When $\nu$ is higher than $1$, extreme values are downweighted. 
Regularizing priors are also defined for all other parameters in the model. For detailed tutorials specifically intended for psycholinguistics, see \textcite{VasishthBeckmanetal,NicenboimVasishthStatMeth,SorensenVasishthTutorial}. More general introductory book-length treatments suitable for psychologists and psycholinguists are \textcite{kruschke2014doing} and \textcite{mcelreath2016statistical}. An advanced treatment is in \textcite{Gelman14}.  

<<lkjvisual,cache=TRUE,echo=FALSE,include=FALSE>>=
fake_data <- list(x = rnorm(30000,0,1),N = 30000, R = 2) 

stancode <- "
data {
  int<lower=0> N; 
  real x[N]; 
  int R;
  }
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  x ~ normal(mu,sigma);  
}
generated quantities {
  corr_matrix[R] LKJ05;
  corr_matrix[R] LKJ1;
  corr_matrix[R] LKJ2;
  corr_matrix[R] LKJ4;
  LKJ05 = lkj_corr_rng(R,.1);
  LKJ1 = lkj_corr_rng(R,.5);
  LKJ2 = lkj_corr_rng(R,1);
  LKJ4 = lkj_corr_rng(R,2);
}
"

fitfake <- stan(model_code = stancode, pars = c("LKJ05","LKJ1","LKJ2","LKJ4"),
                data = fake_data, chains = 4, 
                iter = 2000)

corrs<-extract(fitfake,pars=c("LKJ05[1,2]","LKJ1[1,2]","LKJ2[1,2]","LKJ4[1,2]"))
corrs<-data.frame(corrs)
colnames(corrs)<-c("lkj05","lkj1","lkj2","lkj4")
@

\begin{figure}[!htbp]
\centering
<<figpriors,echo=FALSE,include=TRUE,warning=FALSE,message=FALSE,fig.width=6,fig.height=4>>=

lkjplot1<-ggplot(corrs, aes(lkj05)) +
  geom_density(adjust=2)+xlab(expression(rho))+ggtitle("nu=0.1")+theme_bw()
lkjplot2<-ggplot(corrs, aes(lkj1)) +
  geom_density(adjust=3)+xlab(expression(rho))+ggtitle(expression(nu==0.5))+theme_bw()+ylim(0, 0.6)
lkjplot3<-ggplot(corrs, aes(lkj2)) +
  geom_density(adjust=3)+xlab(expression(rho))+ggtitle("nu=1")+theme_bw()
lkjplot4<-ggplot(corrs, aes(lkj2)) +
  geom_density(adjust=3)+xlab(expression(rho))+ggtitle(expression(nu==2))+theme_bw()+ylim(0, 0.6)
multiplot(lkjplot2,lkjplot4,cols=2)
@
\caption{Example showing two different prior distributions using LKJ priors on a correlation parameter $\rho$. When the $\nu$  (nu) parameter in the LKJ function is 2, this downweights extreme values such as $\pm 1$. The LKJ(2) prior can be used to define priors for arbitrarily large correlation matrices, not just for a single correlation parameter.}\label{fig:prior}
\end{figure}

Throughout this paper, we will summarize the posterior distributions with their mean and the 95\% credible interval.\footnote{\textcite{kruschke2014doing} uses highest posterior density intervals. As \textcite[][p.\ 87]{kruschke2014doing} puts it: ``the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say 95\% of it, such that every point inside the interval has higher credibility than any point outside the interval.''  This interval is identical to the credible interval when the posterior distribution is symmetric about its mean. When the posterior is asymmetric, the HPDI and the credible interval will have a large overlap, but the lower and upper end-points will differ.} This equal-tailed interval demarcates the range over which we are 95\% certain (given the data and the model) that the true parameter lies.
The credible interval therefore allows us to do something that a frequentist confidence interval cannot: quantify our uncertainty about the parameter of interest. The frequentist confidence interval cannot quantify uncertainty about the estimate of interest for two reasons. First, in the NHST way of thinking, a parameter in the frequentist paradigm is an unknown point value. Once one assumes that a parameter can only have a fixed but unknown point value, the parameter cannot have a probability distribution associated with it. All estimation in the frequentist paradigm is done with reference to the sampling distribution of the estimator, $\hat\mu$, which is a function that gives us the sample mean for a particular data set. The estimate of the standard error of the sample mean (SE) is a function of the estimated standard deviation $\hat\sigma$ and the sample size $n$: $SE=\hat\sigma/\sqrt{n}$, is quantifying the uncertainty of the distribution of the sample mean returned by the estimator $\hat\mu$ under hypothetical repeated sampling.
Given a data set consisting of a vector of data points $x$, the sample mean $\bar{x}$ serves as an estimate of the unknown point value $\mu$.  The confidence interval is then computed as $\bar{x} \pm 2\times SE$. 
Thus, a \textit{particular} 95\% confidence interval from a single data set either contains the true, unknown $\mu$ or it doesn't. Second, the meaning of  the confidence interval is so convoluted that it is difficult to understand or communicate: if one were to---counterfactually---repeat the same experiment multiple times and compute a 95\% confidence interval each time, then 95\% of those \textit{hypothetical} confidence intervals would contain the true parameter $\mu$. No probability statement can be made from any single confidence interval anyway. For further discussion of confidence intervals, see \textcite{hoekstra2014robust,kruschke2014doing}. In typical psycholinguistic experiments, the confidence interval and the Bayesian credible interval will look very similar \parencite[for examples, see][]{BatesEtAlParsimonious}; in some cases, the Bayesian interval may be slightly wider than the confidence interval. As a consequence of this rough equivalence, the confidence interval is often interpreted as if it were a Bayesian credible interval, even though this is technically incorrect. In the past, due to limitations of software for carrying out Bayesian analyses, the confidence interval was much easier to compute than the Bayesian credible interval, so equating Bayesian and frequentist intervals was arguably a reasonable approximation. However, today, packages like \texttt{brms} \parencites{buerknerbrms1,buerknerbrms2} make Bayesian linear mixed models relatively easy to fit, and so it is now quite straightforward to compute Bayesian credible intervals.


\paragraph{Statistical methodology}\label{statmeth}
As in the original study, we investigated the main effects of dative position (Dative) and adjunct position (Adjunct) and their interaction, using the same contrast coding that LK employed. Their contrast coding is shown in Table~\ref{tab:meintcontrasts}. A positive coefficient for the main effect of Dative or Adjunct means that a speedup in reading time is seen when the dative NP (respectively, the adjunct) appears within the main clause (Expt 1) or relative clause (Expt 2), i.e., when it is interposed between the grammatical subject and the verb.  

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{lrrrr}
Condition &                  Dat &  Adj &  DatxAdj\\
 a \dots [ Subj \dots ~~~~~~~~~~~~~~\dots Verb]  &  0.5    &     0.5    &   0.5\\    
 b \dots  [ Subj \dots~~~~~~ ADJ \dots Verb] &  0.5    &    -0.5    &  -0.5\\
 c \dots  [ Subj \dots DAT~~~~~~~ \dots Verb]  & -0.5    &     0.5    &   -0.5\\
 d \dots [ Subj \dots DAT ADJ \dots Verb]  & -0.5    &    -0.5    &    0.5\\
\end{tabular}
\end{center}
\caption{The contrast coding used by Levy and Keller (2013) for main effects of Dat(ive), Adj(unct), and their interaction for the two experiments. The structures used in the four conditions are shown schematically; note that the verb was always in the same position because the interveners (Dat and Adj) either intervened between the subject and the verb, or appeared before the subject.  
In Experiment 1, the subject-verb dependency was in the main clause, and in Experiment 2, it was within a relative clause.}\label{tab:meintcontrasts}
\end{table}%

The reading times were log-transformed and a hierarchical (linear mixed) model was fit with full covariance matrices for participants and for items \parencite[the ``maximal'' model recommended by][]{barr2013}. All the code and data are available from https://osf.io/eyphj/. Because a reviewer requested it, all models were also refit using raw reading times with \texttt{lme4}. The results do not change depending on whether one log-transforms or not. In all the Stan models, regularizing, weakly informative priors \parencite{Gelman14} were used for all parameters and hyperparameters. For all parameters (except the parameters of the correlation matrix for the random effects), the prior distribution was defined as the standard normal distribution,  $Normal(0,1)$; 
for variance components these were truncated at 0 (because standard deviations cannot be less than 0). The posteriors are not dependent on these specific priors; other choices (such as a Cauchy prior) lead to similar posterior distributions.
For the correlation parameters in the variance-covariance matrix of the random effects, we defined  regularizing LKJ priors on the correlation matrix \parencite{stan-manual:2016}.  
For each model, we ran four chains with 2000 iterations each. The first half of these were warm-up samples and were discarded. Convergence was checked by visually inspecting the chains and via the R-hat convergence diagnostic \parencite{Gelman14}.

The posterior distributions for the main effects and interaction on the log scale were back-transformed to posterior distributions in milliseconds. This was done as follows. Suppose that the fixed effects part of the model is defined as:


\begin{equation} 
\log (rt) = \beta_0 + \beta_1 Dative + \beta_2 Adjunct + \beta_3 Dative \times Adjunct 
\end{equation}

\noindent
with the effects coded as $\pm 0.5$. Then, we can obtain the posterior distribution of the difference in means between the two levels of Dative by computing:
$exp(\beta_0 + \beta_1\times 0.5) - exp(\beta_0 - \beta_1\times 0.5)$. This computation is done within Stan, taking as input each of the posterior samples of  $\beta_0,\dots \beta_3$, and returning as output the posterior distribution of the difference in means on the raw ms scale. This transformation of the posterior distributions from the log scale to the ms scale allows us to compute credible intervals on the raw scale as well.
Analogous calculations were done for the other factors.

\paragraph{Question-response accuracy in the LK data}

Half of the 24 items were followed by comprehension questions that had yes/no responses. Accuracy on the target items was 69\% in Experiment 1 and 65\% in Experiment 2 (personal communication from Frank Keller).      
% email from Keller: 4/28/16
%Items  Fillers
%Exp1  69.4   97.5
%Exp2  65.0   94.0

\paragraph{Reading time results in the LK data}
It is standard in eyetracking reading research to argue for an effect if just \textit{any} of several dependent measures examined show an effect. For example, \textcite{konieczny2003anticipation} found their effect only in regression path durations. In the LK studies, which take as a starting point the Konieczny and D\"oring design, regression path duration showed no effect at all; instead, other measures showed statistically significant effects. We avoid this approach and instead try to reproduce the effect in one dependent measure that LK would consider representative of their claims.
The LK paper presents a graphical summary of their effects using total reading times for the  two experiments; see LK's  Figures 3 and 4 \parencite[][pp.\ 209, 214]{levy2013expectation}. Because the graphical summary using total reading times was considered by LK to be a representative summary of their overall claims, below we only report the analyses involving total reading times.\footnote{We attempted to obtain the Konieczny and D\"oring estimates for total reading time in order to compare them with the LK estimates, but were unsuccessful.} 

<<nonsigETdepmeasuresLKE2Replication,include=FALSE,eval=FALSE,echo=FALSE,message=FALSE,warning=FALSE>>=
# Eyetracking Replication of LK Experiment 2
# regression probability, skipping probability

# data ET Replication of LK Expt 2, reload to create a standalone chunk:
dat<-read.table("../data/E4ETlevykellerExp2.txt",header=T)
# subset data
dat<-subset(dat,condition!="f" & condition!="p")


# add column skipping probability (1 if word skipped, 0 otherwise)
dat$SKP <- ifelse(dat$TFT>0,0,1)
# add column regression probability (RBRC is no of regression from word n before continuing to the right.
# 1 if regression occured, 0 otherwise.)
dat$FPRP <- as.logical(dat$RBRC)

# contrast coding 
dat$dat<-ifelse(dat$condition%in%c("a","b"),1/2,-1/2)
dat$adj<-ifelse(dat$condition%in%c("b","d"),-1/2,1/2)
dat$int<-ifelse(dat$condition%in%c("b","c"),-1/2,1/2)

dat$roi<-factor(dat$roi)

region<-ifelse(dat$roi==23,"npacc",
               ifelse(dat$roi==25,"verb",
                      ifelse(dat$roi==27,"verb1","noncritical")))

dat$region<-region
dat<-subset(dat,region!="noncritical")
dat$region<-factor(dat$region)

# subset critical and postcritical region
verb <-(subset(dat,region=="verb"))
verb1 <-(subset(dat,region=="verb1"))

## FPRP (first-pass regression prob) crit
#summary(mFPRP <- glmer(FPRP~dat+adj+int+(1|subject)+(1|itemid), verb, family=binomial))

## Skip prob crit
#summary(mSKP <- glmer(SKP~dat+adj+int+(1|subject)+(1|itemid), verb, family=binomial))


# FPRP crit
subj <- as.integer(factor(verb$subject))
N_subj <- length(unique(subj))
item <- as.integer(factor(verb$itemid))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + dat + adj + int, verb))  
attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                prob = verb$FPRP,                  
                subj = subj,  
                item = item)  


# 3. Fit the model.

mFPRPStan <- stan(file = "StanModels/logitmaximal.stan", 
               data = stanDat,
               iter = 2000, 
               chains = 4)


# FPRP postcrit

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                prob = verb1$FPRP,                  
                subj = subj,  
                item = item)  


# 3. Fit the model.

mFPRPpost <- stan(file = "StanModels/logitmaximal.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)



# Skipping prob crit

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                prob = verb$SKP,                  
                subj = subj,  
                item = item)  


# 3. Fit the model.

mSKPStan <- stan(file = "StanModels/logitmaximal.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)




# Skipping prob postcrit 

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                prob = verb1$SKP,                  
                subj = subj,  
                item = item)  


# 3. Fit the model.

mSKPpost <- stan(file = "StanModels/logitmaximal.stan", 
                 data = stanDat,
                 iter = 2000, 
                 chains = 4,
                 control = list(adapt_delta=0.99))


@

Limiting the dependent measure to total reading times (in both our reanalyses of LK's original studies, and in the analyses of our replication attempts) had a second motivation:  Analyzing multiple dependent measures greatly increases Type I error probability \parencite{MalsburgAngele2016}. For example, LK analyzed eight dependent measures in two regions of interest. Thus, for each experiment, $16$ models were fit, so for each of the three predictors (the effect of Dat(ive), Adj(unct), and their interaction, DatxAdj) a total of $32$ statistical tests were conducted for both experiments combined.  
Assuming that a p-value less than $0.05$ is a 
statistically significant outcome,
Dative showed six significant effects, 
Adjunct showed one significant effect, and the interaction showed eight significant effects.
Because of the inflated probability of incorrectly rejecting the null when multiple dependent measures are analyzed, it is vitally important to correct Type I error probability, e.g., via the  Bonferroni correction, to compensate for the inflated false positive rate  \parencite{MalsburgAngele2016}. 

<<originalLKdataExp1, include=FALSE, cache=FALSE,echo=FALSE,warning=FALSE>>=

##### ORIGINAL LEVY KELLER DATA EXPERIMENT 1 #####
# CRITICAL REGION (REGION 7)  #

reading_time <- read.table('../data/OrigLevyKellerData/prediction_experiment_data/experiment1/lmr/results/exp1_tt_r.res', header=TRUE)
#head(reading_time)

condition<-ifelse(reading_time$dat=="sub" & reading_time$adj=="sub","a",
                  ifelse(reading_time$dat=="sub" & reading_time$adj=="main","b",
                         ifelse(reading_time$dat=="main" & reading_time$adj=="sub","c", 
                                ifelse(reading_time$dat=="main" & reading_time$adj=="main","d","NA"))))

reading_time$condition<-factor(condition)


# contrast coding: 
reading_time$dat<-ifelse(reading_time$condition%in%c("a","b"),1/2,-1/2)
reading_time$adj<-ifelse(reading_time$condition%in%c("b","d"),-1/2,1/2)
reading_time$int<-ifelse(reading_time$condition%in%c("b","c"),-1/2,1/2)

                 ## ME DAT ## ME PP-ADJ ## INT
# a DAT-SC; PP-SC    0.5         0.5       0.5    
# b DAT-SC; PP-MC    0.5        -0.5      -0.5
# c DAT-MC; PP-SC   -0.5         0.5      -0.5
# d DAT-MC; PP-MC   -0.5        -0.5       0.5

# Thus, ME positive coefficient= longer RTs/slowdown when DAT bzw. PP ADJ in subordinate clause; 
# negative coefficient= faster RTs/speed-up when DAT bzw. PP ADJ in subordinate clause. 
# Interaction positve coefficient= longer RTs/slowdown when DAT/PP ADJ are in the same - subordinate OR main - clause. 

# remove zeros
reading_time_nozeros <- reading_time[reading_time$region7 != 0,]


# model (log tft) at the critical region (orgininally raw rts, see residuals)
#mLKE1crit  <- lmer(log(region7) ~ dat*adj + (dat*adj|subj) + (dat*adj|item), data=reading_time_nozeros)
#summary(mLKE1crit)

# I subset the data frame here (LK1 conds c and d only, such that I can bind it with conditions c and d from LK2).

dataLK1cd<-subset(reading_time, condition!="a" & condition!="b")
@




<<AnalysisLK1critical,results='hide',include=FALSE,cache=TRUE,echo=FALSE,warning=FALSE>>=


# ANALYSIS of TRT original LK1 
# CRITICAL REGION (main verb)
stanDatLKE1<-createStanDat(d=reading_time_nozeros,
              rt=reading_time_nozeros$region7,
              form=as.formula("~1+dat+adj+int"))

LKE1 <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatLKE1,
                    iter = 2000, 
                    chains = 4)

LKE1_res<-stan_results(m=LKE1,params=c("Dat","Adj","DatxAdj"))

## original:
#interact  <- lmer(region7 ~ dat+adj+int + (dat+adj+int|subj) + (dat+adj+int|item), data=reading_time_nozeros)
#summary(interact)
@


<<AnalysisLK1postcritical,include=FALSE,cache=TRUE,echo=FALSE,warning=FALSE>>=
# ANALYSIS of TRT original LK1 
# POSTCRITICAL REGION

reading_time_nozeros <- reading_time[reading_time$region8 != 0,]

#mLKE1post  <- lmer(log(region8) ~ dat*adj + (dat+adj|subj) + (dat+adj|item), data=reading_time_nozeros)
#model changed to above (failed to converge before (the original model))

stanDatLKE1post<-createStanDat(d=reading_time_nozeros,
              rt=reading_time_nozeros$region8,
              form=as.formula("~1+dat+adj+int"))

LKE1post <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatLKE1post,
                    iter = 2000, 
                    chains = 4)

LKE1post_res<-stan_results(m=LKE1post,params=c("Dat","Adj","DatxAdj"))
@



<<originalLKdataExp2,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=

##### ORIGINAL LEVY KELLER DATA EXPERIMENT 2 #####

# CRITICAL REGION  (REGION 8) #

reading_time <- read.table('../data/OrigLevyKellerData/prediction_experiment_data/experiment2/lmr/results/exp3_tt_r.res', header=TRUE)


condition<-ifelse(reading_time$dat=="sub" & reading_time$adj=="sub","a",
                  ifelse(reading_time$dat=="sub" & reading_time$adj=="main","b",
                         ifelse(reading_time$dat=="main" & reading_time$adj=="sub","c", 
                                ifelse(reading_time$dat=="main" & reading_time$adj=="main","d","NA"))))

reading_time$condition<-factor(condition)


# contrast coding: 
reading_time$dat<-ifelse(reading_time$condition%in%c("a","b"),1/2,-1/2)
reading_time$adj<-ifelse(reading_time$condition%in%c("b","d"),-1/2,1/2)
reading_time$int<-ifelse(reading_time$condition%in%c("b","c"),-1/2,1/2)

                 ## ME DAT ## ME PP-ADJ ## INT
# a DAT-SC; PP-SC    0.5         0.5       0.5
# b DAT-SC; PP-MC    0.5        -0.5      -0.5
# c DAT-MC; PP-SC   -0.5         0.5      -0.5
# d DAT-MC; PP-MC   -0.5        -0.5       0.5


# exlude zeros from tfts
reading_time_nozeros <- reading_time[reading_time$region8 != 0,]

#mLKE2crit  <- lmer(log(region8) ~ dat*adj + (dat*adj|subj) + (dat*adj|item), data=reading_time_nozeros)
#summary(mLKE2crit)

# Subset the data frame (conds c and d only such that one can bind it with conditions c and d from LK1).
# LK2 c, d do not need to be relabelled

dataLK2cd<-subset(reading_time, condition!="a" & condition!="b")
@


<<AnalysisLK2critical,results='hide',include=FALSE, cache=TRUE,echo=FALSE,warning=FALSE>>=

# ANALYSIS of TRT original LK2
# CRITICAL REGION (verb+aux)

stanDatLKE2<-createStanDat(d=reading_time_nozeros,
                       rt=reading_time_nozeros$region8,
                       form=as.formula("~1+dat+adj+int"))

#str(stanDat)
LKE2 <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatLKE2,
                    iter = 2000, 
                    chains = 4)

LKE2_res<-stan_results(m=LKE2,params=c("Dat","Adj","DatxAdj"))
@



<<AnalysisLK2postcritical,include=FALSE,results='hide',cache=TRUE,echo=FALSE,warning=FALSE>>=

# EXP 2 REGION 9 POSTCRITICAL REGION #

reading_time_nozeros <- reading_time[reading_time$region9 != 0,]

#mLKE2post  <- lmer(log(region9) ~ dat*adj + (dat*adj|subj) + (dat+adj|item), data=reading_time_nozeros)
#summary(mLKE2post)


stanDatLKE2post<-createStanDat(d=reading_time_nozeros,
                       rt=reading_time_nozeros$region9,
                       form=as.formula("~1+dat+adj+int"))

LKE2post <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatLKE2post,
                    iter = 2000, 
                    chains = 4)

LKE2post_res<-round(stan_results(m=LKE2post,
                           params=c("Dat","Adj","DatxAdj")))
@

Our estimates of total reading times match LK's published results quite closely (see their Tables 6 and 9, pp.\  208, 213). Note that LK's estimates for the interaction term are twice as large as ours; this is only because they multiplied together their main effects, coded $\pm 0.5$, to obtain their interactions, resulting in the interaction in their analyses being coded as $\pm 0.25$. Some estimates (e.g., the effect of Dative in Experiment 1) differ slightly between LK's analysis and ours, because we analyze on log-transformed data and back-transform to raw reading times, whereas LK analyzed raw reading times. 

Our re-analysis of the LK Experiments 1 and 2 is summarized in Figure~\ref{fig:2}.
Recall that the critical region is the main clause verb in Experiment 1, and the relative clause verb in Experiment 2. The post-critical region consisted of the two words following the verb. 
As shown in Figure~\ref{fig:2}, an analysis of total reading times suggests the following:

\begin{enumerate}
\item In Experiment 1, at the critical region, the mean of the posterior for the effect of Dative is \Sexpr{round(LKE1_res[1,1])}~ms, with a 95\% credible interval [\Sexpr{round(LKE1_res[1,2])}, \Sexpr{round(LKE1_res[1,3])}]. The positive coefficient has the interpretation that interposing the dative NP between the subject and the verb leads to facilitation, as predicted by the expectation-based account. LK explain this result as follows:

\begin{quote}
``[The main effect of Dative] can be explained by assuming that the presence the [sic] additional preverbal material allows the processor to predict the upcoming verb, which leads to a facilitation effect.'' (p. 214)
\end{quote}


\item In Experiment 2, at the post-critical region, the estimate of the interaction between Dative and Adjunct is 
\Sexpr{round(LKE2post_res[3,1])} ms [\Sexpr{round(LKE2post_res[3,2])}, \Sexpr{round(LKE2post_res[3,3])}].
LK's interpretation is that having both the dative NP and adjunct interposed between the subject-verb dependency leads to a slowdown. LK explain this outcome in terms of locality effects emerging under high memory load, i.e., when the subject-verb dependency is embedded inside the relative clause \parencite{levy2013expectation}:

\begin{quote}
``[The interaction] suggests the presence of a locality effect, i.e., the additional material that needs to be integrated at the verb, leading to a distance-based cost. This effect was only present in Experiment 2, which tested relative clauses, rather than main clauses as in Experiment 1. This suggests that locality effects can override expectation effects under conditions of high memory load, as we hypothesized would be most likely to occur in a relative clause.'' (p.\ 214)
\end{quote}
\end{enumerate}

We were interested in replicating these effects because they are consistent with a large body of evidence for both expectation and memory-based accounts of sentence processing. There is compelling evidence consistent with the expectation-based account proposed by \textcite{levy08}   \parencites[some examples are the work of][]{linzenuncertainty,kwon2010cognitive,demberg2008data}. Similarly, many studies show evidence for memory-based effects; see, for example, \textcite{grodner,vandykelewis03,vandykemcelree06,VanDyke2011}. Given the literature, it makes sense that we see effects of memory retrieval only under high processing load induced by encountering a relative clause: all demonstrations of locality effects in the literature \parencites[e.g.,][]{hsiao03,grodner,barteketal09} have involved embedded clauses such as those of LK's Experiment 2. Thus, the LK claim that memory load modulates whether expectation effects are observed is compelling given theory and existing data.

Although the claimed effects are compelling given the prior literature,  
one striking aspect of the LK estimates 
is their large uncertainty. 
The evidence for the first conclusion above comes from an estimate with mean
\Sexpr{round(LKE1_res[1,1])}~ms, but the 95\% credible interval ranges from \Sexpr{round(LKE1_res[1,2])} to \Sexpr{round(LKE1_res[1,3])}~ms; and the evidence for the second conclusion comes from an estimate with mean \Sexpr{round(LKE2post_res[3,1])} ms, with a credible interval ranging from \Sexpr{round(LKE2post_res[3,2])} to \Sexpr{round(LKE2post_res[3,3])}~ms. These wide uncertainties imply that values as small as, for example, 20 ms are also plausible.

There is good reason to believe that reading time effects relating to memory-based retrieval may be closer to 20 ms than 80 ms. 
\textcite{NicenboimEtAlCogSci2018} carried out a self-paced reading study investigating number interference in German with $184$ participants. They estimated the magnitude of the memory retrieval effect in number interference to be $9$ ms with 95\% credible interval $[0,18]$.  A meta-analysis by \textcite{JaegerEngelmannVasishth2017} has also shown that similarity-based interference effects as reported by Van Dyke and colleagues have a 95\% probability of lying between $2$ to $28$ ms, with posterior mean $13$ ms. Similarly, recently published estimates of facilitation in reading time (total reading time) due to memory misretrieval are approximately $-20$ ms, with credible intervals ranging approximately from $-1$ to $-40$ ms \parencite{CunningsSturt2018}.  
If memory retrieval effects generally have a small magnitude in reading studies, and if a sample size of $28$ participants  and $24$ items leads to low power, LK's estimates may well be exaggerated. Their estimates have very large standard errors, a characteristic of low-powered studies. 
For example, assume that the true effect in the LK studies is $30$ and $50$ ms.
In this scenario, power for $28$ participants and $24$ items would be about $13$ to $41$\% (see Appendix~\ref{appendix:powerlk13e1} for full details). 
Because of Type M error, with $28$ participants it would be essentially impossible to obtain statistically significant results \textit{that are also accurate estimates of the effect}. 

But how can we determine whether the effects in the LK studies are the result of Type M error? 
If the LK results were not due to a Type M error and LK's effect sizes were in fact as large as LK's estimates, conducting a replication with $28$ participants should have sufficient power to detect them reliably and we should be able to reproduce the effect consistently. 
However, if the LK results were due to Type M error leading to an overestimate of the true effect, we should fail to detect the effect in the majority of cases. 
Thus, it will be very informative to actually conduct direct replication attempts of the LK experiments using the same sample size that was used in the original study. 

We began by trying to replicate the two significant effects found by LK: the main effect of Dative in Experiment 1 (critical region), and the interaction between Dative and Adjunct in Experiment 2 (post-critical region). 
We did this by  conducting four experiments: two self-paced reading (SPR) studies  of the two LK studies, and two eyetracking (ET) studies. We chose these two methods because they are the two standard behavioral approaches for studying cognitive processing costs in reading, and the previous research on expectation-based effects and memory effects has largely relied on either self-paced reading or eyetracking.

\paragraph{Two definitions of replication success}
Before we discuss the replication attempts, it is necessary to define what counts as a successful replication. A successful replication can mean that a statistically significant result in the original study is also found to be significant in the replication attempt. Alternatively, a successful replication could have the interpretation that the  estimated mean from a replication attempt falls within the 95\% credible interval of the original estimate. We will consider both possible ways to interpret a replication attempt.


\subsection{Experiments 1-4}

We conducted two self-paced reading and two eyetracking studies; the correspondence to the original LK experiments is as shown in Table~\ref{tab:correspondence}.

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{lccc}
Our experiment & Original experiment & participants & items \\
Expt 1 (SPR) & LK Expt 1 & 28 & 24\\
Expt 2 (ET) & LK Expt 1 & 28 & 24\\
Expt 3 (SPR) & LK Expt 2 & 28 & 24\\
Expt 4 (ET) & LK Expt 2 & 28 & 24\\
\end{tabular}
\end{center}
\caption{The correspondence between our experiments and those of Levy \& Keller (2013).}\label{tab:correspondence}
\end{table}%



\paragraph{Participants}
For each of the two self-paced reading experiments and the two eyetracking studies, we used the same numbers of participants and items as LK (28 and 24, respectively). 
Thus, the total number of participants in these four studies was $112$.
Participants were native German undergraduate students from the University of Potsdam who were permitted to take part in only one of the replication studies. 
All had normal or corrected-to-normal vision, and received 7 Euros or course credit for their participation.

\paragraph{Experimental design and materials} 
We followed the  $2 \times 2$  fully-crossed within-participants factorial design of the original study. The factors were Dative (in main or subordinate clause) and Adjunct (in main or subordinate clause). We used the same 24 experimental items as LK from their Experiment 1 and 2, and 48 filler items. The yes/no comprehension questions that followed the items targeted various dependencies; these were also identical to the questions employed in the LK experiments. For the example in Table \ref{itemsE1LK}, the question for condition (a) was `Did the teacher impose something on the naughty son?' (\textit{`Hat der Lehrer dem ungezogenen Sohn etwas verhngt?'}) and the question for condition (b) was `Did the teacher impose detention classes?' 
(\textit{`Hat der Lehrer den Strafunterricht verhngt?'}). \label{exptdesignmat}
For a list of all experimental and filler items with their respective comprehension question, see https://osf.io/eyphj/. 



\paragraph{Procedure: Self-paced reading studies}

Experimental items were presented word-by-word in a centered self-paced reading experiment using Linger.\footnote{See http://tedlab.mit.edu/$\sim$dr/Linger/.} As in the original studies, half the items were followed by yes/no questions.
Due to the length of the sentences, non-critical regions were presented phrase-by-phrase. The experiment began after four practice trials. Participants were required to press the space bar on a keyboard to move on to each subsequent word or phrase; in trials with comprehension questions, they recorded a response via a button press. The experimental procedure lasted approximately 35 minutes. For the purposes of future direct replication, all materials and relevant software settings can be obtained from https://osf.io/eyphj/. 


\paragraph{Procedure: Eyetracking studies}

The experimental procedure was identical in all of our eyetracking experiments. 
Participants' eye movements (right eye monocular tracking) were recorded with an  EyeLink 1000 eye-tracker (SR Research\footnote{http://www.sr-research.com/eyelink1000.html.}) with a desktop-mounted camera system at a sampling rate of 1000 Hz. The participant's head was stabilized using a chin/forehead rest. Stimuli were presented on a 22-inch monitor with a $1680\times 1050$ screen resolution. The eye-to-screen distance measured approximately 66 cm. For the experimental presentation,  SR Research Experiment Builder software was used. 
Stimuli were presented in a monospaced font (Courier new) with font size 24 and were arranged on the presentation screen such that the critical region always appeared in the same position (fourth word on the fourth and final line). 
Each session began with the calibration of the eyetracker and four practice trials preceding the experimental materials. Re-calibrations were carried out when necessary. 
In 50\% of the trials, a comprehension question had to be answered by pressing a button on a gamepad. The entire procedure lasted approximately 40 minutes. 

\paragraph{Differences between the LK studies and ours} 

Our procedure and participants differed from the one used by LK in the following way.  
The original LK experiments  were run with an SR Research Eyelink II eyetracker with a head-mounted camera system at a sampling rate of 500 Hz using Eyetrack software\footnote{https://blogs.umass.edu/eyelab/software/.} for the experimental presentation. 
 
 In LK's Experiment 1, the materials were presented in a non-monospaced font (Times New Roman, font size 20), whereas in their Experiment 2 the materials were presented in a monospaced font (Lucida Console, font size 14).  
 The position on the screen of the critical verb differed in their two experiments:
 In LK's Experiment 1, the critical verb appeared in the middle of either the third or fourth line of the presented text, whereas in their Experiment 2 the critical verb was always the fourth word of the fourth line.

In the eyetracking experiments, the critical and post-critical regions were the same as in the LK studies; in the self-paced reading studies, due to an oversight, the post-critical region consisted of only one word (in the LK studies, the post-critical region consisted of two words). Finally, in two experimental items, a non-critical part of the sentence was changed; one due to a plausibility issue and another due to a repetition of an NP within one sentence. One comprehension question following one of the experimental items was replaced due to an ambiguity in  the question. For details on these changes, see the supplementary materials.

LK had $44$ filler items in each of their Experiments 1 and 2, but not all were identical across the experiments. We combined their fillers from their two experiments to assemble $48$ filler items, which were then held constant across all the experiments we conducted. 

Finally, the population of participants differed significantly between the original LK studies and ours. Our participants were native speakers of German who were undergraduates at the University of Potsdam, whereas LK's participants were native speakers of German living in Edinburgh 
\parencite[][p.\ 204]{levy2013expectation}.

<<ResponseAccSPRLK1data,include=FALSE,echo=FALSE,warning=FALSE,message=FALSE>>=
dat<-read.table("../data/E1SPRlevykellerExp1.txt",header=T)

#head(dat)
#str(dat)

dat$item<-factor(dat$item)
dat$subj<-factor(dat$subj)

# subset acc data
datAcc<-subset(dat, dat$roi=="?")


# adjust column names:
datAcc <- plyr::rename(datAcc, c(word="response"))  # plyr

# "acc" column: 
datAcc <- plyr::rename(datAcc, c(region="acc"))
#str(datAcc)

# convert acc column to integer
datAcc$acc <- as.numeric(as.character(datAcc$acc))
#head(datAcc)

# subset acc data (filler items)
#datFillerAcc<-subset(datAcc, expt=="filler")
# mean acc for fillers
#mean(datFillerAcc$acc)

# subset acc data (exp items)
datAcc<-subset(datAcc, expt=="LKrep")
# mean accuracies exp items
#mean(datAcc$acc)

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$cond, mean),2)
#print(mean_acc)

# contrast coding
datAcc$dat<-ifelse(datAcc$cond%in%c("a","b"),1/2,-1/2)
datAcc$adj<-ifelse(datAcc$cond%in%c("b","d"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$cond%in%c("b","c"),-1/2,1/2)
SPRLK1meanacc<-mean(100*with(datAcc,tapply(acc,cond,mean)),na.rm=TRUE)
@


<<AnalysisAccSPRLK1,cache=TRUE,include=FALSE>>=

stanDatAccSPRLK1<-createStanDatAcc(d=datAcc,acc=datAcc$acc,form=as.formula("~1+dat+adj+int"))

SPRLK1Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAccSPRLK1,
                  iter = 2000, 
                  chains = 4)


AccSPRLK1_res<-stan_results(m=SPRLK1Acc,params=c("beta[2]","beta[3]","beta[4]"))
@

<<DataSPRLK1,include=FALSE>>=

# read in data SPR E1
dat<-read.table("../data/E1SPRlevykellerExp1.txt",header=T)

# subset experimental items (exlude filler, practice items)
E1spr<-subset(dat,roi!="?" & expt=="LKrep")

# in original LK1, postcrit region is "und so/und damit", hence, here sum rts of verb1+verb2

E1spr$region<-ifelse(E1spr$roi==13,"verb",
                     ifelse(E1spr$roi==14, "verb1", 
                            ifelse(E1spr$roi==15, "verb2", "noncritical")))


E1sprCRIT<-subset(E1spr, region=="verb")
E1sprPOST1<-subset(E1spr, region=="verb1")
E1sprPOST2<-subset(E1spr, region=="verb2")

E1sprCRIT<-E1sprCRIT[,c(1,3,4,7,8)]
E1sprPOST1<-E1sprPOST1[,c(1,3,4,7,8)]
E1sprPOST2<-E1sprPOST2[,c(1,3,4,7,8)]


E1sprPOST1$rt<-E1sprPOST1$rt + E1sprPOST2$rt

# inspect data frame/double-check the right rts were added
#E1sprPOST<-cbind(E1sprPOST1,E1sprPOST2)
#E1sprPOST$rtPOST<-E1sprPOST1$rt + E1sprPOST2$rt

E1spr<-rbind(E1sprCRIT,E1sprPOST1)

# contrast coding: 
E1spr$dat<-ifelse(E1spr$cond%in%c("a","b"),1/2,-1/2)
E1spr$adj<-ifelse(E1spr$cond%in%c("b","d"),-1/2,1/2)
E1spr$int<-ifelse(E1spr$cond%in%c("b","c"),-1/2,1/2)

                 ## ME DAT ## ME PP-ADJ ## INT
# a DAT-SC; PP-SC    0.5         0.5       0.5
# b DAT-SC; PP-MC    0.5        -0.5      -0.5
# c DAT-MC; PP-SC   -0.5         0.5      -0.5
# d DAT-MC; PP-MC   -0.5        -0.5       0.5


# subset critical, postcritical 
verb<-subset(E1spr,region=="verb")
verb1<-subset(E1spr,region=="verb1")
@

<<AnalysisSPRLK1critical,cache=TRUE,include=FALSE>>=

# E1 SPR rt at critical 
#head(verb)
stanDatSPRLK1<-createStanDat(d=verb,rt=verb$rt,
                             form=as.formula("~1+dat+adj+int"))

SPRLK1 <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatSPRLK1,
                    iter = 2000, 
                    chains = 4)

# check
#m1 <- lmer(log(rt)~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb)
#summary(m1)

pars<-c("Dat","Adj","DatxAdj")
SPRE1_res<-round(stan_results(m=SPRLK1,params=pars))
@

<<AnalysisSPRLK1criticalraw,echo=FALSE>>=
#SPRLK1critm1 <- lmer(rt~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb)
@

<<AnalysisSPRLK1postcritical,cache=TRUE,echo=FALSE,warning=FALSE>>=

# E1 SPR rt at postcritical
#head(verb1)
stanDatSPRE1post<-createStanDat(d=verb1,rt=verb1$rt,form=as.formula("~1+dat+adj+int"))

SPRE1post <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatSPRE1post,
                    iter = 2000, 
                    chains = 4)

# check
#m1post <- lmer(log(rt)~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb1)
#summary(m1post)

SPRE1post_res<-round(stan_results(m=SPRE1post,params=pars))
@

<<AnalysisSPRLK1postcriticalraw,echo=FALSE>>=
#SPRLK1postcritm1post <- lmer(rt~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb1)
@

<<AccE3data,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=
dat<-read.table("../data/E3SPRlevykellerExp2.txt",header=T)

dat$item<-factor(dat$item)
dat$subj<-factor(dat$subj)

# subset acc data
datAcc<-subset(dat, dat$roi=="?")

# adjust column names:
datAcc <- plyr::rename(datAcc, c(word="response"))  # plyr

# "acc" column: 
datAcc <- plyr::rename(datAcc, c(region="acc"))
#str(datAcc)

# convert acc column to integer
datAcc$acc <- as.numeric(as.character(datAcc$acc))
#head(datAcc)

# subset acc data (filler items)
#datFillerAcc<-subset(datAcc, expt=="filler")
# mean acc for fillers
#mean(datFillerAcc$acc)

# subset acc data (exp items)
datAcc<-subset(datAcc, expt=="LKrep")
# mean accuracies exp items
#mean(datAcc$acc)

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$cond, mean),2)
#print(mean_acc)

# contrast coding
datAcc$dat<-ifelse(datAcc$cond%in%c("a","b"),1/2,-1/2)
datAcc$adj<-ifelse(datAcc$cond%in%c("b","d"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$cond%in%c("b","c"),-1/2,1/2)

SPRLK2meanacc<-100*mean(with(datAcc,tapply(acc,cond,mean)),na.rm=TRUE)
@


<<AnalysisAccE3SPR,cache=TRUE,eval=FALSE,echo=FALSE,warning=FALSE>>=
stanDatAccSPRE2<-createStanDatAcc(d=datAcc,acc=datAcc$acc,form=as.formula("~1+dat+adj+int"))

E3Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAccSPRE2,
                  iter = 2000, 
                  chains = 4)


#print(mE3Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))
@

%Overall response accuracy of the experimental items was 61\% compared with 91\% for the filler items. 

<<DataRTE3crit,include=FALSE>>=

# # E3: ("direct replication of LK13 E2") Analysis
## Method: SPR (Linger)
## no participants or trials excluded
## N=28, items=24

dat<-read.table("../data/E3SPRlevykellerExp2.txt",header=T)

E3spr<-subset(dat,roi!="?" & expt=="LKrep")


# contrast coding (same as E1 above)
E3spr$dat<-ifelse(E3spr$cond%in%c("a","b"),1/2,-1/2)
E3spr$adj<-ifelse(E3spr$cond%in%c("b","d"),-1/2,1/2)
E3spr$int<-ifelse(E3spr$cond%in%c("b","c"),-1/2,1/2)

## subset critical region, postcritical region 
verb<-subset(E3spr,region=="verb")
verb1<-subset(E3spr,region=="verb1")
#head(verb)

# mean RTs at critical region:
#with(verb,round(tapply(rt,cond,mean)))
# mean RTs at postcritical region:
#with(verb1,round(tapply(rt,cond,mean)))

@


<<AnalysisRTcritE3,cache=TRUE,echo=FALSE,include=FALSE>>=

stanDatSPRE2<-createStanDat(d=verb,rt=verb$rt,
              form=as.formula("~1+dat+adj+int"))

SPRE2 <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatSPRE2,
                    iter = 2000, 
                    chains = 4)

# check
#m2 <- lmer(log(rt)~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb)
#summary(m2)

SPRE2_res<-round(stan_results(m=SPRE2,params=pars))
@

<<AnalysisRTcritE3raw,echo=FALSE>>=
#mRTcritE3raw <- lmer(rt~dat+adj+int+(1+dat+adj+int||subj)+(1+dat+adj+int||item),verb)
@

<<AnalysisRTE3postcrit,cache=TRUE,echo=FALSE,include=FALSE>>=
stanDatSPRE2post<-createStanDat(d=verb1,
                                rt=verb1$rt,
              form=as.formula("~1+dat+adj+int"))

SPRE2post <- stan(file = "StanModels/maxModel.stan", 
                    data = stanDatSPRE2post,
                    iter = 2000, 
                    chains = 4)

SPRE2post_res<-round(stan_results(m=SPRE2post,params=pars))
@

<<AnalysisE2Acc,include=FALSE>>=
dat<-read.table("../data/E2ETlevykellerExp1.txt",header=T)

# subset Accuracy data
datAcc<-subset(dat,roi==1)
#summary(datAcc)
#head(datAcc)
#str(datAcc)
#levels(datAcc$condition)
#datAcc$RESPONSE_ACCURACY  
### quesitons only follow 50% of items: remove -2 (RESPONSE ACC) or 0 (answer) (i.e., no questions) and NAs


# subset data filler items (exclude LKrep and practice)
datFillerAcc<-subset(datAcc,condition=="f" & answer!=0)
datFillerAcc<-na.omit(datFillerAcc)

# subset data experimental items (exclude fillers and practice)
datAcc<-subset(datAcc,condition!="f" & condition!="p" & answer!=0)
datAcc<-na.omit(datAcc)

# sanity checks
#xtabs(~subject+condition,datAcc)
#length(unique(datAcc$itemid))


# "acc" column: 
datAcc <- plyr::rename(datAcc, c(RESPONSE_ACCURACY="acc"))
datFillerAcc <- plyr::rename(datFillerAcc, c(RESPONSE_ACCURACY="acc"))
# mean acc exp items
#mean(datAcc$acc) # [1] 0.6448598
# mean acc for fillers
#mean(datFillerAcc$acc) # [1] 0.898773

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$condition, mean),2)
#print(mean_acc)

# contrast coding
datAcc$dat<-ifelse(datAcc$cond%in%c("a","b"),1/2,-1/2)
datAcc$adj<-ifelse(datAcc$cond%in%c("b","d"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$cond%in%c("b","c"),-1/2,1/2)
@

<<E2AnalysisAcc,cache=TRUE,echo=FALSE,include=FALSE>>=

subj <- as.numeric(as.factor(datAcc$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(datAcc$itemid))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + dat + adj + int, datAcc))  
Z_u <- unname(model.matrix(~ 1, datAcc)) 
Z_w <- unname(model.matrix(~ 1, datAcc))

attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDatAcc <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                acc = datAcc$acc,                  
                subj = as.integer(subj),  
                item = as.integer(item))  


# 3. Fit the model.

mE2Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAcc,
                  iter = 2000, 
                  chains = 4)


#print(mE2Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))

#Traceplot
#traceplot(mE2Acc, pars=c("beta","sigma_u","sigma_w"), inc_warmup=FALSE)

ETLK1meanacc<-100*mean(with(datAcc,tapply(acc,condition,mean)),na.rm=TRUE)
@


%For our second replication attempt of LK1, the overall question response accuracy was approximately 64\% for experimental items while accuracy for filler items was much higher (90\%). 
 

<<DataE2,include=FALSE,echo=FALSE>>=
dat<-read.table("../data/E2ETlevykellerExp1.txt",header=T)

# exclude filler and practice items
dat<-subset(dat,condition!="f" & condition!="p")

# roi as factor column
dat$roi<-factor(dat$roi)
#unique(dat$roi)
#str(dat)

# contrast coding (same as SPR E1 and SPR E2)
dat$dat<-ifelse(dat$condition%in%c("a","b"),1/2,-1/2)
dat$adj<-ifelse(dat$condition%in%c("b","d"),-1/2,1/2)
dat$int<-ifelse(dat$condition%in%c("b","c"),-1/2,1/2)


#rois:
#precritical region (acc NP):
#a = 22 (was rois 22+23)
#b = 22 (was rois 22+23)
#c = 22 (was rois 22+23)
#d = 22 (was rois 22+23)

#critical region (verb):
#a = 24
#b = 24
#c = 24
#d = 24

#postcritical region ( "und"):
#a = 25 (was 25+26)
#b = 25 (was 25+26)
#c = 25 (was 25+26)
#d = 25 (was 25+26)

region<-ifelse(dat$roi==22,"npacc",
               ifelse(dat$roi==24,"verb",
                      ifelse(dat$roi==25,"verb1","noncritical")))

#length(region)  
#dim(dat)
#summary(dat)

dat$region<-region
dat<-subset(dat,region!="noncritical")
dat$region<-factor(dat$region)
#summary(dat)

dat$subj<-dat$subject
dat$item<-dat$itemid

# subset critical region, postcritical regionm
verb<-subset(dat,region=="verb")
verb1<-subset(dat,region=="verb1")
@

<<E2AnalysisTFTcrit,include=FALSE,cache=TRUE,echo=FALSE>>=

verbTFT <- subset(verb, verb$TFT>0)
verbRRT <- subset(verb, verb$RRT>0)
verbFPRT <- subset(verb, verb$FPRT>0)

#summary(m_LKrepE1crit<-lmer(FPRT~dat+adj+int+(1+dat+adj+int||subject) + (1+dat+adj+int||itemid),verbFPRT))#summary(m_LKrepE1crit<-lmer(FPRT~dat+adj+int+(1+dat+adj+int||subject) + (1+dat+adj+int||itemid),verbFPRT))

#head(verb)

stanDat<-createStanDat(d=verbTFT,rt=verbTFT$TFT,
              form=as.formula("~1+dat+adj+int"))
#str(stanDatE2)

E2_3 <- stan(file = "StanModels/maxModel.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)


# check
#m6 <- lmer(log(TFT)~dat+adj+int+(1+dat+adj+int||subject)+(1+dat+adj+int||itemid),verbTFT)
#summary(m6)

E2_3_res<-stan_results(m=E2_3,params=pars)
@

<<E2TFTcritraw,echo=FALSE>>=
#mE2TFTcritraw <- lmer(TFT~dat+adj+int+(1+dat+adj+int||subject)+(1+dat+adj+int||itemid),verbTFT)
@

<<E2AnalysisTFTpostcrit,cache=TRUE,echo=FALSE,include=FALSE>>=

verb1TFT <- subset(verb1, verb1$TFT>0)
verb1FPRT <- subset(verb1, verb1$FPRT>0)
verb1RRT <- subset(verb1, verb1$RRT>0)

#head(verb)
#summary(m_LKrepE1postcrit<-lmer(RRT~dat+adj+int+(1+dat+adj+int||subject) + (1+dat+adj+int||itemid),verb1RRT))
#summary(m_LKrepE1postcrit<-lmer(FPRT~dat+adj+int+(1+dat+adj+int||subject) + (1+dat+adj+int||itemid),verb1FPRT))
stanDat<-createStanDat(d=verb1TFT,rt=verb1TFT$TFT,
              form=as.formula("~1+dat+adj+int"))

E2_3post <- stan(file = "StanModels/maxModel.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)


#print(E2_3post, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

E2_3post_res<-stan_results(m=E2_3post,params=pars)
@



<<E4AccData,cache=FALSE,echo=FALSE,include=FALSE>>=
dat<-read.table("../data/E4ETlevykellerExp2.txt",header=T)

# subset Accuracy data
datAcc<-subset(dat,roi==1)
### quesitons only follow 50% of items: remove -2 (RESPONSE ACC) or 0 (answer) (i.e., no questions) and NAs


# subset data filler items (exclude LKrep and practice)
datFillerAcc<-subset(datAcc,condition=="f" & answer!=0)
datFillerAcc<-na.omit(datFillerAcc)

# subset data experimental items (exclude fillers and practice)
datAcc<-subset(datAcc,condition!="f" & condition!="p" & answer!=0)
datAcc<-na.omit(datAcc)

# sanity checks
#
#xtabs(~subject+condition,datAcc)
#length(unique(datAcc$itemid))


# "acc" column: 
datAcc <- plyr::rename(datAcc, c(RESPONSE_ACCURACY="acc"))
datAcc$acc<-as.numeric(as.character(datAcc$acc))
#str(datAcc)
datFillerAcc <- plyr::rename(datFillerAcc, c(RESPONSE_ACCURACY="acc"))
datFillerAcc$acc<-as.numeric(as.character(datFillerAcc$acc))

# mean acc exp items
#mean(datAcc$acc)
# mean acc for fillers
#mean(datFillerAcc$acc) 

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$condition, mean),2)
#print(mean_acc)

# contrast coding
datAcc$dat<-ifelse(datAcc$cond%in%c("a","b"),1/2,-1/2)
datAcc$adj<-ifelse(datAcc$cond%in%c("b","d"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$cond%in%c("b","c"),-1/2,1/2)
@


<<E4AnalysisAccuracy,cache=TRUE,echo=FALSE,include=FALSE>>=

subj <- as.numeric(as.factor(datAcc$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(datAcc$itemid))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + dat + adj + int, datAcc))  
Z_u <- unname(model.matrix(~ 1, datAcc)) 
Z_w <- unname(model.matrix(~ 1, datAcc))

attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDatAcc <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                acc = datAcc$acc,                  
                subj = as.integer(subj),  
                item = as.integer(item))  


# 3. Fit the model.

mE4Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAcc,
                  iter = 2000, 
                  chains = 4)


#print(mE4Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))

#Traceplot
#traceplot(mE4Acc, pars=c("beta","sigma_u","sigma_w"), inc_warmup=FALSE)

ETLK2meanacc<-100*mean(with(datAcc,tapply(acc,condition,mean)),na.rm=TRUE)
@

%Experimental items have an overall response accuracy of approximately 60\% while the question response accuracy for fillers was 91\%.


<<E4RTdata,include=FALSE,echo=FALSE>>=

# E4: ("identical replication of LK13 E2") Analysis
## Method: ET (EyeLink 1000, SR Research)
## Item 14 excluded from lists 1, 5, 10, 11, 16, 18, 26 due to spelling error (item with error was analyzed in original Levy&Keller for all subjects). None of the other 5 replications are affected.
## few missing data points due to skipping of trials (i.e. participant looked at fixation trigger immediately without reading sentence).
## N=28, items=24

dat<-read.table("../data/E4ETlevykellerExp2.txt",header=T)

# subset data
dat<-subset(dat,condition!="f" & condition!="p")

# contrast coding 
dat$dat<-ifelse(dat$condition%in%c("a","b"),1/2,-1/2)
dat$adj<-ifelse(dat$condition%in%c("b","d"),-1/2,1/2)
dat$int<-ifelse(dat$condition%in%c("b","c"),-1/2,1/2)


dat$roi<-factor(dat$roi)
#unique(dat$roi)
#unique(dat$subject)

#rois:
#precritical region (acc NP):
#a = 23 (was rois 23+24)
#b = 23 (was rois 23+24)
#c = 23 (was rois 23+24)
#d = 23 (was rois 23+24)

#critical region (verb):
#a = 25 (was rois 25+26)
#b = 25 (was rois 25+26)
#c = 25 (was rois 25+26)
#d = 25 (was rois 25+26)

#postcritical region (verb1):
#a = 27 (was rois 27+28) * for all conditions (except for items 15,17,18,20; only roi 27, only 1 word)
#b = 27 (was rois 27+28)
#c = 27 (was rois 27+28)
#d = 27 (was rois 27+28)

region<-ifelse(dat$roi==23,"npacc",
               ifelse(dat$roi==25,"verb",
                      ifelse(dat$roi==27,"verb1","noncritical")))

dat$region<-region
dat<-subset(dat,region!="noncritical")
dat$region<-factor(dat$region)

dat$subj<-dat$subject
dat$item<-dat$itemid

# subset critical and postcritical region
verb <-subset(dat,region=="verb")
verb1 <-subset(dat,region=="verb1")
@

<<AnalysisE4TFTcrit,cache=TRUE,echo=FALSE,include=FALSE>>=
verbTFT <- subset(verb, verb$TFT>0)
verbRRT <- subset(verb, verb$RRT>0)
verb1RRT <- subset(verb1, verb1$RRT>0)
verb1TFT <- subset(verb, verb1$TFT>0)

#summary(m_LKE2repRRT<-lmer(RRT~dat+adj+int+(1+dat+adj+int||subject)+(1|item),verb1RRT))
# 

## regression prob: does not converge
verb$reg<-ifelse(verb$FPRT==verb$RPD,0,1)
verb1$reg<-ifelse(verb1$FPRT==verb1$RPD,0,1)
#with(verb1,tapply(reg,condition,mean))
#summary(m_LKE2repregp<-glmer(reg~dat+adj+int+(1+dat+adj+int|subj)+(1|item),family=binomial(),verb1))
#summary(m_LKE2repskip<-glmer(FFP~dat+adj+int+(1|subj)+(1|item),family=binomial(),verb1))

#head(verb)

stanDat<-createStanDat(d=verbTFT,rt=verbTFT$TFT,
              form=as.formula("~1+dat+adj+int"))

E4_3 <- stan(file = "StanModels/maxModel.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

E4_3_res<-stan_results(m=E4_3,params=pars)
@

<<E4ETraw,echo=FALSE>>=
#m_E4ETTFTrawcrit<-lmer(TFT~dat+adj+int+(1+dat+adj+int||subj)+(1|item),verbTFT)
#m_E4ETTFTrawpostcrit<-lmer(TFT~dat+adj+int+(1+dat+adj+int||subj)+(1|item),verbTFT)
@





<<AnalysisE4TFTpostcrit,cache=TRUE,echo=FALSE,include=FALSE>>=
verb1TFT <- subset(verb1, verb1$TFT>0)

stanDat<-createStanDat(d=verb1TFT,rt=verb1TFT$TFT,
              form=as.formula("~1+dat+adj+int"))

E4_3post <- stan(file = "StanModels/maxModel.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

E4_3post_res<-stan_results(m=E4_3post,params=pars)
@



\subsection{Results of Experiment 1-4}

%\begin{figure}[!htbp]
%\centering
<<figurese1,echo=FALSE,include=FALSE,warning=FALSE,message=FALSE>>=
cond<-factor(c("Dat","Adj","DatxAdj"),levels=c("Dat","Adj","DatxAdj"))
cond<-rep(cond,3)
expt<-factor(c(rep("LK Expt 1",3),
        rep("Expt 1, SPR",3),rep("Expt 2, ET",3)),levels=c("LK Expt 1","Expt 1, SPR","Expt 2, ET"))
  
rownames(LKE1_res)<-NULL
rownames(SPRE1_res)<-NULL
rownames(E2_3_res)<-NULL

LKE1all<-data.frame(rbind(LKE1_res,SPRE1_res,E2_3_res))
LKE1all<-data.frame(cond=cond,expt=expt,LKE1all)

p1<-plotresults(LKE1all,xaxisblank=TRUE,
                maintitle="Replications of LK Expt 1 \n (critical region)",removelegend=TRUE)

rownames(LKE1post_res)<-NULL
rownames(SPRE1post_res)<-NULL
rownames(E2_3post_res)<-NULL

LKE1postall<-data.frame(rbind(LKE1post_res,SPRE1post_res,E2_3post_res))
LKE1postall<-data.frame(cond=cond,expt=expt,LKE1postall)

p2<-plotresults(LKE1postall,maintitle="Replications of LK Expt 1 \n (post-critical region)",ylabel="",xaxisblank=TRUE)
#multiplot(p1,p2,cols=1)
@
%\caption{The effects of Dative and Adjunct interposition (and their interaction) at the critical and post-critical regions. Shown are the mean and 95\% credible intervals from the original LK Experiment 1, and the two replication attempts. The self-paced reading study's dependent measure is reading time in ms at the respective regions, and the eyetracking study's dependent measure is total reading times.}\label{fig:1}
%\end{figure}


\begin{figure}[!htbp]
\centering
<<figurese2,echo=FALSE,warning=FALSE,message=FALSE,include=TRUE,fig.width=7,fig.height=7>>=
rownames(LKE2_res)<-NULL
rownames(SPRE2_res)<-NULL
rownames(E4_3_res)<-NULL

LKE2all<-data.frame(rbind(LKE2_res,SPRE2_res,E4_3_res))

expt<-factor(c(rep("LK Expt 2",3),
        rep("Expt 3, SPR",3),rep("Expt 4, ET",3)),levels=c("LK Expt 2","Expt 3, SPR","Expt 4, ET"))


LKE2all<-data.frame(cond=cond,expt=expt,LKE2all)

p1_2<-plotresults(LKE2all,maintitle="Replications of LK Expt 2 \n (critical region)",cols=c("darkgray", "black","black"),removelegend=TRUE)

rownames(LKE2post_res)<-NULL
rownames(SPRE2post_res)<-NULL
rownames(E4_3post_res)<-NULL

LKE2postall<-data.frame(rbind(LKE2post_res,SPRE2post_res,E4_3post_res))
LKE2postall<-data.frame(cond=cond,expt=expt,LKE2postall)

p2_2<-plotresults(LKE2postall,maintitle="Replications of LK Expt 2 \n (post-critical region)",cols=c("darkgray", "black","black"),ylabel="")
multiplot(p1,p1_2,p2,p2_2,cols=2)
@
\caption{The effects of Dat(ive) and Adj(unct) interposition (and their interaction, DatxAdj) at the critical and post-critical regions. Shown are the mean and 95\% credible intervals from the original LK Experiments 1 and 2, and the two replication attempts. SPR stands for self-paced reading, and ET stands for eyetracking.}\label{fig:2}
\end{figure}

\paragraph{Question-response accuracies}

The question-response accuracies for 
Experiments 1 and 2 were \Sexpr{round(SPRLK1meanacc)} and \Sexpr{round(ETLK1meanacc)}, respectively, and for Experiments 3 and 4 they were \Sexpr{round(SPRLK2meanacc)}\%
 and 
\Sexpr{round(ETLK2meanacc)}\%, respectively. These are comparable to LK's 69\% and 65\% in their Experiments 1 and 2, respectively.

\paragraph{Reading time results}

Figure~\ref{fig:2} summarizes the results of our four experiments. 
As mentioned earlier, we only report the analyses of total reading time data.\footnote{In our data, we also analyzed all the dependent measures
(critical and post-critical regions) in which LK found statistical
significance in their data. These were first-pass and
re-reading times in Experiment 1, and re-reading times, the proportion
of first-pass regressions, and skipping proportions in Experiment 2
(in the critical or post-critical region). None of these dependent measures came out
statistically significant in our data.}

Recall that a successful replication can either mean that a significant effect found in an original study is found to be significant in a replication attempt; or it can mean that the estimated means from the replication attempt fall within the 95\% credible interval of the original estimates.
If statistical significance is taken as a criterion for successful replication, 
we failed to replicate
the two key effects in the LK studies: the main effect of Dative in Experiment 1 (critical region), and the interaction of Dative and Adjunct in Experiment 2 (post-critical region). If a frequentist p-value were to be computed for these effects, none would come out even close to significant in any of the four attempts. The means and 95\% credible intervals for the critical comparisons in each experiment are as follows:

\begin{itemize}
\item
Expt 1 (SPR replication of LK Expt 1): Effect of Dative in critical region \Sexpr{round(SPRE1_res[1,1])}~ms [\Sexpr{round(SPRE1_res[1,2])},\Sexpr{round(SPRE1_res[1,3])}].
\item
Expt 2 (Eyetracking replication of LK Expt 1): Effect of Dative in critical region 
\Sexpr{round(E2_3_res[1,1])}~ms [\Sexpr{round(E2_3_res[1,2])},\Sexpr{round(E2_3_res[1,3])}].
\item
Expt 3 (SPR replication of LK Expt 2): Interaction of Dative and Adjunct in post-critical region \Sexpr{round(SPRE2post_res[3,1])}~ms [\Sexpr{round(SPRE2post_res[3,2])},\Sexpr{round(SPRE2post_res[3,3])}].
\item 
Expt 4 (Eyetracking replication of LK Expt 2): Interaction of Dative and Adjunct in post-critical region
\Sexpr{round(E4_3post_res[3,1])}~ms [\Sexpr{round(E4_3post_res[3,2])},\Sexpr{round(E4_3post_res[3,3])}].
\end{itemize}

However, the replication attempts can also be seen as a near-complete success: \textit{all} the total reading times estimates from the eyetracking studies (and $9$ of the $12$ of the estimates computed in the self-paced reading experiments) fall within the 95\% credible intervals of the original studies. 

The crucial point here is that the original estimates are so noisy that, despite the fact that some of the effects in the original paper were statistically significant, the wide credible intervals are consistent with  the effect being near $0$ ms. 
When the estimates are noisy, the p-value furnishes little information about reliability (i.e., that the effect is true) or replicability (i.e., that the significant effect can be reproduced if the study is repeated). Of course, even when estimates are not noisy, the only way to establish replicability is to actually replicate the effect.

In these first four small-sample replication attempts above, we aimed to show that the original estimates are noisy and therefore  uninformative, despite being statistically significant.
Next, we turned our attention to one of the conclusions that LK drew from their study \parencite{levy2013expectation}:

\begin{quote}
``[The interaction] suggests the presence of a locality effect, i.e., the additional material that needs to be integrated at the verb, leading to a distance-based cost. \textit{This effect was only present in Experiment 2, which tested relative clauses, rather than main clauses as in Experiment 1}.'' (p.\ 214)
\end{quote}

The emphasis is ours.
Here, LK are pointing to the fact that the interaction between Dative and Adjunct was found in Experiment 2 but not in Experiment 1. We will refer to this difference between the two experiments as the \textit{Load-Distance interaction}. Our goal here is to show how the estimates of the effect change under a larger-sample replication attempt.

\subsection{Investigating the Load-Distance interaction}



<<CombinedOriginalLK1LK2cd,cache=FALSE,include=FALSE,echo=FALSE,warning=FALSE>>=

# Prepare data (which was subset earlier as dataLK1cd and dataLK2cd) to merge as one data frome.
#head(dataLK1cd)
#head(dataLK2cd)

## LK EXPERIMENT 1

# relabel LK1 c, d as conditions a and b
dataLK1cd$condition[dataLK1cd$condition=="c"] <- "a"
dataLK1cd$condition[dataLK1cd$condition=="d"] <- "b"

dataLK1cd$subj<-as.factor(dataLK1cd$subj)
dataLK1cd$exp<-"LK1"
dataLK1cd$exp<-as.factor(dataLK1cd$exp)
#str(dataLK1cd)

# add new subject column that makes subj "1" "1LK1"
dataLK1cd$subject<-with(dataLK1cd, paste(subj,exp, sep=""))
dataLK1cd$subject<-as.factor(dataLK1cd$subject)
dataLK1cd<-dataLK1cd[,c(2,5,6,7,8,9,10,11,12,13,14,17)]
#head(dataLK1cd)

## LK EXPERIMENT 2

# recode such that we have the same critcal/postcritical regions for LK1 and LK2 in my merged data frame
# ==> for LK2, delete region 7

dataLK2cd <- dataLK2cd[,-11]

# ==> for LK2 rename region 8: now region 7
# ==> for LK2 rename region 9: now region 8

names(dataLK2cd)[names(dataLK2cd)=="region8"] <- "region7"
names(dataLK2cd)[names(dataLK2cd)=="region9"] <- "region8"
names(dataLK2cd)[names(dataLK2cd)=="region10"] <- "region9"


dataLK2cd$subj<-as.factor(dataLK2cd$subj)
dataLK2cd$exp<-"LK2"
dataLK2cd$exp<-as.factor(dataLK2cd$exp)
#str(dataLK2cd)

# add new subject column that makes subj "1" "1LK2"
dataLK2cd$subject<-with(dataLK2cd, paste(subj,exp, sep=""))
dataLK2cd$subject<-as.factor(dataLK2cd$subject)
dataLK2cd<-dataLK2cd[,c(2,5,6,7,8,9,10,11,12,13,14,17)]


#head(dataLK1cd)
#head(dataLK2cd)
#str(dataLK1cd)

## bind LK1 (c,d) and LK2 (c,d) together as one data frame:

dataLK1LK2cd<-rbind(dataLK1cd,dataLK2cd)
#head(dataLK1LK2cd)
#str(dataLK1LK2cd)

## contrast coding
dataLK1LK2cd$load<-ifelse(dataLK1LK2cd$condition%in%c("a","b"),-1/2,1/2)
dataLK1LK2cd$dist<-ifelse(dataLK1LK2cd$condition%in%c("a","c"),-1/2,1/2)
dataLK1LK2cd$int<-ifelse(dataLK1LK2cd$condition%in%c("a","d"),-1/2,1/2)


                         ## ME LOAD ## ME DIST  ## INT
# a DAT-MC;     PP-SC      -0.5        -0.5      -0.5   (originally E1 LK13 cond c)
# b DAT-MC;     PP-MC      -0.5         0.5       0.5   (orininally E1 LK13 cond d)
# c DAT-MC emb; PP-SC emb   0.5        -0.5       0.5   (originally E2 LK13 cond c)
# d DAT-MC emb; PP-MC emb   0.5         0.5      -0.5   (originally E2 LK13 cond d)

# ME load: positive coef = longer RTs in higher memory load conditions (MC embedding in RC); negative coefficient = shorter RTs (speedup) in high memory load conditions
# ME dist: positive coef = longer RTs when distance btw. subject and verb increased, i.e. when both ADJ and DAT in MC independent of whether MC is embedded in RC or not. 

# Region 7 (merged data frame) = critical region, 
# Region 8 = postcritical region 


@


<<AnalysisLK1LK2critical,cache=TRUE,echo=FALSE,include=FALSE,warning=FALSE,message=FALSE>>=

# original LK1 and LK2 merged (conditions c, d)

# subset data
dataLK1LK2cdCRIT<-subset(dataLK1LK2cd, dataLK1LK2cd$region7>0)
#head(dataLK1LK2cdCRIT)
## load is a between subjects factor:
#xtabs(~subj+load,dataLK1LK2cdCRIT)
## but all predictors are within items
##xtabs(~item+load,dataLK1LK2cdCRIT)

subj <- as.numeric(as.factor(dataLK1LK2cdCRIT$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(dataLK1LK2cdCRIT$item))
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, dataLK1LK2cdCRIT))
Z_u <- unname(model.matrix(~ 1 + dist + int, dataLK1LK2cdCRIT))
Z_w <- unname(model.matrix(~ 1 + load + dist + int, dataLK1LK2cdCRIT))

attr(X, which="assign") <- NULL

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X),          
                P = ncol(X),               
                n_u = ncol(Z_u),              
                n_w = ncol(Z_w),       
                X = X,                      
                Z_u = Z_u,              
                Z_w = Z_w,                   
                J = N_subj,                
                K = N_items,
                rt = dataLK1LK2cdCRIT$region7,                   
                subj = as.integer(subj),
                item = as.integer(item))
#str(stanDat)        

# 3. Fit the model.

LKmerged <- stan(file = "StanModels/maxModelmerged.stan", 
                    data = stanDat,
                    iter = 2000, 
                    chains = 4,
                    control = list(adapt_delta = 0.99)) 

# added adapt delta as got divergent transitions warning once, and low neff/high Rhat another time. 
# sigmau1 and sigmau2 still low neffs.
# increase iteration to 4000

#print(LKmerged, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))
#print(LKmerged, pars=c("Load","Dist","LoadxDist"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_LKmerged, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)
# sigmau1 and sigmau2 ?

pars<-c("Load","Dist","LoadxDist")
# extract the model estimates
LKmerged_res<-stan_results(m=LKmerged,params=pars)
@

<<AnalysisLK1LK2postcritical,cache=TRUE,echo=FALSE,include=FALSE,message=FALSE>>=

# subset data
dataLK1LK2cdPOST<-subset(dataLK1LK2cd, dataLK1LK2cd$region8>0)
#head(dataLK1LK2cdPOST)

subj <- as.numeric(as.factor(dataLK1LK2cdPOST$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(dataLK1LK2cdPOST$item))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + load + dist + int, dataLK1LK2cdPOST))
Z_u <- unname(model.matrix(~ 1 + dist + int, dataLK1LK2cdPOST))
Z_w <- unname(model.matrix(~ 1 + load + dist + int, dataLK1LK2cdPOST))

attr(X, which="assign") <- NULL

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X),               
                P = ncol(X),               
                n_u = ncol(Z_u),           
                n_w = ncol(Z_w),             
                X = X,                      
                Z_u = Z_u,               
                Z_w = Z_w,                   
                J = N_subj,                 
                K = N_items,
                rt = dataLK1LK2cdPOST$region8,                   
                subj = as.integer(subj),
                item = as.integer(item))

# 3. Fit the model.

### 
LKmergedpost <- stan(file = "StanModels/maxModelmerged.stan", 
                    data = stanDat,
                    iter = 2000, 
                    chains = 4,
                    control = list(adapt_delta = 0.99))


#print(LKmergedpost, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_LKmergedpost, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
LKmergedpost_res<-stan_results(m=LKmergedpost,params=pars)
@

%% merged replications:

<<E5ResponseAccdata,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=
dat<-read.table("../data/E5SPRlevykellerExp12.txt",header=T)

dat$item<-factor(dat$item)
dat$subj<-factor(dat$subj)

# subset acc data
datAcc<-subset(dat, dat$roi=="?")

# adjust column names:
datAcc <- plyr::rename(datAcc, c(word="response"))  # plyr

# "acc" column: 
datAcc <- plyr::rename(datAcc, c(region="acc"))
#str(datAcc)

# convert acc column to integer
datAcc$acc <- as.numeric(as.character(datAcc$acc))
#head(datAcc)

# subset acc data (filler items)
#datFillerAcc<-subset(datAcc, expt=="filler")
# mean acc for fillers
#mean(datFillerAcc$acc)

# subset acc data (exp items)
datAcc<-subset(datAcc, expt=="LKrep")
# mean accuracies exp items
#mean(datAcc$acc)

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$cond, mean),2)
#print(mean_acc)

# contrast coding
datAcc$load<-ifelse(datAcc$cond%in%c("a","b"),-1/2,1/2)
datAcc$dist<-ifelse(datAcc$cond%in%c("a","c"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$cond%in%c("a","d"),-1/2,1/2)

# "combined exp, conds c, d of LK1 and LK2

#              load    dist    int
# a            -0.5   -0.5    -0.5
# b            -0.5   +0.5    +0.5 
# c            +0.5   -0.5    +0.5
# d            +0.5   +0.5    -0.5

E5meanacc<-round(mean(100*with(datAcc,tapply(acc,cond,mean)),na.rm=TRUE))
@




<<AnalysisResponseAccE5SPR, include=FALSE,results='hide',cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

subj <- as.numeric(as.factor(datAcc$subj))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(datAcc$item))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + load + dist + int, datAcc))  
Z_u <- unname(model.matrix(~ 1, datAcc)) 
Z_w <- unname(model.matrix(~ 1, datAcc))

attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDatAcc <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                acc = datAcc$acc,                  
                subj = as.integer(subj),  
                item = as.integer(item))  


# 3. Fit the model.

mE5Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAcc,
                  iter = 2000, 
                  chains = 4)


#print(mE5Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))

#Traceplot
#traceplot(mE5Acc, pars=c("beta","sigma_u","sigma_w"), inc_warmup=FALSE)

@

%The overall response accuracy for experimental items was around 60\%; for filler items accuracy was higher (approximately 90\%).


<<AnalysisE5SPRcrit,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=

# E5: LK13 combined ("replication of LK13 E1 and E2 conds c, d") Analysis
## Method: SPR (Linger)
## no participants or trials excluded
## N=28, items=24

dat<-read.table("../data/E5SPRlevykellerExp12.txt",header=T)

E5spr<-subset(dat,roi!="?" & expt=="LKrep")

#round(with(subset(E5spr,region=="verb1"),tapply(rt,cond,mean)))
#round(with(subset(E5spr,region=="verb2"),tapply(rt,cond,mean)))


# ROIs: crit: "verb" (a,b = "versteckt, c,d = "versteckt hat"), 
# postcrit: "verb1" (a,b = "und damit", c,d = "den Besuch")
# postcrit for a,b not presented together in Linger, therefore, merge here (verb1+verb2)

#datab<-subset(E5spr,cond!="c" & cond!="d")

#E5sprCRIT<-subset(datab,region=="verb")
#E5sprPOST1<-subset(datab,region=="verb1")

#E5sprPOST2<-subset(datab,region=="verb2")

#E5sprCRIT<-E5sprCRIT[,c(1,3,4,7,8)]
#E5sprPOST1<-E5sprPOST1[,c(1,3,4,7,8)]
#E5sprPOST2<-E5sprPOST2[,c(1,3,4,7,8)]

#E5sprPOST1$rt<-E5sprPOST1$rt + E5sprPOST2$rt

#datab<-rbind(E5sprCRIT,E5sprPOST1)

#datcd<-subset(E5spr,cond!="a" & cond!="b")
#datcd<-subset(datcd,region==c("verb","verb1"))   # "verb1 region",e.g. "den_Konkurs" was shown as one roi in Linger
#datcd<-datcd[,c(1,3,4,7,8)]

#E5spr<-rbind(datab,datcd)

#boxplot(log(rt)~cond,subset(E5spr,region=="verb1"))


# contrast coding
E5spr$load<-ifelse(E5spr$cond%in%c("a","b"),-1/2,1/2)
E5spr$dist<-ifelse(E5spr$cond%in%c("a","c"),-1/2,1/2)
E5spr$int<-ifelse(E5spr$cond%in%c("a","d"),-1/2,1/2)

                         ## ME LOAD ## ME DIST  ## INT
# a DAT-MC;     PP-SC      -0.5        -0.5      -0.5   (originally E1 LK13 cond c)
# b DAT-MC;     PP-MC      -0.5         0.5       0.5   (orininally E1 LK13 cond d)
# c DAT-MC emb; PP-SC emb   0.5        -0.5       0.5   (originally E2 LK13 cond c)
# d DAT-MC emb; PP-MC emb   0.5         0.5      -0.5   (originally E2 LK13 cond d)



# subset crit, postcrit
verb<-subset(E5spr,region=="verb")
verb1<-subset(E5spr,region=="verb1")


@

<<AnalysisE5RTcrit,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

subj <- as.numeric(as.factor(verb$subj))
#unique(verb$subj)
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(verb$item))
#unique(verb$item)
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verb))
Z_u <- unname(model.matrix(~ 1, verb))
Z_w <- unname(model.matrix(~ 1, verb))

attr(X, which="assign") <- NULL

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X),           
                P = ncol(X),               
                n_u = ncol(X),            
                n_w = ncol(X),              
                X = X,                      
                Z_u = X,                  
                Z_w = X,                  
                J = N_subj,
                K = N_items, 
                rt = verb$rt,                   
                subj = as.integer(subj),   
                item = as.integer(item))   

# 3. Fit the model.

E5 <- stan(file = "StanModels/maxModelmerged2.stan", 
                    data = stanDat,
                    iter = 2000, 
                    chains = 4)


#print(E5, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_E5, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E5_res<-stan_results(m=E5,params=c("Load","Dist","LoadxDist"))
@


<<AnalysisE5RTpostcrit,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

subj <- as.numeric(as.factor(verb1$subj))
#unique(verb1$subj)
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(verb1$item))
#unique(verb1$item)
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verb1))
Z_u <- unname(model.matrix(~ 1, verb1))
Z_w <- unname(model.matrix(~ 1, verb1))

attr(X, which="assign") <- NULL

# 2. Make Stan data (list)
stanDat <- list(N = nrow(X),           
                P = ncol(X),               
                n_u = ncol(X),            
                n_w = ncol(X),              
                X = X,                      
                Z_u = X,                  
                Z_w = X,                  
                J = N_subj,
                K = N_items, 
                rt = verb1$rt,                   
                subj = as.integer(subj),   
                item = as.integer(item))   

# 3. Fit the model.

E5post <- stan(file = "StanModels/maxModelmerged2.stan", 
                    data = stanDat,
                    iter = 2000, 
                    chains = 4)


#print(E5post, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_E5post, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E5post_res<-stan_results(m=E5post,params=c("Load","Dist","LoadxDist"))
@

<<sprmergedraw,echo=FALSE>>=
#msprmergedcritraw<-lmer(rt~load + dist + int+(1+load + dist + int||subj)+(1+load + dist + int||item),verb)
#msprmergedpostcritraw<-lmer(rt~load + dist + int+(1+load + dist + int||subj)+(1+load + dist + int||item),verb1)
@

<<E6ResponseAccdata,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=
dat<-read.table("../data/E6ETlevykellerExp12.txt",header=T)

# subset Accuracy data
datAcc<-subset(dat,roi==1)
#head(datAcc)
#str(datAcc)
 
### quesitons only follow 50% of items: remove -2 (RESPONSE ACC) or 0 (answer) (i.e., no questions) and NAs

# subset data filler items (exclude LKrep and practice)
datFillerAcc<-subset(datAcc,condition=="f" & answer!=0)
datFillerAcc<-na.omit(datFillerAcc)

# subset data experimental items (exclude fillers and practice)
datAcc<-subset(datAcc,condition!="f" & condition!="p" & answer!=0)
datAcc<-na.omit(datAcc)

# sanity checks
#xtabs(~subject+condition,datAcc)
#length(unique(datAcc$itemid))


# "acc" column: 
datAcc <- plyr::rename(datAcc, c(RESPONSE_ACCURACY="acc"))
datFillerAcc <- plyr::rename(datFillerAcc, c(RESPONSE_ACCURACY="acc"))
# mean acc exp items
#mean(datAcc$acc) # [1] 0.6448598
# mean acc for fillers
#mean(datFillerAcc$acc) # [1] 0.898773

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$condition, mean),2)
#print(mean_acc)

# contrast coding: 
datAcc$load<-ifelse(datAcc$condition%in%c("a","b"),-1/2,1/2)
datAcc$dist<-ifelse(datAcc$condition%in%c("a","c"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$condition%in%c("a","d"),-1/2,1/2)

E6meanacc<-round(mean(100*with(datAcc,tapply(acc,condition,mean)),na.rm=TRUE))
@

<<E6AnalysisAccuracy,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

subj <- as.numeric(as.factor(datAcc$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(datAcc$itemid))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + load + dist + int, datAcc))  
Z_u <- unname(model.matrix(~ 1, datAcc)) 
Z_w <- unname(model.matrix(~ 1, datAcc))

attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDatAcc <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                acc = datAcc$acc,                  
                subj = as.integer(subj),  
                item = as.integer(item))  


# 3. Fit the model.

E6Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAcc,
                  iter = 2000, 
                  chains = 4)


#print(mE6Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))

#Traceplot
#traceplot(mE6Acc, pars=c("beta","sigma_u","sigma_w"), inc_warmup=FALSE)

@

%Overall response accuracy for the experimental items was 62\% while accuracy for filler items was 89\%.

%\subsubsection{Eye-tracking reading measures}

<<E6RTData,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=


dat<-read.table("../data/E6ETlevykellerExp12.txt",header=T)

# contrast coding (same as for E5 SPR merged)
dat$load<-ifelse(dat$condition%in%c("a","b"),-1/2,1/2)
dat$dist<-ifelse(dat$condition%in%c("a","c"),-1/2,1/2)
dat$int<-ifelse(dat$condition%in%c("a","d"),-1/2,1/2)

#str(dat)
dat$roi<-factor(dat$roi)


## rois
#critical region (verb):
#a = 24 
#b = 24 
#c = 25 (merged roi 25 + 26)
#d = 25 (merged roi 25 + 26)

#precritical region (acc NP)
#a = 22 (merged 22 + 23)
#b = 22 (merged 22 + 23)
#c = 23 (merged 23 + 24)
#d = 23 (merged 23 + 24)

#postcritical region (varies: a/b = "und so/damit", c/d= "einen Verlust"/"Recht")
#a = 25 (merged 25 + 26)
#b = 25 (merged 25 + 26)
#c = 26 (merged 27 + 28) * expect for items 15,17,18,20 only region 27, now 26, (as only e.g. "Recht")
#d = 26 (merged 27 + 28) * same as above


# rois in ET (differ for conds a, b vs c,d)
region<-ifelse(dat$condition%in%c("a","b") & dat$roi==22,"npacc",
               ifelse(dat$condition%in%c("c","d") & dat$roi==23,"npacc",
                ifelse(dat$condition%in%c("a","b") & dat$roi==24,"verb",
                    ifelse(dat$condition%in%c("c","d") & dat$roi==25,"verb",   
                              ifelse(dat$condition%in%c("a","b") & dat$roi==25,"verb1",
                    ifelse(dat$condition%in%c("c","d") & dat$roi==26,"verb1","noncritical"))))))

dat$region<-region
dat<-subset(dat,region!="noncritical")
dat$region<-factor(dat$region)

# check data:
#dim(dat)
#str(dat)
#xtabs(~subject+region,dat)
#xtabs(~subject+itemid,dat)
#xtabs(~itemid+region,dat)

# subset critical region 
verb <-(subset(dat,region=="verb"))
verb1<-subset(dat,region=="verb1")
@


<<AnalysisE6TFTcrit,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

verbTFT <- subset(verb,verb$TFT>0)

subj <- as.numeric(as.factor(verbTFT$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(verbTFT$itemid))
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verbTFT))  
Z_u <- unname(model.matrix(~ 1, verbTFT)) 
Z_w <- unname(model.matrix(~ 1, verbTFT))

attr(X, which="assign") <- NULL   

# 2. Make Stan data.
stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(X),  
                n_w = ncol(X),    
                X = X,           
                Z_u = X,         
                Z_w = X,         
                J = N_subj,
                K = N_items, 
                rt = verbTFT$TFT,                                                                      
                subj = as.integer(subj), 
                item = as.integer(item))

# 3. Fit the model.

E6_3 <- stan(file = "StanModels/maxModelmerged2.stan", 
                  data = stanDat,
                  iter = 2000, 
                 chains = 4)

# print parameters
#print(E6_3, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_E6_3, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E6_3_res<-stan_results(m=E6_3,params=c("Load","Dist","LoadxDist"))
@

<<E6etcritraw,echo=FALSE>>=
#mE6etcritraw<-lmer(TFT~load + dist + int+(load + dist + int||subject)+(load + dist + int||itemid),verbTFT)
@

<<AnalysisE6TFTpostcrit,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

verb1TFT <- subset(verb1,verb1$TFT>0)

subj <- as.numeric(as.factor(verb1TFT$subject))
#unique(verb1TFT$subject)
#unique(subj)
N_subj <- length(unique(subj))

#unique(verb1TFT$itemid)
item <- as.numeric(as.factor(verb1TFT$itemid))
#unique(item)
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verb1TFT))  
Z_u <- unname(model.matrix(~ 1, verb1TFT)) 
Z_w <- unname(model.matrix(~ 1, verb1TFT))

attr(X, which="assign") <- NULL   

# 2. Make Stan data.
stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(X),  
                n_w = ncol(X),    
                X = X,           
                Z_u = X,         
                Z_w = X,         
                J = N_subj,
                K = N_items, 
                rt = verb1TFT$TFT,                                                                     
                subj = as.integer(subj), 
                item = as.integer(item))

# 3. Fit the model.

E6_3post <- stan(file = "StanModels/maxModelmerged2.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

# print parameters
#print(m_E6_3post, pars=c("beta","sigma_e","sigma_u","sigma_w"), probs=c(.025,.5,.975))

# Traceplot
#traceplot(m_E6_3post, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E6_3post_res<-stan_results(m=E6_3post,params=c("Load","Dist","LoadxDist"))
@

<<E6etpostcritraw,echo=FALSE>>=
#mE6etpostcritraw<-lmer(TFT~load + dist + int+(load + dist + int||subject)+(load + dist + int||itemid),verb1TFT)
@


<<E7AccData,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=

dat<-read.table("../data/data_LK13rep100subj.txt",header=T)

# subset Accuracy data
datAcc<-subset(dat,roi==1)
#head(datAcc)
#str(datAcc)
 
### quesitons only follow 50% of items: remove -2 (RESPONSE ACC) or 0 (answer) (i.e., no questions) and NAs

# subset data filler items (exclude LKrep and practice)
datFillerAcc<-subset(datAcc,condition=="f" & answer!=0)
datFillerAcc<-na.omit(datFillerAcc)

# subset data experimental items (exclude fillers and practice)
datAcc<-subset(datAcc,condition!="f" & condition!="p" & answer!=0)
datAcc<-na.omit(datAcc)

# sanity checks
#xtabs(~subject+condition,datAcc)
#length(unique(datAcc$itemid))


# "acc" column: 
datAcc <- plyr::rename(datAcc, c(RESPONSE_ACCURACY="acc"))
datFillerAcc <- plyr::rename(datFillerAcc, c(RESPONSE_ACCURACY="acc"))
# mean acc exp items
#mean(datAcc$acc) # [1] 0.6448598
# mean acc for fillers
#mean(datFillerAcc$acc) # [1] 0.898773

# compute means accuracies (condition)
#mean_acc<-round(tapply(datAcc$acc, datAcc$condition, mean),2)
#print(mean_acc)

# contrast coding: 
datAcc$load<-ifelse(datAcc$condition%in%c("a","b"),-1/2,1/2)
datAcc$dist<-ifelse(datAcc$condition%in%c("a","c"),-1/2,1/2)
datAcc$int<-ifelse(datAcc$condition%in%c("a","d"),-1/2,1/2)

E7meanacc<-round(mean(100*with(datAcc,tapply(acc,condition,mean)),na.rm=TRUE))

@



<<E7AnalysisAcc,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=

subj <- as.numeric(as.factor(datAcc$subject))
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(datAcc$itemid))
N_items <- length(unique(item))


X <- unname(model.matrix(~ 1 + load + dist + int, datAcc))  
Z_u <- unname(model.matrix(~ 1, datAcc)) 
Z_w <- unname(model.matrix(~ 1, datAcc))

attr(X, which="assign") <- NULL              

# 2. Make Stan data (list)
stanDatAcc <- list(N = nrow(X), 
                P = ncol(X), 
                n_u = ncol(X),
                n_w = ncol(X),
                X = X,       
                Z_u = X,    
                Z_w = X,      
                J = N_subj, 
                K = N_items,
                acc = datAcc$acc,                  
                subj = as.integer(subj),  
                item = as.integer(item))  


# 3. Fit the model.

mE7Acc <- stan(file = "StanModels/responseaccmaximal.stan", 
                  data = stanDatAcc,
                  iter = 2000, 
                  chains = 4)


#print(mE7Acc, pars=c("beta","sigma_u","sigma_w"), probs=c(.025,.5,.975))

#Traceplot
#traceplot(mE7Acc, pars=c("beta","sigma_u","sigma_w"), inc_warmup=FALSE)

@

%Overall response accuracy for Experiment 7 was approximately 64\% for experimental items and 90\% for filler items. 


%\subsubsection{Eye-tracking reading measures}

<<E7rtData,include=FALSE,cache=FALSE,echo=FALSE,warning=FALSE>>=

dat<-read.table("../data/data_LK13rep100subj.txt",header=T)

dat<-subset(dat,condition!="f" & condition!="p")

# contrast coding (same as for E5 SPR merged)
dat$load<-ifelse(dat$condition%in%c("a","b"),-1/2,1/2)
dat$dist<-ifelse(dat$condition%in%c("a","c"),-1/2,1/2)
dat$int<-ifelse(dat$condition%in%c("a","d"),-1/2,1/2)

## nested contrasts
dat$lodist<-ifelse(dat$condition=="a",-1/2,
                   ifelse(dat$condition=="b",1/2,0))
dat$hidist<-ifelse(dat$condition=="c",-1/2,
                   ifelse(dat$condition=="d",1/2,0))

#xtabs(~condition+hidist,dat)


#str(dat)
#head(dat)
#dat$roi<-factor(dat$roi)

# rois in ET (differ for conds a, b vs c,d)
region<-ifelse(dat$condition%in%c("a","b") & dat$roi==22,"npacc",
               ifelse(dat$condition%in%c("c","d") & dat$roi==23,"npacc",
                ifelse(dat$condition%in%c("a","b") & dat$roi==24,"verb",
                    ifelse(dat$condition%in%c("c","d") & dat$roi==25,"verb",   
                              ifelse(dat$condition%in%c("a","b") & dat$roi==25,"verb1",
                    ifelse(dat$condition%in%c("c","d") & dat$roi==26,"verb1","noncritical"))))))

#length(region)  

dat$region<-region
dat<-subset(dat,region!="noncritical")
dat$region<-factor(dat$region)


# check data:
#xtabs(~subject+region,dat)    # subj 41 (bad cali) saw rois x 19 (instead of 24)
#xtabs(~subject+itemid,dat)
#xtabs(~itemid+region,dat)

#test1<-subset(dat,dat$itemid==6)

#xtabs(~subject+itemid,test1)
#xtabs(~subject+condition,test1)
#xtabs(~itemid+condition,test1)

#subset critical region 
verb <-subset(dat,region=="verb")

## quick sanity check to see if the conventional analysis would have found anything that TRT could not tell you, answer is no:
#summary(mFPRT<-lmer(log(FPRT)~load+dist+int+(1|subject)+(1|itemid),subset(verb,FPRT>0)))
#summary(mRPD<-lmer(log(RPD)~load+dist+int+(1|subject)+(1|itemid),subset(verb,RPD>0)))
#summary(mRRT<-lmer(log(RRT)~load+dist+int+(1|subject)+(1|itemid),subset(verb,RRT>0)))
## summary(mTFT<-lmer(log(TFT)~load+dist+int+(1|subject)+(1|itemid),subset(verb,TFT>0)))

#summary(mTFT<-lmer(log(TFT)~load+lodist+hidist+(1|subject)+(1|itemid),subset(verb,TFT>0)))

#summary(m)
#subset postcritical region 
verb1 <-subset(dat,region=="verb1")

#xtabs(~subject+condition,verb)
#xtabs(~subject+condition,verb1)
@

<<AnalysisE7TFTcrit,include=FALSE,results='hide',cache=TRUE,echo=FALSE,warning=FALSE>>=

verbTFT <- subset(verb,verb$TFT>0)
#head(verb)

subj <- as.numeric(as.factor(verbTFT$subject))
#unique(verbTFT$subject)
#unique(subj)
N_subj <- length(unique(subj))

#unique(verbTFT$itemid)
item <- as.numeric(as.factor(verbTFT$itemid))
#unique(item)
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verbTFT))  
attr(X, which="assign") <- NULL   

# 2. Make Stan data.
stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(X),  
                n_w = ncol(X),    
                X = X,           
                Z_u = X,         
                Z_w = X,         
                J = N_subj,
                K = N_items, 
                rt = verbTFT$TFT,                                                                      
                subj = as.integer(subj), 
                item = as.integer(item))


# 3. Fit the model.

E7_3 <- stan(file = "StanModels/maxModelmerged2.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

# print parameters
#print(E7_3, pars=c("beta","sigma_e","sigma_u","sigma_w"), digits=4, probs=c(.025,.5,.975))

# check
#m13 <- lmer(log(TFT)~load+dist+int+(1+load+dist+int||subject)+(1+load+dist+int||itemid),verbTFT)
#summary(m13)

# Traceplot
#traceplot(m_E7_3, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E7_3_res<-stan_results(m=E7_3,params=c("Load","Dist","LoadxDist"))

#round(with(verbTFT,tapply(TFT,condition,mean)))
@

<<E7etcritraw,echo=FALSE>>=
#mE7etcritraw<-lmer(TFT~load + dist + int+(load + dist + int||subject)+(load + dist + int||itemid),verbTFT)
@

<<E7etpostcritraw,echo=FALSE>>=
#mE7etpostcritraw<-lmer(TFT~load + dist + int+(load + dist + int||subject)+(load + dist + int||itemid),verb1TFT)
@


<<AnalysisE7TFTcritsmallsample,echo=FALSE,eval=FALSE>>=
nsim<-50
estimates<-matrix(rep(NA,nsim*3*3),ncol=9)
subjID<-unique(verbTFT$subj)

for(i in 1:nsim){
small_sample<-sample(subjID,28)
dsmall<-subset(verbTFT,subject%in%c(small_sample))
stanDatsmall<-createStanDat(d=dsmall,rt=dsmall$TFT,
              form=as.formula("~1+load+dist+int"))
#str(standatsmall)
E7_3small <- stan(file = "StanModels/maxModelmerged2.stan", 
                  data = stanDatsmall,
                  iter = 2000, 
                  chains = 4)
estimates[i,]<-as.vector(stan_results(m=E7_3small,params=c("Load","Dist","LoadxDist")))
}
save(estimates,file="../data/smallsamplesestimates.Rda")
@

<<AnalysisE7TFTcritnested,include=FALSE,results='hide',cache=TRUE,echo=FALSE,warning=FALSE>>=
## nested:
X <- unname(model.matrix(~ 1 + load + lodist + hidist, verbTFT))  
attr(X, which="assign") <- NULL   
#head(X)
Z_u <- Z_w <- X
#  unname(model.matrix(~ 1 , verbTFT)) 
#attr(Z_u, which="assign") <- NULL   
#attr(Z_w, which="assign") <- NULL   



stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(Z_u),  
                n_w = ncol(Z_w),    
                X = X,           
                Z_u = Z_u,         
                Z_w = Z_w,         
                J = N_subj,
                K = N_items, 
                rt = verbTFT$TFT,               
                subj = as.integer(subj), 
                item = as.integer(item))

#str(stanDat)

E7_3nested <- stan(file = "StanModels/maxModelmerged2nested.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

E7_3nested_res<-stan_results(m=E7_3nested,params=c("Load","LoDist","HiDist"))
@


<<AnalysisE7TFTpostcrit,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE>>=
verb1TFT <- subset(verb1,verb1$TFT>0)
#head(verb)
#round(with(verb1TFT,tapply(TFT,condition,mean)))


subj <- as.numeric(as.factor(verb1TFT$subject))
#unique(verb1TFT$subject)
#unique(subj)
N_subj <- length(unique(subj))

item <- as.numeric(as.factor(verb1TFT$itemid))
#unique(item)
N_items <- length(unique(item))

X <- unname(model.matrix(~ 1 + load + dist + int, verb1TFT))  

attr(X, which="assign") <- NULL   

# 2. Make Stan data.
stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(X),  
                n_w = ncol(X),    
                X = X,           
                Z_u = X,         
                Z_w = X,         
                J = N_subj,
                K = N_items, 
                rt = verb1TFT$TFT,               
                subj = as.integer(subj), 
                item = as.integer(item))


# 3. Fit the model.
#library(rstan)
#rstan_options(auto_write = TRUE)
#options(mc.cores = parallel::detectCores())

E7_3post <- stan(file = "StanModels/maxModelmerged2.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

# print parameters
#print(E7_3post, pars=c("beta","sigma_e","sigma_u","sigma_w"), digits=4, probs=c(.025,.5,.975))

# check
#m13 <- lmer(log(TFT)~load+dist+int+(1+load+dist+int||subject)+(1+load+dist+int||itemid),verb1TFT)
#m13raw <- lmer(TFT~load+dist+int+(1+load+dist+int||subject)+(1+load+dist+int||itemid),verb1TFT)
#summary(m13raw)

# Traceplot
#traceplot(m_E7_3post, pars=c("beta","sigma_e","sigma_u","sigma_w"), inc_warmup=FALSE)

# extract the model estimates
E7_3post_res<-stan_results(m=E7_3post,params=c("Load","Dist","LoadxDist"))

@

<<AnalysisE7TFTpostcritnested,include=FALSE,results='hide',cache=TRUE,echo=FALSE,warning=FALSE>>=
## nested:
X <- unname(model.matrix(~ 1 + load + lodist + hidist, verb1TFT))  
attr(X, which="assign") <- NULL   

Z_u <- Z_w <- X

stanDat <- list(N = nrow(X),    
                P = ncol(X), 
                n_u = ncol(Z_u),  
                n_w = ncol(Z_w),    
                X = X,           
                Z_u = Z_u,         
                Z_w = Z_w,         
                J = N_subj,
                K = N_items, 
                rt = verb1TFT$TFT,                                subj = as.integer(subj), 
                item = as.integer(item))

E7_3postnested <- stan(file = "StanModels/maxModelmerged2nested.stan", 
                  data = stanDat,
                  iter = 2000, 
                  chains = 4)

E7_3postnested_res<-stan_results(m=E7_3postnested,params=c("Load","LoDist","HiDist"))
@


LK describe the Load-Distance interaction in their General Discussion in the following manner:

\begin{quote}
``[Experiment 1 showed] that the presence of a dative noun phrase led to decreased reading time at the corresponding verb, compared to a condition in which there is no preceding dative noun phrase.

``Experiment 2 showed an interaction of adjunct position and dative position, with the verb more difficult to process when both the adjunct and the dative phrase were present than when only one was present.

``[O]urs is the first demonstration to our knowledge that both expectation and locality effects can occur in the same structure in the same language, and that the two effects interact with each other.''
\end{quote}

This claimed interaction between expectation and locality across the two experiments can be investigated in several different ways. One way to interpret the interaction is in terms of the contrast in reading time patterns in their Experiment 1 vs.\ 2. 
LK's Figures 3 and 4 \parencite[][pp.\ 209, 214]{levy2013expectation}, which summarize total reading times at the critical region, clearly show that Experiment 1 exhibits a speedup in (d) vs.\ (c), whereas Experiment 2 exhibits a slowdown in these conditions (see our Tables~\ref{itemsE1LK} and \ref{itemsE2LK} for the items). 
Although visual inspection of the figures does suggest a cross-over interaction between Load and Distance, 
as \textcite{nieuwenhuis2011erroneous} have pointed out, 
 the interaction must be formally tested. Such an interaction would allow us to conclude, as LK did, that ``\textit{\dots both expectation and locality effects can occur in the same structure in the same language, and that the two effects interact with each other}''. LK did investigate the expectation-locality interaction in their Experiment 2, but the claim to be investigated involves the patterns seen across their Experiments 1 and 2, and this was not checked. We evaluate this claimed interaction next.

\noindent
\doublespacing
\begin{table}[htbp]
\caption{Example sentences (simplified) for investigating the Load-Distance interaction by combining the conditions (c) and (d) of LK's Experiment 1 and of Experiment 2. The abbreviations mean the following: ADJ: adjunct; DAT: dative; PP: prepositional phrase; NP: noun phrase.} \label{tab:combineditems}
\setlength{\extrarowheight}{2pt}
{\scriptsize
%\resizebox{\textwidth}{!}{
\setlength\tabcolsep{1.5pt}
\begin{tabular}{l l l l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}[3pt][0pt]}l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}}l l l >{\columncolor[gray]{.9}}l >{\columncolor[gray]{.9}}l l l l l l l l l}
\multicolumn{11}{l}{\textbf{a [E1 c].   PP adjunct in subordinate clause, dative NP in main clause}}\\
Nachdem & der & Lehrer & & [\textbf{ADJ} zur & Ahndung] & & & \dots,\\ 
\textit{After} & \textit{the} & \textit{teacher} & & [\textbf{ADJ} \textit{as} & \textit{payback}] &  & & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} &     & & & [\textbf{DAT} dem & Sohn] &  den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}},& & und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}}& \textcolor{black}{\textit{Gerstner}} &   & & & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & & \textit{and} & \textit{thus}\dots\\
\multicolumn{11}{l}{\textbf{b [E1 d]. PP adjunct in main clause, dative NP in main clause}}\\
Nachdem & der & Lehrer & &  &  & & & \dots,\\ 
 \textit{After} & \textit{the} & \textit{teacher} & &  &  & & & \dots,\\ \addlinespace[2pt]
hat & \textcolor{black}{Hans} & \textcolor{black}{Gerstner} & & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}}, & & und & somit\dots\\
\textit{has} & \textcolor{black}{\textit{Hans}}& \textcolor{black}{\textit{Gerstner}} & & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}}, & & \textit{and} & \textit{thus}\dots \\
\multicolumn{11}{l}{\textbf{c [E2 c].  PP adjunct in subordinate clause, dative NP in relative clause}}\\
Nachdem & der & Lehrer & & [\textbf{ADJ} zur & Ahndung] & & & \dots,\\
\textit{After} & \textit{the} & \textit{teacher} & & [\textbf{ADJ} \textit{as} & \textit{payback}] &  & & \dots, \\ \addlinespace[2pt]
hat & \textcolor{black}{der} & \textcolor{black}{Mitsch\"uler}, & der &    & & [\textbf{DAT} dem & Sohn] &  den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}},& die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}} & \textcolor{black}{\textit{classmate}}, & \textit{who} & & & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots\\
\multicolumn{11}{l}{\textbf{d [E2 d]. PP adjunct in relative clause, dative NP in relative clause}}\\
Nachdem & der & Lehrer & & &  & & & \dots,\\
 \textit{After} & \textit{the} & \textit{teacher} & &  & & & & \dots, \\ \addlinespace[2pt]
 hat & \textcolor{black}{der} & \textcolor{black}{Mitsch\"uler}, & der & [\textbf{ADJ} zur & Ahndung] & [\textbf{DAT} dem & Sohn] & den & Fu{\ss}ball & \textcolor{black}{\textbf{versteckt}} & \textcolor{black}{\textbf{hat}}, & die & Sache\dots\\
\textit{has} & \textcolor{black}{\textit{the}} & \textcolor{black}{\textit{classmate}},  & \textit{who} & [\textbf{ADJ} \textit{as} & \textit{payback}] & [\textbf{DAT} \textit{the} & \textit{son}] & \textit{the} & \textit{football} & \textcolor{black}{\textit{hidden}} & \textcolor{black}{\textit{had}}, & \textit{the} & \textit{affair}\dots
\end{tabular}}
%}
\label{itemsE12}
\begin{footnotesize}
\bigskip
\begin{tablenotes}
\item \textit{`After the teacher imposed detention classes, Hans Gerstner/the classmate (who) hid the football from the naughty son of the industrious janitor as additional payback for the multiple wrongdoings corrected the affair.'}
\end{tablenotes}
\end{footnotesize}
\end{table}

\restoregeometry

\subsubsection{Re-analysis of conditions (c) and (d) of LK's Experiments 1 and 2}
 
We investigated the interaction statistically by combining the original LK data from conditions (c) and (d) of each experiment; see Table~\ref{itemsE12} for the design. This analysis tested for the main effects of Load, Distance, and their interaction. As shown in Table~\ref{tab:meintcontrasts2}, a positive coefficient for Load would imply that processing a verb within a relative clause is more difficult than in a main clause; note that this effect is not interesting because the verb phrase (\textit{versteckt hat}) in conditions (c) and (d) of Experiment 2 is longer than the verb phrase (\textit{versteckt}) in conditions (c) and (d) of Experiment 1.
More interesting is the effect of Distance. A positive coefficient for Distance would imply that increasing subject-verb distance by interposing an adjunct (which contains a new discourse referent) in addition to a dative NP will lead to longer reading times at the verb; this is as predicted by memory-based accounts such as the Dependency Locality Theory \parencite{gibson00}.
A negative sign would support the expectation-based account of \textcite{levy08}, as discussed earlier. Finally, a negative coefficient for the Load-Distance interaction would confirm the cross-over interaction seen visually in Figures 3 and 4 of LK's paper: interposing a dative NP and an adjunct vs.\ a dative NP alone should lead to a slowdown only in the relative clause conditions.

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{lrrrr}
Condition          &  Load &  Dist &  Load$\times$Dist \\
 E1 c \dots  [$_{MC}$ Subj \dots DAT ~~~~~~~ \dots Verb]   &  -0.5    &     -0.5    &   -0.5\\    
 E1 d \dots  [$_{MC}$ Subj \dots DAT ADJ \dots Verb]  &  -0.5    &    0.5    &  0.5\\
 E2 c \dots  [$_{RC}$ Subj \dots DAT ~~~~~~~ \dots Verb]  & 0.5    &     -0.5    &   0.5\\
 E2 d \dots  [$_{RC}$ Subj \dots DAT ADJ \dots Verb]  & 0.5    &    0.5    &    -0.5\\
\end{tabular}
\end{center}
\caption{The contrast coding used for main effects of Load, Dist(ance), and their interaction in the two experiments by Levy and Keller (2013). The first two conditions here are conditions (c) and (d) of Experiment 1, and the last two conditions are conditions (c) and (d) of Experiment 2.}\label{tab:meintcontrasts2}
\end{table}%



\subsubsection{Results: The Load-Distance interaction in the LK data}

As shown in Figure~\ref{fig:3}, in the LK data  
the estimates for the interaction in the critical region are 
\Sexpr{round(LKmerged_res[3,1])}~ms [\Sexpr{round(LKmerged_res[3,2])},\Sexpr{round(LKmerged_res[3,3])}]; and in the post-critical region, \Sexpr{round(LKmergedpost_res[3,1])}~ms [\Sexpr{round(LKmergedpost_res[3,2])},\Sexpr{round(LKmergedpost_res[3,3])}]. Here again, even though the interaction has the predicted sign, we have very noisy estimates; the credible intervals have a width of about 100 ms. If a significance test were to be conducted, the interaction would not come out significant. 
However, significance is not interesting for us. 
We wanted to know whether we can obtain estimates for the Load-Distance interaction in our replication attempts that have the same sign as the original LK experiments, and whether our estimates are plausible given the wide credible intervals in the LK data. 
%As before, we had 28 participants for each of the two experiments, a self-paced reading study and an eyetracking study. We also carried out a third, larger-sample study (100 participants) to illustrate what changes when we have higher precision estimates.

\subsection{Experiments 5, 6: Replication attempts of the Load-Distance interaction}

We carried out two attempts to reproduce the Load-Distance interaction.
As discussed above, we designed the experiment to pit Load and Distance against each other by taking conditions (c) and (d) of the original LK Experiment 1 (which we will refer to as the low memory load conditions) and conditions (c) and (d) of  Experiment 2 (high memory load conditions). We conducted a self-paced reading study and an eyetracking study, each with the same sample size as the original experiments (28 participants, 24 items). The procedure was as described for the preceding studies. 



\begin{figure}[!htbp]
\centering
<<figurese3,echo=FALSE,include=TRUE,warning=FALSE,message=FALSE>>=
cond<-factor(c("Load","Dist","LoadxDist"),levels=c("Load","Dist","LoadxDist"))
cond<-rep(cond,4)
expt<-factor(c(rep("Original LK data",3),
        rep("E5, SPR",3),rep("E6, ET",3),rep("E7, ET (n=100)",3)),levels=c("Original LK data","E5, SPR","E6, ET","E7, ET (n=100)"))
  

rownames(LKmerged_res)<-NULL
rownames(E5_res)<-NULL
rownames(E6_3_res)<-NULL
rownames(E7_3_res)<-NULL

LKmergedall<-data.frame(rbind(LKmerged_res,E5_res,E6_3_res,E7_3_res))
LKmergedall<-data.frame(cond=cond,expt=expt,LKmergedall)


p1<-plotresults(LKmergedall,maintitle="Load vs. Distance (critical region)",,cols=c("darkgray", "black","black","black"),legendposition=c(0.8,0.75),lowery=-150,uppery=400)

rownames(LKmergedpost_res)<-NULL
rownames(E5post_res)<-NULL
rownames(E6_3post_res)<-NULL
rownames(E7_3post_res)<-NULL

LKmergedpostall<-data.frame(rbind(LKmergedpost_res,E5post_res,
                                  E6_3post_res,E7_3post_res))
LKmergedpostall<-data.frame(cond=cond,expt=expt,LKmergedpostall)
p2<-plotresults(LKmergedpostall,maintitle="Load vs. Distance (post-critical region)",cols=c("darkgray", "black","black","black"),removelegend=TRUE,lowery=-150,uppery=400)
multiplot(p1,p2,cols=1)
@
\caption{Load and Distance effects at the critical and post-critical regions. Shown are the mean and 95\% credible intervals from conditions (c) and (d) of the two original LK Experiments 1 and 2; and from our three replication attempts (Expts 5-7). SPR stands for self-paced reading, and ET stands for eyetracking.}\label{fig:3}
\end{figure}


As shown in Figure~\ref{fig:3}, 
both replication attempts showed that the estimate for Load in the critical region had a positive sign:

\begin{itemize}
\item Expt 5 (SPR):
\Sexpr{round(E5_res[1,1])}~ms [\Sexpr{round(E5_res[1,2])},\Sexpr{round(E5_res[1,3])}] 
\item 
Expt 6 (ET, total reading times)  \Sexpr{round(E6_3_res[1,1])}~ms [\Sexpr{round(E6_3_res[1,2])},\Sexpr{round(E6_3_res[1,3])}]. 
\end{itemize}

These effects suggest that 
 increasing load (the relative clause conditions (c) and (d) in Table~\ref{tab:combineditems}) leads to increased processing difficulty. 
However, recall that the effect of Load is not interesting because the verb length differs in the two sets of conditions. Differently put, the Load effect could at least partly be due to the word length effect.
Therefore, we disregard the Load effect, even though theoretically the sign of the effect makes sense under the LK account.

The estimate for Distance is close to 0~ms:  
\Sexpr{round(E5_res[2,1])}~ms [\Sexpr{round(E5_res[2,2])},\Sexpr{round(E5_res[2,3])}] in Expt 5; 
and  \Sexpr{round(E6_3_res[2,1])}~ms [\Sexpr{round(E6_3_res[2,2])},\Sexpr{round(E6_3_res[2,3])}] in Expt 6. 
Finally, the interaction between Load and Distance is not far from 0~ms;
 \Sexpr{round(E5_res[3,1])}~ms [\Sexpr{round(E5_res[3,2])},\Sexpr{round(E5_res[3,3])}] in Expt 5, and \Sexpr{round(E6_3_res[3,1])}~ms [\Sexpr{round(E6_3_res[3,2])},\Sexpr{round(E6_3_res[3,3])}] in Expt 6. 

An interesting question arises here. If we were to run the experiment with a larger sample size, would we perhaps detect the Load-Distance interaction? After all, the interaction claimed by LK is very well-motivated both theoretically and empirically. We turn to this larger-sample study next.

\subsection{Expt 7 (Eyetracking): A larger-sample replication attempt of the Load-Distance interaction}

Before we discuss the results of Experiment 7, we first  explain how we decided on sample size. We used an approach that \textcite{kruschke2014doing} refers to as the region of practical equivalence (ROPE). Below, we also discuss how the ROPE approach can be used to make decisions about the research question.


\subsubsection{Determining sample size using a Bayesian approach} \label{rope}

The Bayesian framework allows us to incrementally determine how many participants we should run in order to make a decision about our research question. One way to do this is to define what constitutes ``no effect'' as a region rather than a point value. This approach was developed  in the context of clinical trials, where it is essential to stop the trial if the treatment is turning out to harm the patients, or when it is immediately clear that the treatment is superior to the control \parencites{cornfield1966sequential,armitage1989inference,spiegelhalter2004bayesian,berry2010bayesian,freedman1984stopping,spiegelhalter1994bayesian}. \textcite{kruschke2014doing} re-introduced this idea into psychology, but it has not yet been widely adopted. This approach serves both as a stopping rule, and for deciding whether one has evidence for one's theory. We will use Kruschke's terminology here. 

As mentioned above, the starting point is to define what counts as ``no effect.'' Instead of the frequentist approach of asserting a point null value, we can define a \textit{region of practical equivalence} that counts as a null region. For example, in LK Experiment 1, we start by asserting that in total reading times, what we count as ``no effect'' is a range of possible values:  $-20$ to $20$ ms. This range can be seen as representing a 95\% credible interval over a distribution (say a normal distribution with mean 0) of plausible values.
Note that if we were investigating first-pass reading times, the range would be much smaller, because effects in first-pass reading time will be smaller in magnitude. 

How did we decide on the width of $40$ ms for the region of practical equivalence? This decision is subjective but not arbitrary. It is based on estimates derived from what is already known and well-established empirically.\footnote{This estimation approach is sometimes called Fermi-zation \parencite{tetlock2016superforecasting}. The name comes from Fermi's skill in obtaining rough but  accurate estimates for physical phenomena; an example is the 1945 nuclear detonation conducted as part of the Manhattan project (the Trinity test). Fermi obtained remarkably accurate estimates of the blast's force before the data were available. The essential point here is to use the information available to arrive at reasonable estimates; this is not very different from the elicitation of expert opinion in Bayesian data analysis of clinical data \parencites{ohagan2006uncertain,morris2014web}.} 
For clear grammaticality violations that the reader is immediately consciously aware of, total reading time effects (at the word where the ungrammaticality is  detected) can show effect magnitudes of approximately $100$ to $150$ ms. For example, the data in  \textcite{Dillon-EtAl-2013} (their Experiment 1) showed a $41$ ms $[23, 58]$ effect of ungrammaticality (n=40) in 
first-pass reading time (FPRT),  and a $100$ ms $[69, 134]$  effect in  total reading time (TRT). In a large-sample (n=181) replication attempt of Dillon et al.'s Experiment 1 \parencite{JaegerMertzenVanDykeVasishth2018}, we found an effect of $55$ ms $[45, 65]$  in FPRT, and an effect of $121$ ms  $[100, 141]$  in TRT. We consistently find this magnitude of effect or smaller effects when the sentence is ungrammatical; for example, 
 \textcite{wagersetal} and \textcite{lago2015agreement} also showed the effect of (un)grammaticality in SPR with estimates similar to those found by Dillon and colleagues. Sometimes we see even larger effects for ungrammaticality; for example, an eyetracking study by \textcite{paapehemforthvasishth2018} found that total reading times at the moment that an ungrammaticality was registered in French was $176$ ms, with 95\% credible intervals $84$ and $264$ ms.
Now, if we consider more subtle experimental manipulations in sentence processing, the effects in total reading time are likely to be in a lower range than effects of grammaticality.  As an example, we mentioned earlier that a meta-analysis showed that the similarity-based interference effects found by Van Dyke and colleagues have a posterior mean of about $13$ ms, with 95\% credible intervals $[2,28]$ ms. Since these estimates were based on SPR data and first-pass reading times (FPRT), it is reasonable to assume that in total reading times (TRT) the effect of interference would be larger; from experience with eyetracking data, we can say that the effect is approximately twice as large, i.e., $30$ ms (TRTs are a sum of FPRT and re-reading times, so they are bound to be larger than FPRT).
Given these assumptions, for TRT we fixed $\pm 20$ ms around $0$ ms as counting as effectively a null effect for the LK studies. 

Our estimates of the region of practical equivalence (ROPE) are based on an empirical argument, but are of course open to challenge. We cannot provide a one-size-fits-all recommendation for deciding on a null region for specific phenomena, but we believe that for the present question, our estimates are reasonable. For subtle phenomena for which no data exist, some initial experiments could be used to establish a ROPE, and/or quantitative predictions from a computational process model could be used as a guide \parencite[an example is discussed in][]{EngelmannJaegerVasishthSubmitted2018}.

Once we have decided on a null region, the goal should be to collect data until the 95\% credible interval of the parameter of interest is at most as wide as the null region; in the above example, it should be at most $40$ ms wide. 
This is how we established our stopping rule in our pre-registration of the larger-sample study (which is available from: https://osf.io/eyphj/). Note that, unlike the frequentist power analysis, we do not fix a sample size in advance, but rather run the experiment until a certain precision is reached: until the 95\% credible interval of the posterior distribution has width $40$ ms or less.

For interpreting the results, the ROPE method can be used as follows. As shown in Figure~\ref{fig:precision} \parencite[adapted from][p.\ 184]{spiegelhalter2004bayesian}, once the data with the pre-determined precision have been collected, there are five possible scenarios. For illustration purposes, we assume that a positive sign on a parameter  validates some theory X, and a negative sign validates a competing theory Y. A concrete example of such opposing predictions is the expectation vs.\ locality question discussed by LK in their paper.

The five outcome scenarios are as follows:

\begin{itemize}
\item
A, B: data's credible interval falls clearly outside the null region. Decision: reject the null region, and conclude that theory X or Y is validated (depending on the sign).
\item
C,D: data's credible interval overlaps with the null region. Decision: if the sign is positive, reject theory Y; if the sign is negative, reject theory X. 
\item
E: data's credible interval falls within the null region. Decision: conclude that the data are consistent with ``no effect.''  Note that we do not say here that the decision is that we have ``proved'' that the null is true, but merely that the data are consistent with the posited ROPE. Only direct replications can establish whether an estimate and its 95\% credible interval consistently falls within the ROPE. 
\end{itemize}

Incidentally, the ROPE method can also be used for affirming a theory's predictions, if the theory makes quantitative predictions. A stringent test of a theory's predictions would be that the posterior's credible interval falls within the range predicted by theory; weaker evidence for a theory would involve overlap with the predicted range of values; and a rejection of a theory would involve a credible interval from data that falls completely outside a predicted range of values. In the General Discussion, we give an example of how ROPE can be used for model evaluation.

An obvious objection to the ROPE approach is its subjectivity. One can empirically justify a region of practical equivalence, but different researchers could define different regions of equivalence. But this is no worse than the way NHST is used; subjective decisions are routinely taken in NHST and there are no fixed standards for these \parencite{chambers2017seven}. Another obvious objection is that the ROPE approach can be misused. For example, one could first run the study and compute the standard error and then retroactively define the null region as four times the estimated standard error. However, we are assuming here that the definition of the null region will be decided on before the experiment is conducted---this is the same in NHST, where a prospective power calculation must be done before conducting the study.

Finally, this null region approach does not solve the problem of demonstrating replicability; whatever the outcome of an experiment, one would still need to replicate the effect. The only way to establish replicability is to actually conduct pre-registered direct replications. We  discuss pre-registration and replication in the general discussion.

	\begin{figure}
		\centering
		%\scalebox{0.7}{
			\begin{tikzpicture}
			%\tikz{
			\node[](A) {A};
			\node[right of=A, xshift=5.8cm] (a) {};%
			\draw (a) circle (0.1cm);
			\draw[-|] (a) --++(-0:1cm);
			\draw[-|] (a) --++(0:-1cm);
				
			\node[below of=A, yshift=0.5cm](B) {B};
			\node[right of=B, xshift=1cm] (b) {};%
			\draw (b) circle (0.1cm);
			\draw[-|] (b) --++(-0:1cm);
			\draw[-|] (b) --++(0:-1cm);
			
			\node[below of=B, yshift=0.5cm](C) {C};
			\node[right of=C, xshift=2cm] (c) {};%
			\draw (c) circle (0.1cm);
			\draw[-|] (c) --++(-0:1cm);
			\draw[-|] (c) --++(0:-1cm);
			
			
			\node[below of=C, yshift=0.5cm](D) {D};
			\node[right of=D, xshift=4.8cm] (d) {};%
			\draw (d) circle (0.1cm);
			\draw[-|] (d) --++(-0:1cm);
			\draw[-|] (d) --++(0:-1cm);
			
			\node[below of=D, yshift=0.5cm](E) {E};
			\node[right of=E, xshift=3.4cm] (e) {};%
			\draw (e) circle (0.1cm);
			\draw[-|] (e) --++(-0:1cm);
			\draw[-|] (e) --++(0:-1cm);
			
			\node[below of=E](L) {};
			\node[right of=L, xshift=3.4cm] (l) {};%
			\draw (l) circle (0.1cm);
			\draw[-|] (l) --++(-0:1.2cm);
			\draw[-|] (l) --++(0:-1.2cm);
			
			\node[below of=l, xshift=-1.2cm, yshift=0.5cm](l1) {-20ms};
			\node[below of=l, xshift=1.2cm, yshift=0.5cm](l2) {20ms};
			
		
			\end{tikzpicture}
	%	}
		\caption{The five possible outcomes when using the null region or ``region of practical equivalence'' method for decision-making (Kruschke, 2015). Outcome A supports a theory X that predicts a positive sign on the parameter; B supports a theory Y that predicts a negative sign. Outcome C rejects theory X, and D rejects Y; and the estimate E is consistent with the null region. The width of the null region here is 40 ms, but would depend on the dependent measure, and the measurement precision achievable by the measurement instrument.}
		\label{fig:precision}
	\end{figure}
\label{ropeend}

We now turn to Experiment 7, in which we investigated the Load-Distance interaction with a larger sample.

\subsubsection{Results of Experiment 7}

The estimates are summarized in Figure~\ref{fig:3}. This time, the estimate of Load at the critical region is
\Sexpr{round(E7_3_res[1,1])}~ms [\Sexpr{round(E7_3_res[1,2])},\Sexpr{round(E7_3_res[1,3])}];
the effect of Distance is 
\Sexpr{round(E7_3_res[2,1])}~ms [\Sexpr{round(E7_3_res[2,2])},\Sexpr{round(E7_3_res[2,3])}];
and the Load-Distance interaction is 
\Sexpr{round(E7_3_res[3,1])}~ms [\Sexpr{round(E7_3_res[3,2])},\Sexpr{round(E7_3_res[3,3])}].\footnote{At first glance, it may be surprising that in the post-critical region, the 95\% credible interval for the effect of Load in Experiment 7 is as wide as that of Experiment 6, which had 28 participants. One might expect that a larger-sample study always yields a narrower credible interval. But this need not necessarily be true in a particular sample; the credible interval is dependent on the estimates of the variance components, which will vary from study to study.} \label{smalllargesamplese}

\subsubsection{Discussion}

In Experiment 7, 
the positive coefficient for Distance suggests that increasing subject-verb distance by interposing an adjunct in addition to a dative NP led to slower reading times at the verb. 
A follow-up analysis using nested contrast coding shows that in the critical region, the Distance effect in the low-load conditions is \Sexpr{round(E7_3nested_res[2,1])} ms [\Sexpr{round(E7_3nested_res[2,2])},\Sexpr{round(E7_3nested_res[2,3])}]; and in the high-load conditions, it is \Sexpr{round(E7_3nested_res[3,1])} ms  [\Sexpr{round(E7_3nested_res[3,2])},\Sexpr{round(E7_3nested_res[3,3])}]. The larger distance effect in the high-load conditions is compatible with the LK argument in their paper that locality effects outweigh expectation effects when memory load is high.
However, the expectation account incorrectly predicts a negative coefficient in the low-load conditions. One possible explanation for the smaller distance effect in low-load conditions could be that expectation and locality act in opposite directions. Such an explanation is compatible with the LK proposal, and is consistent with the data. However, note that when we use the region of practical equivalence approach, both the two nested contrasts and the main effect of Distance are not conclusive because the 95\% credible interval of the respective estimates overlap with the ROPE of $\pm 20$ ms centered around $0$ ms.

<<loadsmallsampleestimates,echo=FALSE>>=
load(file="../data/smallsamplesestimates.Rda")
@

\begin{figure}[!htbp]
\centering
<<E7smallsampleplot,echo=FALSE,include=TRUE,fig.width=8,fig.height=6>>=
est_Load<-estimates[,c(1,4,7)]
#dim(est_Load)
est_Dist<-estimates[,c(1,4,7)+1]
est_LoadxDist<-estimates[,c(1,4,7)+2]

est<-data.frame(rbind(est_Load,est_Dist,est_LoadxDist))
colnames(est)<-c("mean","lower","upper")
est$significance<-ifelse(sign(est$lower)==sign(est$upper),"p<0.05","p>0.05")
est$predictor<-factor(rep(c("Load","Dist","LoadxDist"),each=50))
est$sample<-rep(1:50,3)

estLoad<-subset(est,predictor=="Load")
estDist<-subset(est,predictor=="Dist")
estLoadxDist<-subset(est,predictor=="LoadxDist")

#summary(abs(estDist$mean/E7_3_res[2,1]))

pd<-position_dodge(0.6)

plot_Load<-ggplot(estLoad, aes(x=sample, 
                             y=mean)) +
  geom_errorbar(aes(ymin=lower, ymax=upper),
                width=.25, size=.5, position=pd) +
  labs(title="Effect of Load: \n Estimates from repeated samples (n=28)") +
  xlab("sample")+
  ylab("estimate (ms)")+
  geom_hline(yintercept=E7_3_res[1,1])+
  geom_hline(yintercept=E7_3_res[1,2],linetype=2)+
  geom_hline(yintercept=E7_3_res[1,3],linetype=2)+
  geom_point(position=pd, size=2)+
  theme_bw()+magnifytext()+coord_flip()

estDist<-estDist[order(estDist$mean), ]
## relabel samples in increasing order:
estDist$sample<-1:50

plot_Dist<-ggplot(estDist, aes(x=sample, 
                             y=mean,shape=significance,ymin=lower, ymax=upper)) +
  #geom_errorbar(aes(ymin=lower, ymax=upper),
  #              width=.25, size=.5, position=pd) +
  geom_pointrange()+
  geom_point(size=2.5)+
  scale_shape_manual(values=c(1, 17))+
  labs(title="Effect of Distance: \n Estimates from repeated samples (n=28)") +
  xlab("Sample id")+
  ylab("Estimates (ms)")+
  geom_hline(yintercept=E7_3_res[2,1])+
  xlim(1,50)+
  geom_hline(yintercept=E7_3_res[2,2],linetype=2)+
  geom_hline(yintercept=E7_3_res[2,3],linetype=2)+
  geom_point(position=pd, size=2)+
  theme_bw()+magnifytext()+geom_hline(yintercept=0, linetype="dotted")
#+coord_flip()

plot_LoadxDist<-ggplot(estLoadxDist, aes(x=sample, 
                             y=mean)) +
  geom_errorbar(aes(ymin=lower, ymax=upper),
                width=.25, size=.5, position=pd) +
  labs(title="Effect of Load x Distance: \n Estimates from repeated samples (n=28)") +
  xlab("")+
  ylab("Estimates (ms)")+
  geom_hline(yintercept=E7_3_res[3,1])+
  geom_hline(yintercept=E7_3_res[3,2],linetype=2)+
  geom_hline(yintercept=E7_3_res[3,3],linetype=2)+
  geom_point(position=pd, size=2)+
  theme_bw()+magnifytext()+coord_flip()

plot_Dist
@
\caption{A demonstration of the fluctuation in the estimates for the effect of Distance when we choose 28 participants pseudo-randomly from the 100-participant experiment. The solid horizontal line is the estimated mean from the 100-participant data set, and the broken lines show the corresponding 95\% credible intervals. The points show the means and 95\% credible intervals when randomly sampling from the 100-participant data set.}\label{fig:smallsample}
\end{figure}


It is worth considering how our estimates from this 100-participant study would differ from a study that has only 28 participants. This can be demonstrated by repeatedly sampling 28 participants pseudo-randomly from this larger-sample data set, and then fitting a maximal linear mixed model using Stan. We carried out this repeated sampling 100 times. The mean and 95\% credible intervals for the effect of Distance are shown in Figure~\ref{fig:smallsample}, along with the mean and credible interval from the 100-participant study. The wide credible intervals and the fluctuation around the larger sample's estimated mean illustrates the problem that arises with low-precision studies: wide uncertainty of the estimate and fluctuation of means under repeated sampling. Because of this fluctuation, those estimates that happen to come out significant in a frequentist test will, due to Type M error, necessarily be overestimates relative to the reference point of the mean and credible intervals estimated from the full data set. For a similar demonstration investigating similarity-based interference using a larger data set, see \textcite{NicenboimEtAlCogSci2018}.



In conclusion, in this 100-participant study we don't see any grounds for claiming an interaction between Load and Distance. The most that we can conclude is that the data are consistent with memory-based accounts such as the Dependency Locality Theory \parencite{gibson00}, which predict increased processing difficulty when subject-verb distance is increased. However, this Distance effect yields estimates that are also  consistent with our posited null region; so the evidence for the Distance effect cannot be considered convincing.





<<echo=FALSE,eval=FALSE>>=
test <- rbind(mtcars[1:10, 1:5], 
              colSums(mtcars[1:10, 1:5]))
rownames(test)[11] <- "Sum"

test[11, ] <- paste0("BOLD", test[11, ])

bold.somerows <- 
        function(x) gsub('BOLD(.*)',paste('\\\\textbf{\\1','}'),x)

test.xt <- xtable(test, label="table", caption='test')
#align(test.xt) <- "|l|l|l|r|r|r|"#
#print(test.xt, type="latex",tabular.environment='tabular', include.rownames = FALSE, floating=TRUE, sanitize.text.function = bold.somerows)
@

\section{General Discussion}

Experiment 1-6 showed that the statistically significant (or nearly-significant) effects found in \textcite{levy2013expectation} are noisy enough that a broad range of possible outcomes---including no effect---can be seen as consistent with the original studies' estimates.
The noisiness of the estimates in the original LK study, expressed in the wide credible intervals, implies low power, which can---and in this case did---lead to exaggerated effects in the original studies.  Had we carried out statistical significance tests on these replication attempts, we would have found that the original results would not be replicable, if by replicable we mean that  significance should be found consistently. 

Regarding the absence of locality and expectation effects in our experiments, 
our point here is not that the effects found by LK are not true. One cannot definitively conclude much from the original studies and our replication attempts. Rather, our aim is to draw attention to the point that we cannot learn much from a low-precision experiment, regardless of whether or not  statistically significant effects are found. 

Experiment 7 showed that a larger-sample study generally delivers narrower credible intervals than the 28-participant studies. It also delivers a smaller estimate of the posterior mean for the Load-Distance interaction compared to the original study; the larger-sample estimate is probably more realistic. Experiment 7 suggests that the key claim of a Load-Distance interaction in LK's original experiments may be consistent with no effect. One interesting suggestion from this 100-participant study is that the locality effect that is predicted by account such as the Dependency Locality Theory \parencite{gibson00} may have some weak support. 
Since this is, to our knowledge, the first time that any evidence for locality has been seen in German, clearly further investigation is needed. Locality effects have been reported for other head-final languages such as Hindi \parencite{HusainVasishthNarayanan2015}, and Persian \parencite{SafaviEtAlFrontiers2016}; but it remains to be seen whether these and other head-final languages consistently show locality. An important line of research would be to attempt to replicate the published results for head-final languages like German, Hindi, and Persian, and to investigate other head-final languages like Japanese and Korean. If the LK experiment design is followed up on in future work, it would be advisable to choose simpler sentences than the ones LK used;  comprehension accuracy needs to be better than in the studies discussed in the present paper.

A legitimate concern at this point is that most of the effects investigated in our seven experiments showed results consistent with no effect. Could it be that there is something fundamentally wrong with our experimental methodology? In order to address this worry, we checked whether we could recover well-known word length and frequency effects from the filler items in the four eyetracking experiments. The details are discussed in Appendix~\ref{appendix:freqwleffects}, but briefly, all four experiments show the expected effects. Thus, the methodology does not seem to have any fundamental problems. Of course, with null results one cannot be certain that no effect is present, especially when power is low. In future work, other labs should attempt to replicate LK's and our reported estimates  of effects.

<<agrmtattrn,echo=FALSE,warning=FALSE,message=FALSE,eval=FALSE>>=

## Dillon et al 2013 Expt 1
DillonE1<-read.table("../data/DillonE1.txt",header=T)
## Lago et al 2015 data (all expts):
Lago<-read.csv("../data/Lago.csv",header=T)
##Wagers et al 2009 data (all expts):
load("../data/Wagers.Rdata")
load("../data/Tucker.RData")

## Dillon E1:
DillonE1$cond<-factor(DillonE1$cond)
DillonE1Mism<-subset(DillonE1,fixationtype=="tt" & cond%in%c(3,4) & value!="NA")
DillonE1Mism$cond<-factor(DillonE1Mism$cond)
DillonE1Mism$int<-ifelse(DillonE1Mism$cond==3,"low","high")
DillonE1Mism$x<-ifelse(DillonE1Mism$cond==3,-1,1)
dillonE1<-DillonE1Mism[,c(1,3,4,14,15)]
dillonE1$expt<-factor("dillonE1")
colnames(dillonE1)[3]<-"rt"

nsubj_dillonE1<-length(unique(dillonE1$subj))

##Lago:
dat<-Lago
## critical region: not used because published paper found
## significant effects in postcrit region only
e1<-subset(dat,Experiment=="Experiment1" & Region=="06v1")
e2<-subset(dat,Experiment=="Experiment2" & Region=="06aux")
e3a<-subset(dat,Experiment=="Experiment3A" & Region=="06aux")
e3b<-subset(dat,Experiment=="Experiment3B" & Region=="aux")

nsubj_lagoe1<-length(unique(e1$Subject))
nsubj_lagoe2<-length(unique(e2$Subject))
nsubj_lagoe3a<-length(unique(e3a$Subject))
nsubj_lagoe3b<-length(unique(e3b$Subject))


## postcritical region:
poste1<-subset(dat,Experiment=="Experiment1" & Region=="07prep")
poste2<-subset(dat,Experiment=="Experiment2" & Region=="07adv")
poste3a<-subset(dat,Experiment=="Experiment3A" & Region=="07a")
poste3b<-subset(dat,Experiment=="Experiment3B" & Region=="a")

##e1: a,b
#-(a) Ungram , singular attractor (interference condition)
#La *nota* que la chica escribieron en la clase alegr a su amiga
#The note that the girl wrotepl during class cheered her friend up
#-(b) Ungram , plural attractor (baseline condition)
#Las *notas* que la chica escribieron en la clase alegraron a su amiga      
#The notes that the girl wrotepl during class cheered her friend up
poste1<-subset(poste1,Condition%in%c("a","b"))
poste1$Condition<-factor(poste1$Condition)
poste1$x<-ifelse(poste1$Condition=="a",-1,1)
poste1$int<-ifelse(poste1$Condition=="a","low","high")
poste1<-poste1[,c(1,3,8,15,14)]
poste1$expt<-factor("lagoE1")
lagoE1<-poste1
colnames(lagoE1)<-c("subj","item","rt","int","x","expt")

## e2: c,d
poste2<-subset(poste2,Condition%in%c("c","d"))
poste2$Condition<-factor(poste2$Condition)
poste2$x<-ifelse(poste2$Condition=="c",-1,1)
poste2$int<-ifelse(poste2$Condition=="c","low","high")
#head(poste2)
poste2<-poste2[,c(1,3,8,15,14)]
poste2$expt<-factor("lagoE2")
lagoE2<-poste2
colnames(lagoE2)<-c("subj","item","rt","int","x","expt")

## e3a: e,f
poste3a<-subset(poste3a,Condition%in%c("e","f"))
poste3a$Condition<-factor(poste3a$Condition)
#-(e) Ungram, singular attractor (interference condition)
#La *nota* que la chica van a escribir en la clase alegrar a su amiga
#The note that the girl are going to write during class will cheer her friend up
#-(f) Ungram, plural attractor (baseline condition)
#Las *notas* que la chica van a escribir en la clase alegrarn a su amiga
#The notes that the girl are going to write during class will cheer her friend up
#boxplot(RT~Condition,poste3a)
poste3a$x<-ifelse(poste3a$Condition=="e",-1,1)
poste3a$int<-ifelse(poste3a$Condition=="e","low","high")
poste3a<-poste3a[,c(1,3,8,15,14)]
poste3a$expt<-factor("lagoE3a")
lagoE3a<-poste3a
colnames(lagoE3a)<-c("subj","item","rt","int","x","expt")

## e3b: e,f
poste3b<-subset(poste3b,Condition%in%c("e","f"))
poste3b$Condition<-factor(poste3b$Condition)
#-(e) Ungram, singular attractor (baseline condition)
#The player that the coach were always praising very enthusiastically decided to     leave the team 
#-(f) Ungram, plural attractor (interference condition)
#The players that the coach were always praising very enthusiastically decided to     leave the team
poste3b$x<-ifelse(poste3b$Condition=="e",-1,1)
poste3b$int<-ifelse(poste3b$Condition=="e","low","high")
poste3b<-poste3b[,c(1,3,8,15,14)]
poste3b$expt<-factor("lagoE3b")
lagoE3b<-poste3b
colnames(lagoE3b)<-c("subj","item","rt","int","x","expt")

## Wagers:
E2postcrit<-subset(Experiment2,Region==7)
nsubj_wagerse2<-length(unique(E2postcrit$Subj))
#E2$intr.au<-ifelse(E2$rchead=="pl" & E2$gramm=="ungram",1/2,
#                   ifelse(E2$rchead=="sg" & E2$gramm=="ungram",-1/2,
#                          0))
## d (sing),h (plu)
#unique(subset(E2postcrit,gramm=="ungram")$Condition)
E2postcrit<-subset(E2postcrit,Condition%in%c("d","h"))
E2postcrit$Condition<-factor(E2postcrit$Condition)
E2postcrit$x<-ifelse(E2postcrit$Condition=="d",-1,1)
E2postcrit$int<-ifelse(E2postcrit$Condition=="d","low","high")
#colnames(E2postcrit)
E2postcrit<-E2postcrit[,c(4,3,8,13,12)]
E2postcrit$expt<-factor("wagersE2")
wagersE2<-E2postcrit
colnames(wagersE2)<-c("subj","item","rt","int","x","expt")

## E3
E3postcrit<-subset(Experiment3,Region==7)
nsubj_wagerse3<-length(unique(E3postcrit$Subj))
#E3crit$intr.au.pl<-ifelse(E3crit$gramm=="ungram" & E3crit$rcsubj=="sg" &
#                            E3crit$rchead=="pl",1/2,
#                         ifelse(E3crit$gramm=="ungram" & E3crit$rcsubj=="sg" & 
#                                   E3crit$rchead=="sg",-1/2,0))

#E3crit$intr.au.sg<-ifelse(E3crit$gramm=="ungram" & E3crit$rcsubj=="pl" &
#                            E3crit$rchead=="sg",1/2,
#                          ifelse(E3crit$gramm=="ungram" & E3crit$rcsubj=="pl" & 
#                                   E3crit$rchead=="pl",-1/2,0))

E3postcrit_pl<-subset(E3postcrit,gramm=="ungram" & rcsubj=="sg")
E3postcrit_pl$Condition<-factor(E3postcrit_pl$Condition)
E3postcrit_sg<-subset(E3postcrit,gramm=="ungram" & rcsubj=="pl")
E3postcrit_sg$Condition<-factor(E3postcrit_sg$Condition)

#unique(E3postcrit_pl$Condition) ## b,f
#unique(E3postcrit_sg$Condition) ## c,g

#head(subset(E3postcrit_sg,rchead=="sg"))
#head(E3postcrit_pl)

## plural:
E3postcrit_pl$x<-ifelse(E3postcrit_pl$Condition=="b",-1,1)
E3postcrit_pl$int<-ifelse(E3postcrit_pl$Condition=="b","low","high")
E3postcrit_pl<-E3postcrit_pl[,c(4,3,8,15,14)]
E3postcrit_pl$expt<-factor("wagersE3pl")
colnames(E3postcrit_pl)<-c("subj","item","rt","int","x","expt")
wagersE3pl<-E3postcrit_pl

## singular:
E3postcrit_sg$x<-ifelse(E3postcrit_sg$Condition=="c",-1,1)
E3postcrit_sg$int<-ifelse(E3postcrit_sg$Condition=="c","low","high")
E3postcrit_sg<-E3postcrit_sg[,c(4,3,8,15,14)]
E3postcrit_sg$expt<-factor("wagersE3sg")
colnames(E3postcrit_sg)<-c("subj","item","rt","int","x","expt")
wagersE3sg<-E3postcrit_sg

## E4
E4postcrit<-subset(Experiment4,Region==8) ##
nsubj_wagerse4<-length(unique(E4postcrit$Subj))
#head(subset(Experiment4,Condition=="c"),n=10)
#postcritical region
#E4postcrit$intr.au<-ifelse(E4postcrit$gramm=="ungram" & E4postcrit$match=="match",-1/2,
#                           ifelse(E4postcrit$gramm=="ungram" & E4postcrit$match=="mismatch",1/2,0))
E4postcrit<-subset(E4postcrit,gramm=="ungram")
E4postcrit$Condition<-factor(E4postcrit$Condition)
E4postcrit$x<-ifelse(E4postcrit$Condition=="c",-1,1)
E4postcrit$int<-ifelse(E4postcrit$Condition=="c","low","high")
E4postcrit<-E4postcrit[,c(4,3,8,13,12)]
E4postcrit$expt<-factor("wagersE4")
colnames(E4postcrit)<-c("subj","item","rt","int","x","expt")
wagersE4<-E4postcrit

# E5
E5postcrit<-subset(Experiment5,Region==8) ##postcritical region
nsubj_wagerse5<-length(unique(E5postcrit$Subj))
E5postcrit<-subset(E5postcrit,gramm=="ungram")
E5postcrit$Condition<-factor(E5postcrit$Condition)
## c,d
E5postcrit$x<-ifelse(E5postcrit$Condition=="c",-1,1)
E5postcrit$int<-ifelse(E5postcrit$Condition=="c","low","high")
E5postcrit<-E5postcrit[,c(4,3,8,13,12)]
colnames(E5postcrit)<-c("subj","item","rt","int","x")
E5postcrit$expt<-factor("wagersE5")
wagersE5<-E5postcrit

#head(wagersE5)

dat<-rbind(dillonE1,wagersE2,
           lagoE1,lagoE2,
           lagoE3a,lagoE3b,
           wagersE2,
           wagersE3pl,wagersE3sg,
           wagersE4,wagersE5)
dat$subj<-factor(paste(dat$expt,dat$subj,sep=""))
dat$item<-factor(paste(dat$expt,dat$item,sep=""))
with(dat,tapply(subj,expt,function(x)length(unique(x))))
     
@

<<agrmtattrn2,echo=FALSE,warning=FALSE,message=FALSE,eval=FALSE>>=
## Dillon E1: 
stanDat<-createStanDat(d=dillonE1,
                       rt=dillonE1$rt,
                       form=as.formula("~ 1 + x"))
#str(stanDat)
DillonE1 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
             data = stanDat,
             iter = 2000, 
             chains = 4)
## Int is Interference:
pars<-c("Int","beta[2]","sigma_u[1]","sigma_u[2]","sigma_w[1]","sigma_w[2]","sigma_e")
DillonE1_res<-stan_results(DillonE1,params=pars[1])

stanDat<-createStanDat(d=lagoE1,
                       rt=lagoE1$rt,
                       form=as.formula("~ 1 + x"))

LagoE1 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                 data = stanDat,
                 iter = 2000, 
                 chains = 4)
LagoE1_res<-stan_results(LagoE1,params=pars[1])

stanDat<-createStanDat(d=lagoE2,
                       rt=lagoE2$rt,
                       form=as.formula("~ 1 + x"))

LagoE2 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
               data = stanDat,
               iter = 2000, 
               chains = 4)
LagoE2_res<-stan_results(LagoE2,params=pars[1])

stanDat<-createStanDat(d=lagoE3a,
                          rt=lagoE3a$rt,
                       form=as.formula("~ 1 + x"))

LagoE3a <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
               data = stanDat,
               iter = 2000, 
               chains = 4)

LagoE3a_res<-stan_results(LagoE3a,params=pars[1])

stanDat<-createStanDat(d=lagoE3b,
                          rt=lagoE3b$rt,
                       form=as.formula("~ 1 + x"))

LagoE3b <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                data = stanDat,
                iter = 2000, 
                chains = 4)

LagoE3b_res<-stan_results(LagoE3b,params=pars[1])

stanDat<-createStanDat(d=wagersE2,
                          rt=wagersE2$rt,
                       form=as.formula("~ 1 + x"))

WagersE2 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                data = stanDat,
                iter = 2000, 
                chains = 4)
WagersE2_res<-stan_results(WagersE2,params=pars[1])

stanDat<-createStanDat(d=wagersE3pl,
                          rt=wagersE3pl$rt,
                       form=as.formula("~ 1 + x"))

WagersE3pl <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                 data = stanDat,
                 iter = 2000, 
                 chains = 4)
WagersE3pl_res<-stan_results(WagersE3pl,params=pars[1])

stanDat<-createStanDat(d=wagersE3sg,
                          rt=wagersE3sg$rt,
                          form=as.formula("~ 1 + x"))

WagersE3sg <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                   data = stanDat,
                   iter = 2000, 
                   chains = 4)
WagersE3sg_res<-stan_results(WagersE3sg,params=pars[1])

stanDat<-createStanDat(d=wagersE4,
                          rt=wagersE4$rt,
                          form=as.formula("~ 1 + x"))

WagersE4 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                   data = stanDat,
                   iter = 2000, 
                   chains = 4)
WagersE4_res<-stan_results(WagersE4,params=pars[1])

stanDat<-createStanDat(d=wagersE5,
                       rt=wagersE5$rt,
                       form=as.formula("~ 1 + x"))

WagersE5 <- stan(file = "StanModels/maxModelTargetMismatch.stan", 
                 data = stanDat,
                 iter = 2000, 
                 chains = 4)
WagersE5_res<-stan_results(WagersE5,params=pars[1])

## cunnings and sturt 2018 
CS18E1<-c(-22,-42,-4)

CS18E2<-c(-19,-40,1)

posteriors<-data.frame(expt=factor(1:12),rbind(DillonE1_res,LagoE1_res,LagoE2_res,LagoE3a_res,LagoE3b_res,WagersE2_res,WagersE3pl_res,WagersE3sg_res,WagersE4_res,WagersE5_res,CS18E1,CS18E2))

## order expts to match posteriors:
dat$expt<-factor(dat$expt,levels=c("dillonE1","lagoE1","lagoE2","lagoE3a","lagoE3b","wagersE2","wagersE3pl","wagersE3sg","wagersE4","wagersE5","CS18E1","CS18E2"))
#levels(dat$expt)

n_subj<-as.vector(with(dat,tapply(subj,expt,function(x)length(unique(x)))))
n_item<-as.vector(with(dat,tapply(item,expt,function(x)length(unique(x)))))
n_subj[11:12]<-c(48,48)
n_item[11:12]<-c(32,32)

posteriors$n_subj<-n_subj
posteriors$n_item<-n_item
## width of credible interval
posteriors$width<-posteriors$upper-posteriors$lower

## reorder by mean magnitude:
posteriors<-posteriors[with(posteriors,order(mean)),]

## reorder expt by mean's magnitude:
posteriors$expt <- factor(posteriors$expt,levels=posteriors$expt[order(posteriors$mean)])
save(posteriors,file="../data/posteriorsTargetMismatch.Rda")
#xtable(posteriors)
@

<<echo=FALSE>>=
load(file="../data/posteriorsTargetMismatch.Rda")
posteriors<-rbind(posteriors,
      c(NA,-26,-57,-10,NA,NA,NA))

@

We return now to the point that the significant effects in LK's experiments are too noisy to interpret using statistical significance. Noisiness is not a  property that is unique to the LK study considered here. Reading studies on other well-established effects also have issues similar to those discussed here.
One example is the difference in reading times at the head noun of subject vs.\ object relative clauses in Chinese. A meta-analysis of 12 studies \parencite{VasishthetalPLoSOne2013}  
showed that the estimates of the effect (from self-paced reading and eyetracking) across different studies fluctuate quite a lot, from $-123$ to $100$ ms, with confidence intervals ranging in width from $80$ to $320$ ms \parencite[also see][]{Vasishth:MScStatistics}.  
A more recent example is so-called number agreement attraction. Here, ungrammatical sentences like the following are investigated: \textit{The key to the cabinet/cabinets are on the table}. For theoretical reasons that don't concern us here \parencite[see][]{EngelmannJaegerVasishthSubmitted2018}, faster reading times are expected at the auxiliary when the preceding noun agrees in number with the auxiliary's number marking (i.e., the auxiliary verb in \textit{cabinets are} is read faster than in  \textit{cabinet are}). One theory, the \textcite{lewisvasishth:cogsci05} cue-based retrieval model, predicts that the mean expected facilitation is around $-26$ ms for most parameter configurations; if the model parameters are varied over a narrow range, the predicted facilitation varies from approximately $-10$ to $-57$~ms.\footnote{\textcite{EngelmannJaegerVasishthSubmitted2018} provide a detailed investigation of the range of predictions that the model makes for the facilitatory interference discussed here. They varied the latency factor $F$, the noise parameter $ANS$, the maximum associative strength $MAS$, the  mismatch penalty $MP$, and the retrieval threshold $\theta$, and computed model predictions for the range of  parameter combinations given by $F \in \{0.01, 0.02, \dots, 0.6\}, ANS \in \{0.1, 0.2, 0.3\}, MAS \in \{1,2,3,4\}, MP \in \{0,1,2\}, \theta \in \{-2,-1.5,\dots,0\}$. In order to derive approximate upper and lower bounds of model predictions, we take the median together with the first and third quartiles of Engelmann et al.'s simulated facilitatory interference effects (6000 iterations for each parameter combination), with the difference that we only used parameter configurations with a latency factor of $0.05$ to $0.6$. The calculations can be reproduced using the Shiny app: https://engelmann.shinyapps.io/inter-act/.}
Several studies have been published showing statistically significant facilitation effects, as predicted by theory. Because of the repeated significant effects found, this facilitation effect is considered very reliable in psycholinguistics. We re-analyzed the data (self-paced reading; in one study, total reading time from eyetracking) from $10$ published experiments, 8 out of 10 reported a significant effect. We fit Bayesian linear mixed models with full variance-covariances matrices for all random effects, and the same regularizing, weakly informative priors that we used in the LK data. Unlike the original studies, we did not delete extreme values; rather, we modeled the reading-time data as being generated from a log-normal distribution and back-transformed the estimates to milliseconds (see Appendix~\ref{app:agrmt} for details).
We find that the uncertainty of the estimates in the data is quite high: 
The ten studies' mean estimates range from \Sexpr{round(min(posteriors[,2]))} to \Sexpr{round(max(posteriors[,2]))} ms, with credible intervals ranging in width from 
\Sexpr{round(min(posteriors[,4]-posteriors[,3]))} to \Sexpr{round(max(posteriors[,4]-posteriors[,3]))} ms. These empirical estimates (along with their 95\% credible intervals) are all  consistent with the model predictions ($-10$ to $-57$ ms), in the sense that the credible intervals from these 10 studies overlap with the theoretically predicted range. But these data are not strongly consistent with  the Lewis and Vasishth model predictions. If these estimates had been more precise (i.e., had much narrower credible intervals) and their 95\% credible intervals had fallen within the predicted range, this would have been a stronger validation of the model's predictions. With such wide credible intervals in the data, a broad range of outcomes is compatible with the data, including effectively no facilitatory effect at all. Thus, even in the relatively clear agreement attraction case, in future work higher precision replication attempts need to be carried out to determine better estimates of the facilitation effect.

A central problem is that underpowered studies can yield a statistically significant result due to Type M error, and these significant results will be overestimates. Given that significant results are favored by journals and reviewers, effects reported in the literature are \textit{guaranteed} to be overestimates when power is low. They will also be seen as very convincing because of their large magnitude. 
A large effect like 200 ms with a large standard error of 80 ms, leading to a t-value of 2.5, seems more convincing than a small effect of 9 ms with a small standard error of 4.5 ms and a t-value of 2. In fact, with a null region defined under the region of practical equivalence approach, both results could be  consistent with there being ``no effect.'' However, the smaller estimate with narrower credible intervals may reflect reality better. Thus, when power is low, using significance to decide whether to publish a result leads to a proliferation of exaggerated estimates in the literature. There is in principle no harm in publishing low-powered studies in top journals, 
as long as  strong claims are avoided. This is what statisticians mean when they suggest that researchers ``accept uncertainty and embrace variation'' \parencite{mcshane2017abandon}. Currently, in psycholinguistics and other areas, we are taught to have the expectation that every experiment be a ``win.'' Under this prior belief in routine success, even null results from low-powered studies start to look informative.

It is of course possible to publish more informative studies by simply running higher-power experiments. But how can we decide what constitutes a higher-powered study? Frequentist statistics has several proposals for sequential testing \parencite[e.g.,][]{frick1998better}, which  avoid running unnecessarily large numbers of participants. A Bayesian approach that we used in this paper is to define a region of practical equivalence for total reading time (specifically, $\pm 20$ ms around $0$ ms) and to run the experiment until the desired precision was reached.
Our choice of a 95\% credible interval width of $40$ ms was only for illustration purposes; depending on the resources available, one could aim for even higher precision.  For example, $184$ participants in the \textcite{NicenboimEtAlCogSci2018} self-paced reading study had a 95\% credible interval  of $20$ ms. Note that the goal here should not be to find a credible interval that does not include an effect of 0 ms; that would be identical to applying the statistical significance filter and is exactly the practice that we criticize in this paper.\footnote{Doing hypothesis testing with Bayes factors would lead to similar problems unless one can specify a fully generative model \parencite{Gelman14}.}  Rather, the goal is to achieve a particular precision level for the estimate, and to use the region of practical equivalence for interpreting the results, possibly alongside the p-value. 

Once we have fixed the region of practical equivalence, we effectively also fix the precision (the 95\% credible interval) that is theoretically meaningful to us. Now we can run the experiment until we reach this desired level of precision. This has at least two advantages over a conventional power analysis. First, in the Bayesian framework, there is no need to define a stopping criterion in advance of running our experiment. In psycholinguistics, running more participants until a desired outcome  (statistical significance with a particular sign of the effect) is reached is a fairly common practice. But within the frequentist paradigm, this stopping criterion will inflate Type I error \parencite[e.g.,][]{pocock2013clinical}. 
In the Bayesian framework, there is no concept of hypothetical replications; the data at hand are not interpreted in the light of the properties of data from imagined repeated sampling. The Bayesian framework rather obeys the likelihood principle, which states that all the information from the data is contained in the likelihood function \parencites{Gelman14,lee2012bayesian}. We can therefore check the precision of our estimates while running the experiment, and stop the experiment when the desired precision  is reached (as opposed to the desired effect becoming significant). 

A second advantage of using precision as a guide to data collection is that we can shift the focus to what really matters: quantifying our uncertainty about the estimate of interest. A conventional power analysis assumes a good guess about the magnitude of the true effect, and this guess is often based on previously published data.
As we have shown here, when the sample sizes are small and there is a bias to only publish statistically significant effects, effect magnitudes will be overestimated by a large amount. Using these estimates leads to a large underestimation of  the sample size needed for high-powered replications.
In a precision-based analysis, the focus is on the amount of uncertainty in the estimate that we are willing to tolerate. The magnitude of the estimate, together with its uncertainty, are much more important theoretically than just counting the number of significant vs.\ not significant results in the literature. Such a vote-counting approach is commonly adopted to summarize the literature in narrative reviews, and to decide whether an effect is ``present'' vs.\ ``absent.'' The voting-based approach would be fine if there were no publication bias at all and if power were sufficiently high in published studies. For an example of a voting-based approach to deciding whether an effect is present or absent, see \textcite{phillips2011grammatical}. There, when discussing whether reflexives show similarity-based interference effects, the authors conclude: ``Thus, most evidence suggests that the processing of simple argument reflexives in English is insensitive to structurally inappropriate antecedents, indicating that the parser engages a retrieval process that selectively targets the subject of the current clause.'' If power in the studies that Phillips and colleagues base their conclusions on is low, then many null results are to be expected. It is well-known in statistical theory that null results from low-powered studies should be treated as inconclusive rather than proving that the null hypothesis is true; unfortunately, this detail is not widely appreciated.  In sum, simple vote-counting would be highly misleading when power is low and publication bias exists.
\label{votecounting}

Many researchers have pointed out  that we should 
aim for higher-precision estimates and focus on estimation rather than only focusing on statistical significance \parencite[e.g.,][]{claridge2016estimation,greenland2016statistical}.
Focusing on estimation will allow for better-quality meta-analyses and better quantitative model comparisons of competing computational models. The first comprehensive quantitative evaluation of the computational memory-retrieval model of \textcite{lewisvasishth:cogsci05}
 involved comparing model predictions to estimates from $77$ published results on retrieval processes \parencite{EngelmannJaegerVasishthSubmitted2018}. This evaluation was only possible because the estimates (and their uncertainty) were available from a meta-analysis \parencite{JaegerEngelmannVasishth2017}. The meta-analysis provided estimates based on all relevant reading-time studies which were then compared with the model predictions. Although the meta-analytic estimates are likely to be biased (due to publication bias and Type M error in individual studies), they are more precise than the estimates from individual studies because the meta-analysis aggregates data from multiple studies after weighting them by their precision: the meta-analysis allows us to take into account accumulated knowledge in a quantitative manner. However, 
the results of the quantitative evaluation by \textcite{EngelmannJaegerVasishthSubmitted2018} would have been more informative if the estimates from the published individual studies had had higher precision.

In addition to fixing precision in advance, a second suggestion \parencite{chambers2017seven} is that we should attempt to conduct direct, pre-registered replications of experiments, because there is no guarantee that a result reflects reality just because it is statistically significant. Every major claim should be either accompanied by a pre-registered direct replication, or even better, other researchers from competing labs should be encouraged to replicate the original result. 
Direct replications are necessary even for higher-precision studies, because population differences, lab practices, etc., can easily bias an individual result.


 As \textcite{chambers2017seven} explains, pre-registration involves defining in advance the analysis that is planned and depositing this in an embargoed repository like OSF (osf.io) or aspredicted.org. OSF time-stamps the pre-registration, which serves as a transparent way to demonstrate that the analysis plan was defined before the data were collected. Pre-registering will also minimize problems like p-hacking, HARKing (hypothesizing after the results are known), and the garden-of-forking paths problem \parencite{gelman2016statistical,forstmeier2017detecting,Simmonsetal2011} that have plagued psychology and other areas. With pre-registration, the researcher is still free to explore their data, but pre-registration is a valuable tool that clearly separates the prior analysis plan from the exploratory part \parencites{deGroot2014}. Currently, due to the unreasonable pressure to publish fast and to report novel results in top journals, crucial data-analysis decisions are often made after examining the data. For example, the same researcher will often include or exclude data on different criteria,  so that it eventually passes the statistical significance filter. Sometimes, excluding or including a few data points can make the difference between significance and non-significance \parencite{VasishthetalPLoSOne2013}. Another example is region-of-interest selection in reading studies: researchers often change the region of interest from study to study or even within a study, driven exclusively by the search for significance \parencite[an example is discussed in][]{VasishthNicenboimStatMeth}.   Another common approach is to run the study, check for significance, then either run more participants if significance is desired but not reached, or stop collecting data if a null result is desired. These decisions are often not reported in the published paper. 
  Pre-registration would remove these degrees of freedom and thereby ensure a clear separation between confirmatory and exploratory analyses \parencite{deGroot2014}.\footnote{A common objection we hear is that anyone could defeat the purpose of pre-registration by first collecting the data and then depositing a fake pre-registration. However, this would just be scientific fraud; pre-registration is not designed to solve that problem.}

Our third suggestion is that data and code be released mandatorily along with the published paper. Some authors are happy to share their data and code, but in many other cases the crucial information---the data itself---are not available. For example, \textcite{nieuwland2017limits} tried but failed to obtain the data for the published result  \parencite{delong2005probabilistic} that they attempted to replicate. Many researchers have  generously released their data to us in connection with the present and other replication attempts. But attempts to obtain data from published studies are often unsuccessful. \textcite{wicherts2006poor} report an attempt to obtain data from 141 articles from major psychology journals, which had a total of 249 experiments. Of these, 73\% of the data were not released. Wicherts and colleagues report that this is approximately the same non-response rate as in 1962.  The continued absence of reproducible code and data seriously harms cumulative progress in science;  evidence synthesis (meta-analysis) needs accurate estimates from published papers. Our experience with meta-analysis \parencite{NicenboimRoettgeretal,JaegerEngelmannVasishth2017} shows that published summaries are usually far from adequate because they often don't contain the minimal information (the estimated mean of the parameter of interest and the standard error) needed to conduct the meta-analysis. Sometimes the published analysis is incorrect  and as a result the published statistics are unusable \parencite[some examples are discussed in][]{NicenboimRoettgeretal}.
Obtaining and reanalyzing the original data (using the originally used code) is the most reliable way to obtain accurate estimates for evidence synthesis.

Leading journals could trigger a positive change by requiring data and code release for all articles, and introducing a special article type (e.g., a pre-registered Replication Report) for direct replication attempts.  Currently, direct replications are not considered to be novel enough to be worth publishing, and novelty of results is given disproportionate weight. However, replication is an important tool for  establishing reliability.  This is something that a p-value, especially a p-value computed from an underpowered study, cannot ever deliver. Increasing precision and conducting direct replications are vital for any empirically rigorous science.

There is clearly a downside to focusing on higher precision and direct replications. Perhaps the biggest one is that carrying out experiments towards the aim of increasing precision would take much longer. For example, the experiments in the present paper were started on 26 November 2015, and ended on 29 September 2017, a period of nearly two years. This means that at least in smaller universities, where recruiting participants is not easy, internet experiments may serve as a partial solution (but this comes with other disadvantages). Another obvious side-effect is that the speed with which we can publish papers will go down. Clearly, expectations regarding publication rate need to change.  
In closing, a contribution of the present paper is to demonstrate through direct replication attempts the fact (well-known in some scientific communities) that published results---even results published in top journals---may not be all that newsworthy because they may be consistent with effectively no effect and may not be replicable in the sense that significant effects may not be found to be significant under replication. Too often, published empirical results are treated as a novel contribution simply because of the application of the statistical significance filter. How many published claims in psycholinguistics actually reflect reality remains to be seen. Big effects involving, e.g., grammaticality violations or strong garden paths, are likely to be replicable, but more subtle effects may not be. 
For example, the recent failure to find significant effects in anticipatory processing by \textcite{nieuwland2017limits} and \textcite{kochari2018lexical} suggests that replicability problems arising from the statistical significance filter could run deep in psycholinguistics. 
Of course, the issues are not limited to psycholinguistics and extend to all other scientific disciplines that use this decision criterion to decide whether or not to publish results. The reliability of published results may improve if we finally start to follow the best practices that have been advocated again and again by statisticians but which have been largely ignored by psychology and other areas: aim at higher precision, conduct direct, pre-registered replications, and release data and code. These changes will contribute towards improving the reliability of published results.

\section{Conclusion}

In sentence processing, many results, such as the classical garden-path findings \parencite{frazierrayner82}, have large and robust effects. These are very likely to be easily replicable. But the low-hanging fruit has long been picked.  Subtle manipulations require designs and sample sizes that deliver accurate estimates.

History has shown that any suggestions to improve power and to replicate results have generally not been adopted in  psychology \parencite[see discussion in][]{lane1978estimating}. 
 We  nevertheless reiterate some proposals that many others have made in the past \parencites[e.g.,][]{bakan1966test,amrhein2018abandon,mcshane2017abandon,chambers2017seven}. 
 Researchers should (i) move their focus away from statistical significance and attend instead to increasing the precision of their estimates (e.g., by increasing sample size, or improving the quality of measurements, or designing stronger manipulations);
(ii) carry out direct (not just conceptual) replications in order to demonstrate the existence of an effect; 
(iii) pre-register their designs and planned analyses and deposit them in venues like osf.io and  aspredicted.org; and (iv) release their data and code upon publication. Journals can encourage these practices by favoring pre-registered analyses,  introducing a short-article type featuring direct replications, and mandating open data and code release upon publication. Some of the leading journals already require data and code release upon publication, and in some cases during the review process. This needs to become the default. 

\section{Acknowledgements}

We are grateful to Roger Levy and Frank Keller for their openness in sharing their data and code with us; without their assistance and cooperation, this paper would not have been possible. We also owe a debt of gratitude to Colin Phillips, Matt Wagers, Brian Dillon, and Sol Lago for generously sharing their data; without their data, our project would have been considerably reduced in scope. Roger Levy provided detailed comments on the paper; we are very grateful for his advice.
This project began in December 2015 as part of a demonstration of a replication for a course that the first author taught at the University of Tokyo, Japan; Doug Roland and Yuki Hirose provided many useful comments at this initial stage.  Our grateful thanks to Johanna Thieke, lab manager of Vasishth Lab at the University of Potsdam, Germany, for carrying out the experiments over a period of nearly two years. 
We also thank Reinhold Kliegl, Sander Greenland, Steven Goodman, Christian Robert, Titus von der Malsburg, Daniel Schad, Sandra Hanne, Sol Lago, Jan Vanhove, Tatjana Scheffler, Michael C.\ Frank, Jo$\tilde{\hbox{a}}$o Ver\'issimo, Dario Paape,   Bruno Nicenboim, Steven Goodman, Keith O'Rourke, Daniel Lakeland, and Martha Smith for helpful discussions. For partial 
support of this research, we thank the Volkswagen Foundation through grant 89 953, the Deutsche Forschungsgemeinschaft, Collaborative Research Center (SFB) 1287 (\textit{Limits of Variability in Language}), project Q (PIs Shravan Vasishth and Ralf Engbert), which partly funded Lena J\"ager, and B03 (PIs Ralf Engbert and Shravan Vasishth), which partly funded Daniela Mertzen; and the U.S.\ Office of Naval Research through grant N00014-15-1-2541.

\printbibliography

\clearpage

\appendix


\section{How the statistical significance filter leads to inflated estimates of power}  \label{app:a}

Assume for simplicity the case that we carry out a one-sided statistical test where the null hypothesis is that the true mean is $\mu_0=0$ and the alternative is that $\mu>0$.\footnote{The presentation below generalizes to the two-sided test.}
Given some continuous data $x_1,\dots,x_n$ (such as reading times), we can compute the t-statistic and derive the p-value from it. For a large sample size $n$, a normal approximation allows us to use the z-statistic, $Z=\frac{\bar X-\mu_0}{ \sigma_X/\sqrt{n}}$, to compute the p-value. Here, $\bar{X}$ is the mean estimated from the data,  $\sigma_X$ the standard deviation, and $n$ the sample size. 

One informal definition of the p-value is the following: ``A p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.'' \parencite{pvals}.
The p-value is itself a random variable $P$ with the probability density function \parencite{hung1997behavior}: 

\begin{equation}
g_\delta(p) = \frac{\phi(Z_p - \delta)}{\phi(Z_p)},\quad 0<p<1
\end{equation}

\noindent
where 
\begin{itemize}
\item
$\phi(\cdot)$ is the pdf of the standard normal distribution, Normal(0,1).
\item
$Z_p$, a random variable, is the (1-p)th percentile of the standard normal distribution. %This determines the p-value: $p=1-\int_{-\infty}^{Z_p} \phi(x)\, dx$.
\item
$\delta=\frac{\mu-\mu_0}{\sigma_X/\sqrt{n}}$ is 
the true point value expressed as a z-score. Here, $\mu$ is the true (unknown) point value of the parameter of interest.
\end{itemize}

\textcite{hung1997behavior} further observe that the cumulative distribution function (cdf) of $P$ is:

\begin{equation}
G_\delta(p) = \int_0^p g_\delta(x)\, dx = 1- \Phi(Z_p-\delta),\quad 0<p<1
\end{equation}

\noindent
where $\Phi(\cdot)$ is the cdf of the standard normal.

Once we have observed a particular z-statistic $z_p$, 
the cdf $G_\delta(p)$ allows us to estimate power based on the z-statistic \parencite{hoenigheisey}.
To estimate the p-value in the case where the null hypothesis is in fact true, let the true value be $\mu=0$. It follows that $\delta=0$. Then:

\begin{equation}
p = 1-\Phi(z_p)
\end{equation}

To estimate power from the observed $z_p$, set $\delta$ to be the observed statistic $z_p$, and let the critical z-score be $z_\alpha$, where $\alpha$ is the Type I error (typically $0.05$).
The power is therefore:

\begin{equation} \label{powerequation}
G_{z_p}(\alpha) = 1- \Phi(z_\alpha - z_p)
\end{equation}

In other words, power estimated from the observed statistic is a monotonically increasing function of the observed z-statistic: the larger the statistic, the higher the power estimate based on this statistic (Figure~\ref{fig:powerz}). 
Together with the common practice that only statistically significant results get published, and especially results with a large z-statistic, this leads to overestimates of power. As mentioned above, one doesn't need to actually estimate power in order to fall prey to the illusion; merely  scanning the statistically significant z-scores gives an impression of consistency and invites the inference that the effect is replicable and robust. The word ``reliable'' is frequently used in psychology, presumably with the meaning that the result is replicable and reflects reality. 

\begin{figure}[!htbp]
\centering
<<powerz,echo=FALSE,fig.width=4,fig.height=4,include=TRUE>>=
zalpha<-qnorm(0.95)
zp<-seq(0,4,by=0.01)
pval<-1-pnorm(zp)
plot(zp,(1-pnorm(zalpha-zp)),type="l",
     xlab="Unknown z-score of true effect",
     ylab="Power",
     ylim=c(0,1),axes=FALSE)
axis(1, pos=0)
axis(2, pos=0)
#plot(pval,(1-pnorm(zalpha-zp)),type="l",
#     xlab="p-value",ylab="Power")
@
\caption{The relationship between power and the unknown z-score of the true effect. Larger z-scores are easier to publish due to the statistical significance filter, and these studies therefore give a mistaken impression of higher power.}\label{fig:powerz}
\end{figure}


<<echo=FALSE>>=
n<-36
d<-1/10
s<-1
pow<-power.t.test(delta=d,sd=s,n=n,alternative = "one.sided",type="one.sample",strict=TRUE)$power
#pow<-1-pnorm(zalpha-.1/(1/sqrt(36)))
@

A direct consequence of Equation~\ref{powerequation} is that overestimates of the z-statistic will lead to overestimates of power.
For example, if we have $\Sexpr{n}$ data points, the true effect is $\Sexpr{d}$ on some scale, and standard deviation is $\Sexpr{s}$, then
statistical power is $\Sexpr{round(pow*100,0)}$\%.\footnote{This can be confirmed by running the following command using R \parencite{R}: \texttt{power.t.test(delta=0.1,sd=1,n=36,alternative = "one.sided",type="one.sample",strict=TRUE)}.}

If we now re-run the same study, collecting $36$ data points each time, and impose the condition that only statistically significant results with  Type I error probability ($\alpha$) $0.05$ are published, then only observed z-scores larger than $\Sexpr{round(zalpha,2)}$ (for  a one-sided test) would be published and the power estimate based on these z-scores must have a lower bound of

\begin{equation}
G_{Z_\alpha}(\alpha) = 1- \Phi(1.64-1.64)=0.5
\end{equation}

\noindent
Thus, in a scenario where the real power is 15\%, and only z-scores greater than or equal to $z_\alpha$ are published, 
the power estimate based on the z-score will be inflated by at least a factor of 0.5/0.15=\Sexpr{round(.5/0.15,2)}. 

Now, lower p-values are widely regarded as more ``reliable'' than p-values near the Type I error probability of $0.05$.\footnote{Treating lower p-values as furnishing more evidence against the null hypothesis reflects a misunderstanding about the meaning of the p-value; given a continuous dependent measure, when the null hypothesis that $\mu=0$ is true, under repeated sampling the p-value has a uniform distribution. This has the consequence that, when the null is true, a p-value near 0 is no more surprising than a p-value near 0.05.}    
This incorrect belief among researchers has the effect that studies with lower p-values are more likely to be reported and published, with the consequence that the inflation in power will tend to be even higher than the lower bound discussed here.

\section{Prospective power analysis for repeated measures designs}\label{appendix:powerlk13e1}

We show here how power can be computed for data that are analyzed using linear mixed models, with crossed random effects for participants and items.
Consider the LK Experiment 1 data; we can estimate all effects and variance components from this $2\times 2$ design by fitting a ``maximal'' linear mixed model and then estimating prospective power \textit{for a future study} using a range of plausible effects. In other words, this is not intended to be a post-hoc power analysis; that would provide no new information beyond the p-value \parencite{hoenigheisey}. 

When we do such a prospective power analysis, for an effect of $30$ to $50$~ms, which is close to the estimates from our meta-analysis of memory retrieval effects \parencite{JaegerEngelmannVasishth2017}, power is around $13$ to $41$\%; see Table~\ref{tab:powerlk13e1}.
If the true effect were as large as $80$ ms (this is the estimate we obtained in the LK Experiment 1 for the effect of Dative), a sample size of $28$ participants and $24$ items would lead to approximately $75$\% power.  If the true effect is even smaller than $30$ ms, obtaining power greater than $80$\% would require hundreds of participants and more items. 

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{cc}
Effect (ms) & Power (percentage) \\
 30 & 13 \\ 
 50 & 41 \\ 
 80 & 75 \\ 
  \end{tabular}
\end{center}
\caption{Estimates of prospective power for different effect magnitudes for Levy and Keller's Experiment 1. These estimates of power use estimates of variance components computed from the Levy and Keller data.}\label{tab:powerlk13e1}
\end{table}%

How exactly did we compute these power estimates? For a repeated measures design,  one convenient way is to use fake-data simulation. As an illustration, we consider how we would compute prospective power for a future replication of the LK13 Experiment 1.

\begin{enumerate}
\item
Fit a ``maximal'' linear mixed model to existing data. As an example, we fit the model to the LK Experiment 1 data below.
\item Extract all variance component estimates and fixed effects estimates (hereafter, the parameter estimates) from the fitted model. For the fixed effect of interest, choose a range of effect magnitudes that are considered realistic (this is discussed below in detail). 
\item Using the parameter estimates from the step above, and the assumed effect magnitude, 
repeatedly generate $100$ fake data sets with a particular number of participants and items,  and compute the proportion of times that the relevant predictor is ``significant'' at the specified $\alpha$ value (here, $0.05$). This is the estimated prospective power for a future study.
\item For sample size calculations with the goal of achieving 80\% power, given a range of effect magnitudes, increase the number of participants and/or items until you have 80\% power. 
\end{enumerate}

We illustrate this procedure next. In order to generate fake data from a $2\times 2$ repeated measures design with a Latin square design, we first define a function, \texttt{gen\_fake\_lnorm2x2}; see Listing~\ref{fig:fixefcode}. This function generates log-normally distributed data because the dependent variable is reading time data, and this is often assumed to be generated from a log-normal distribution.
We start by setting the number of participants and items to those used in the LK experiments (28 participants, 24 items).

\singlespacing
\begin{listing}[!htbp]
\begin{Verbatim}[numbers=left,frame=single,fontfamily=courier,fontsize=\footnotesize]
library(MASS)
gen_fake_lnorm2x2 <- function(nitem=24,nsubj=28,
                         beta=NULL,
                         Sigma_u=NULL,Sigma_w=NULL,sigma_e=NULL){
  ## prepare data frame for four condition in a latin square design:
  g1<-data.frame(item=1:nitem,
                 cond=rep(letters[1:4],nitem/4))
  g2<-data.frame(item=1:nitem,
                 cond=rep(letters[c(2,3,4,1)],nitem/4))
  g3<-data.frame(item=1:nitem,
                 cond=rep(letters[c(3,4,1,2)],nitem/4))
  g4<-data.frame(item=1:nitem,
                 cond=rep(letters[c(4,1,2,3)],nitem/4))
  ## assemble data frame:
  gp1<-g1[rep(seq_len(nrow(g1)), 
              nsubj/4),]
  gp2<-g2[rep(seq_len(nrow(g2)), 
              nsubj/4),]
  gp3<-g3[rep(seq_len(nrow(g3)), 
              nsubj/4),]
  gp4<-g4[rep(seq_len(nrow(g4)), 
              nsubj/4),]
  fakedat<-rbind(gp1,gp2,gp3,gp4)
  ## add subjects:
  fakedat$subj<-rep(1:nsubj,each=nitem)
  ## add contrast coding:
  ## main effect 1:
  fakedat$c1<-ifelse(fakedat$cond%in%c("a","b"),-1/2,1/2)
  ## main effect 2: 
  fakedat$c2<-ifelse(fakedat$cond%in%c("a","c"),-1/2,1/2)
  ## interaction:
  fakedat$c3<-ifelse(fakedat$cond%in%c("a","d"),-1/2,1/2)
  ## subject random effects:
  u<-mvrnorm(n=length(unique(fakedat$subj)),
             mu=c(0,0,0,0),Sigma=Sigma_u)
  ## item random effects
  w<-mvrnorm(n=length(unique(fakedat$item)),
             mu=c(0,0,0,0),Sigma=Sigma_w)
  ## generate data row by row:  
  N<-dim(fakedat)[1]
  rt<-rep(NA,N)
  for(i in 1:N){
    rt[i] <- rlnorm(1,beta[1] + 
                      u[fakedat[i,]$subj,1] +
                      w[fakedat[i,]$item,1] + 
                      (beta[2]+u[fakedat[i,]$subj,2]+
                         w[fakedat[i,]$item,2])*fakedat$c1[i]+
                      (beta[3]+u[fakedat[i,]$subj,3]+
                         w[fakedat[i,]$item,3])*fakedat$c2[i]+
                      (beta[4]+u[fakedat[i,]$subj,4]+
                         w[fakedat[i,]$item,4])*fakedat$c3[i],
                   sigma_e)}   
  fakedat$rt<-rt
  fakedat$subj<-factor(fakedat$subj); fakedat$item<-factor(fakedat$item)
  fakedat}
\end{Verbatim}
\caption{Function for generating log-normally distributed fake data.}\label{fig:fixefcode}
\end{listing}
\doublespacing


<<fakedatasim,echo=FALSE,include=TRUE>>=
library(MASS)
## assumes that no. of subjects and no. of items is divisible by 4.
gen_fake_lnorm2x2<-function(nitem=24,
                         nsubj=28,
                         beta=NULL,
                         Sigma_u=NULL, # subject vcov matrix
                         Sigma_w=NULL, # item vcov matrix
                         sigma_e=NULL){
  ## prepare data frame for four condition latin square:
  g1<-data.frame(item=1:nitem,
                 cond=rep(letters[1:4],nitem/4))
  g2<-data.frame(item=1:nitem,
                 cond=rep(letters[c(2,3,4,1)],nitem/4))
  g3<-data.frame(item=1:nitem,
                 cond=rep(letters[c(3,4,1,2)],nitem/4))
  g4<-data.frame(item=1:nitem,
                 cond=rep(letters[c(4,1,2,3)],nitem/4))
  
  
  ## assemble data frame:
  gp1<-g1[rep(seq_len(nrow(g1)), 
              nsubj/4),]
  gp2<-g2[rep(seq_len(nrow(g2)), 
              nsubj/4),]
  gp3<-g3[rep(seq_len(nrow(g3)), 
              nsubj/4),]
  gp4<-g4[rep(seq_len(nrow(g4)), 
              nsubj/4),]
  fakedat<-rbind(gp1,gp2,gp3,gp4)
  
  ## add subjects:
  fakedat$subj<-rep(1:nsubj,each=nitem)
  
  ## add contrast coding:
  ## main effect 1:
  fakedat$c1<-ifelse(fakedat$cond%in%c("a","b"),-1/2,1/2)
  ## main effect 2: 
  fakedat$c2<-ifelse(fakedat$cond%in%c("a","c"),-1/2,1/2)
  ## interaction:
  fakedat$c3<-ifelse(fakedat$cond%in%c("a","d"),-1/2,1/2)
  
  ## subject random effects:
  u<-mvrnorm(n=length(unique(fakedat$subj)),
             mu=c(0,0,0,0),Sigma=Sigma_u)
  
  ## item random effects
  w<-mvrnorm(n=length(unique(fakedat$item)),
             mu=c(0,0,0,0),Sigma=Sigma_w)

  ## generate data row by row:  
  N<-dim(fakedat)[1]
  rt<-rep(NA,N)
  for(i in 1:N){
    rt[i] <- rlnorm(1,beta[1] + 
                      u[fakedat[i,]$subj,1] +
                      w[fakedat[i,]$item,1] + 
                      (beta[2]+u[fakedat[i,]$subj,2]+
                         w[fakedat[i,]$item,2])*fakedat$c1[i]+
                      (beta[3]+u[fakedat[i,]$subj,3]+
                         w[fakedat[i,]$item,3])*fakedat$c2[i]+
                      (beta[4]+u[fakedat[i,]$subj,4]+
                         w[fakedat[i,]$item,4])*fakedat$c3[i],
                   sigma_e) 
  }   
  fakedat$rt<-rt
  fakedat$subj<-factor(fakedat$subj)
  fakedat$item<-factor(fakedat$item)
  fakedat}
@

Then, we fit a linear mixed model to the Levy and Keller Experiment 1 data (log-transformed) to obtain estimates of all the variance components and fixed effects. These estimates will then be used for the power analysis. See Listing~\ref{listing:lkfit} for the parameter estimates from the LK Experiment 1 data.

\singlespacing
\begin{listing}[!htbp]
\begin{Verbatim}[numbers=left,frame=single,fontfamily=courier,fontsize=\footnotesize]
Linear mixed model fit by REML ['lmerMod']
Formula: 
log(region7) ~ dat + adj + int + (dat + adj + int | subj) + (dat +  
    adj + int | item)
   Data: reading_time_nozeros

REML criterion at convergence: 1079.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.70628 -0.66295  0.02944  0.62573  3.10073 

Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 subj     (Intercept) 0.147897 0.38457                   
          dat         0.016867 0.12987   0.28            
          adj         0.007536 0.08681   0.15  0.88      
          int         0.014489 0.12037  -0.47  0.69  0.77
 item     (Intercept) 0.025317 0.15911                   
          dat         0.033824 0.18391   0.11            
          adj         0.013480 0.11610   0.33 -0.89      
          int         0.019574 0.13991  -0.34  0.13 -0.16
 Residual             0.233152 0.48286                   
Number of obs: 660, groups:  subj, 28; item, 24

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.27331    0.08180  76.690
dat          0.14966    0.05855   2.556
adj          0.01296    0.04740   0.273
int         -0.03357    0.05245  -0.640

Correlation of Fixed Effects:
    (Intr) dat    adj   
dat  0.132              
adj  0.112 -0.159       
int -0.257  0.170  0.069
\end{Verbatim}
\caption{Output of the linear mixed model fit to Levy and Keller's experiment 1 data.}\label{listing:lkfit}
\end{listing}
\doublespacing

<<reanalyzeE1,warning=FALSE,echo=FALSE,include=TRUE>>=
reading_time <- read.table('../data/OrigLevyKellerData/prediction_experiment_data/experiment1/lmr/results/exp1_tt_r.res', header=TRUE)

condition<-ifelse(reading_time$dat=="sub" & reading_time$adj=="sub","a",
                  ifelse(reading_time$dat=="sub" & reading_time$adj=="main","b",
                         ifelse(reading_time$dat=="main" & reading_time$adj=="sub","c", 
                                ifelse(reading_time$dat=="main" & reading_time$adj=="main","d","NA"))))

reading_time$condition<-factor(condition)

# contrast coding: 
reading_time$dat<-ifelse(reading_time$condition%in%c("a","b"),1/2,-1/2)
reading_time$adj<-ifelse(reading_time$condition%in%c("b","d"),-1/2,1/2)
reading_time$int<-ifelse(reading_time$condition%in%c("b","c"),-1/2,1/2)

                 ## ME DAT ## ME PP-ADJ ## INT
# a DAT-SC; PP-SC    0.5         0.5       0.5    
# b DAT-SC; PP-MC    0.5        -0.5      -0.5
# c DAT-MC; PP-SC   -0.5         0.5      -0.5
# d DAT-MC; PP-MC   -0.5        -0.5       0.5

# remove zeros
reading_time_nozeros <- reading_time[reading_time$region7 != 0,]

library(lme4)
m<-lmer(log(region7) ~ dat+adj+int + 
          (dat+adj+int|subj) + 
          (dat+adj+int|item), 
        data=reading_time_nozeros)
@

Next, we set the parameters for the fake-data simulation using the above model's results. See Listing~\ref{listing:setpars}.

\singlespacing
\begin{listing}[!htbp]
\begin{Verbatim}[numbers=left,frame=single,fontfamily=courier,fontsize=\footnotesize]
# Extract parameter estimates:
beta<-round(summary(m)$coefficients[,1])
sigma_e<-round(attr(VarCorr(m),"sc"))

## assemble variance covariance matrix for subjects:
subj_ranefsd<-round(attr(VarCorr(m)$subj,"stddev"))
subj_ranefcorr<-round(attr(VarCorr(m)$subj,"corr"),1)
## choose some intermediate values for correlations:
corr_matrix<-(diag(4) + matrix(rep(1,16),ncol=4))/2
Sigma_u<-SIN::sdcor2cov(stddev=subj_ranefsd,corr=corr_matrix)

## assemble variance covariance matrix for items:
item_ranefsd<-round(attr(VarCorr(m)$item,"stddev"))
Sigma_w<-SIN::sdcor2cov(stddev=item_ranefsd,corr=corr_matrix)
\end{Verbatim}
\caption{Fix parameters for fake-data simulation based on the linear mixed model fit of LK Experiment 1.}\label{listing:setpars}
\end{listing}
\doublespacing

<<setparameters,echo=FALSE,include=TRUE>>=
## set true parameter values:
beta<-round(summary(m)$coefficients[,1],4)
sigma_e<-round(attr(VarCorr(m),"sc"),4)
subj_ranefsd<-round(attr(VarCorr(m)$subj,"stddev"),4)
subj_ranefcorr<-round(attr(VarCorr(m)$subj,"corr"),1)
## choose some intermediate values for correlations:
corr_matrix<-(diag(4) + matrix(rep(1,16),ncol=4))/2

## assemble variance matrix for subjects:
Sigma_u<-SIN::sdcor2cov(stddev=subj_ranefsd,corr=corr_matrix)

item_ranefsd<-round(attr(VarCorr(m)$item,"stddev"),4)

## assemble variance matrix for items:
Sigma_w<-SIN::sdcor2cov(stddev=item_ranefsd,corr=corr_matrix)
@

Finally, we simulate data $100$ times, for a range of effect magnitudes ($30$, $50$, and $80$ ms), $28$ participants and $24$ items, and estimate power for each effect magnitude. See Listing~\ref{listing:sim}.

\singlespacing
\begin{listing}[!htbp]
\begin{Verbatim}[numbers=left,frame=single,fontfamily=courier,fontsize=\footnotesize]
set.seed(4321)
nsim<-100
## effect size ranging from 30 to 80 ms on log scale:
## e.g.: exp(6.3+.056/2)-exp(6.3-.056/2)=30 ms
(beta2<-c(0.056,0.095,0.155))
tvalsc1<-tvalsc2<-tvalsc3<-matrix(rep(NA,nsim*length(beta2)),ncol=nsim)
failed<-matrix(rep(0,nsim*length(beta2)),ncol=nsim)
for(j in 1:length(beta2)){
for(i in 1:nsim){
  beta[2]<-beta2[j]
  dat<-gen_fake_lnorm2x2(nitem=24,
                         nsubj=28,
                       beta=beta,
                       Sigma_u=Sigma_u,
                       Sigma_w=Sigma_w,
                      sigma_e=sigma_e)

## no correlations estimated to avoid convergence problems:  
m<-lmer(log(rt) ~ c1+c2+c3 + (c1+c2+c3||subj) + 
          (c1+c2+c3||item), data=dat)

## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
  failed[j,i]<-1
} else{
tvalsc1[j,i]<-summary(m)$coefficients[2,3]
tvalsc2[j,i]<-summary(m)$coefficients[3,3]
tvalsc3[j,i]<-summary(m)$coefficients[4,3]
}}}
## proportion of convergence failures:
rowMeans(failed)
## estimate power:
pow<-rep(NA,length(beta2))
for(k in 1:length(beta2)){
  pow[k]<-mean(abs(tvalsc1[k,])>2,na.rm=TRUE)
}
\end{Verbatim}
\caption{Simulate data and compute power for different effect sizes.}\label{listing:sim}
\end{listing}
\doublespacing

<<simulatedata,echo=FALSE,include=FALSE,cache=TRUE>>=
set.seed(4321)
nsim<-100
## effect size ranging from 30 to 80 ms:
(beta2<-c(0.056,0.095,0.155))
estc1<-tvalsc1<-tvalsc2<-tvalsc3<-matrix(rep(NA,nsim*length(beta2)),ncol=nsim)
failed<-matrix(rep(0,nsim*length(beta2)),ncol=nsim)
for(j in 1:length(beta2)){
for(i in 1:nsim){
  beta[2]<-beta2[j]
  dat<-gen_fake_lnorm2x2(nitem=24,
                         nsubj=28,
                       beta=beta,
                       Sigma_u=Sigma_u,
                       Sigma_w=Sigma_w,
                      sigma_e=sigma_e)

## no correlations estimated to avoid convergence problems: 
## analysis done after log-transforming:  
m<-lmer(log(rt) ~ c1+c2+c3 + (c1+c2+c3||subj) + 
          (c1+c2+c3||item), data=dat)
estc1[j,i]<-round(summary(m)$coefficients[2,1],4)
## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
  failed[j,i]<-1
} else{
tvalsc1[j,i]<-summary(m)$coefficients[2,3]
tvalsc2[j,i]<-summary(m)$coefficients[3,3]
tvalsc3[j,i]<-summary(m)$coefficients[4,3]
}}}
## proportion of convergence failures:
rowMeans(failed)
@



<<powercalculation,eval=TRUE,echo=FALSE,include=FALSE>>=
pow<-rep(NA,length(beta2))
for(k in 1:length(beta2)){
  pow[k]<-mean(abs(tvalsc1[k,])>2,na.rm=TRUE)
}
@

\section{Estimates from 12 reading studies on facilitation effects, and model predictions}\label{app:agrmt}

\textcolor{black}{Figure~\ref{fig:agrmt} shows the 95\% credible intervals for ten agreement attraction studies that were part of the meta-analysis in \textcite{JaegerEngelmannVasishth2017}; and two recently published studies on semantic plausibility effects \parencite{CunningsSturt2018}. The number agreement studies (one was an eyetracking study and the rest were self-paced reading) investigated ungrammatical sentences such as \textit{The key to the cabinet/cabinets are on the table}. The reading time was either recorded at the critical (the auxiliary \textit{are}) or post-critical region. Theory \parencite{EngelmannJaegerVasishthSubmitted2018} predicts a facilitation effect at the auxiliary or the following region when the noun preceding the auxiliary is \textit{cabinets} vs.\ \textit{cabinet}.
The two semantic plausibility studies investigated by \textcite{CunningsSturt2018} involved implausible sentences like \textit{Sue remembered the letter that the butler with the cup/tie accidently shattered today in the dining room.} These are implausible because letters can't shatter. Here, theory  predicts a facilitation effect at \textit{shattered} due to misretrieval of the non-subject \textit{cup} (vs.\ \textit{tie}) \parencite[details are discussed in][]{EngelmannJaegerVasishthSubmitted2018}.
In the number agreement experiments,  
study 1 is the ungrammatical agreement data from Experiment 1 of \textcite{Dillon-EtAl-2013}; studies 2-5 are the experiments reported in \textcite{lago2015agreement}, and 6-10 are from \textcite{wagersetal}. Studies 11 and 12 are from \textcite{CunningsSturt2018}. The estimates shown in the figure were computed by fitting a linear mixed model (with full covariance matrices for random effects) in Stan using log-transformed reading times, and then by back-transforming the estimate of the facilitation effect to milliseconds. Our estimates may be slightly different from the original published estimates in some cases because we did not remove any data.}
\textcolor{black}{The ten studies' mean estimates range from \Sexpr{round(min(posteriors[,2]))} to \Sexpr{round(max(posteriors[,2]))}, with credible intervals ranging in width from 
\Sexpr{round(min(posteriors[,4]-posteriors[,3]))} to \Sexpr{round(max(posteriors[,4]-posteriors[,3]))} ms. Again, our interest here is not in whether effects were significant or not significant---only one of these 10 studies would show a significant effect if a p-value were to be computed.  Rather, what's remarkable here is the wide variation in the estimates of the mean effect, and the large uncertainty in many of the estimates expressed by the 95\% credible intervals.}

\begin{figure}[!htbp]
\centering
<<figagrmtattrn,echo=FALSE,fig.width=6,fig.height=4,include=TRUE,warning=FALSE>>=
posteriors$data_model<-c(rep("data",12),"model")

posteriors$expt<-factor(posteriors$expt,levels=c(levels(posteriors$expt),"model"))

pd<-position_dodge(0.6)

plot_targetmismatch<-ggplot(posteriors, aes(x=expt,
                                            y=mean, 
                             group=expt,shape=data_model)) +
  geom_errorbar(aes(ymin=lower, ymax=upper),
                width=.25, size=.5, position=pd) +
  scale_x_discrete(labels=levels(posteriors$expt[1:12]))+
  labs(title="Subject-verb dependencies") +
  #scale_x_continuous(limits = c(0, 11))+
  xlab("Experiment id / LV05 model")+
  ylab("Estimates (ms)")+
    theme_bw()+
  scale_shape_discrete(labels=c("data",
                                "model"))+
  theme(legend.position=c(0.8,0.2)) +
  #theme(legend.title=element_text("data / model"))+
  #guide_legend(title="data / model")+
  geom_hline(yintercept=0)+
  geom_point(position=pd, size=2)
#+coord_flip()

num_subj<-as.vector(posteriors$n_subj)
num_item<-as.vector(posteriors$n_item)

annsze<-3
plot_targetmismatch+annotate("text", x = 1:13, y = 25, label = num_subj,size=annsze)+
  annotate("text", x = 1:13, y = 15, label = num_item,size=annsze)+annotate("text", x=1,y= 30, label="subj",size=annsze)+annotate("text", x=1,y= 20, label="item",size=annsze)+    magnifytext()
@
\caption{The means and 95\% credible intervals of the predicted facilitation effect from 10 published studies on  subject-verb dependencies with number agreement \parencites{Dillon-EtAl-2013,lago2015agreement,wagersetal}, and two studies on subject-verb dependencies with a semantic plausibility manipulation \parencite{CunningsSturt2018}, and model predictions from the Lewis \& Vasishth (2005) model for these configurations. Also shown are the number of participants (subj) and the number of items (item) in each study.}\label{fig:agrmt}
\end{figure}


%\section{An example of how to determine sample size for a given precision}

<<rawrtanalyses,eval=FALSE,echo=FALSE,include=FALSE>>=
summary(SPRLK1critm1)
summary(SPRLK1postcritm1post)
summary(m2)
summary(m6)
summary(m_E4ETTFTrawcrit)
summary(m_E4ETTFTrawpostcrit)
summary(mE6etcritraw)
summary(mE6etpostcritraw)
summary(msprmergedcritraw)
summary(msprmergedpostcritraw)
@

\clearpage
\section{Word length and frequency effects in the eyetracking data} \label{appendix:freqwleffects}

Because we found almost no effects in the eyetracking studies, a legitimate concern is that there may have been a systemic problem in the data-collection. We therefore checked whether the well-known word length and word frequency effects on reading time \parencite{kliegl2006tracking} can be seen in all the four eyetracking data sets. If word length and frequency effects cannot be found, then there would be something fundamentally wrong with the data.
We extracted type-frequencies (occurrences of a type per million tokens) of all words occurring in a filler item from the  dlexDB database \parencite{Heister2011}, which is based on the reference corpus underlying the  Digital Dictionary of the German Language (DWDS)  \parencite{dwds}.
We only investigated first-pass reading time. Linear mixed models were fit using \texttt{lme4} with centered log frequency and centered word length as predictors, with all variance components but without intercept-slope correlations for random effects. The results are shown in Table~\ref{tab:freq}; there are clear effects of word length and frequency, in the expected directions. 
Thus, our data do have the basic characteristics of eyetracking data. Obviously, we cannot entirely rule out that there may be important systematic differences between the original studies and ours that could explain why only effects in the original work passed the statistical significance filter. But this is a  limitation of any replication attempt.

<<wordlenfreq,cache=TRUE,echo=FALSE,eval=TRUE,include=TRUE,warning=FALSE,message=FALSE>>=
# Sanity check on filler items: Is there a frequency effect and word length effect as expected?

# Eyetracking data
e2 <- read.table('../data/Exp2fillersFreq.txt', header=TRUE)
#summary(e2)


e2$cfreq<-scale(e2$type_logFreq,scale=FALSE)
e2$clen<-scale(e2$len,scale=FALSE)

#xtabs(~itemid+cfreq,e2)

e2ma<-lmer(FPRT~cfreq+clen+(1+cfreq+clen||subject)+(1+cfreq+clen||itemid),e2)
#summary(e2ma)

e4 <- read.table('../data/Exp4fillersFreq.txt', header=TRUE)

e4$cfreq<-scale(e4$type_logFreq,scale=FALSE)
e4$clen<-scale(e4$len,scale=FALSE)

e4ma<-lmer(FPRT~cfreq+clen+(1+cfreq+clen||subject)+(1+cfreq+clen||itemid),e4)
#summary(e4ma)


e6 <- read.table('../data/Exp6fillersFreq.txt', header=TRUE)
e6$cfreq<-scale(e6$type_logFreq,scale=FALSE)
e6$clen<-scale(e6$len,scale=FALSE)

e6ma<-lmer(FPRT~cfreq+clen+(1+cfreq+clen||subject)+(1+cfreq+clen||itemid),e6)
#summary(e6ma)

e7 <- read.table('../data/Exp7fillersFreq.txt', header=TRUE)

e7$cfreq<-scale(e7$type_logFreq,scale=FALSE)
e7$clen<-scale(e7$len,scale=FALSE)

e7ma<-lmer(FPRT~cfreq+clen+(1+cfreq+clen||subject)+(1+cfreq+clen||itemid),e7)
#summary(e7ma)

est<-rbind(summary(e2ma)$coefficients[2:3,c(1,2,3)],
summary(e4ma)$coefficients[2:3,c(1,2,3)],
summary(e6ma)$coefficients[2:3,c(1,2,3)],
summary(e7ma)$coefficients[2:3,c(1,2,3)])

Expt<-paste(rep("Expt",8),rep(1:4,each=2),sep=" ")

est<-data.frame(Expt=Expt,Predictor=rep(c("Freq","Len"),4),est)

#xtable(est,digits=c(1,1,1,2,2,2))
@

\begin{table}[!htbp]
\centering
\begin{tabular}{llrrr}
  \hline
  ET Experiment & Predictor & Estimate & Std.Error & t-value \\ 
  \hline
Expt 1 (LK13 Expt 1) & Freq & -3 & 1 & -3 \\ 
 & Len & 22 & 2 & 15 \\ 
 Expt 2 (LK13 Expt 2) & Freq & -2 & 1 & -3 \\ 
  & Len & 22 & 1 & 16 \\ 
 Expt 3 (LoadxDist n=28) & Freq & -4 & 1 & -3 \\ 
  & Len & 21 & 1 & 16 \\ 
 Expt 4 (LoadxDist n=100) & Freq & -2 & 1 & -2 \\ 
  & Len & 24 & 1 & 17 \\ 
   \hline
\end{tabular}
\caption{The effect of centered word frequency and centered word length on first-pass reading times in the four eyetracking studies. Experiment 1 is the replication attempt of LK's Experiment 1; Experiment 2 is the replication attempt of LK's Experiment 2; and Experiments 3 and 4 are small and larger-sample experiments investigating the Load-Distance interaction.}\label{tab:freq}
\end{table}

<<saveallanalyses,echo=FALSE,eval=FALSE>>=
## just for future analyses:
save.image(file="allanalyses.Rda")
@

\end{document}

