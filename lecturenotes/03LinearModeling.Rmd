# Linear modeling 

In this chapter, we will fit linear models of the following type. Suppose $y$ is a vector of continuous responses; assume for now that it is coming from a normal distribution:

$y \sim Normal(\mu,\sigma)$

This is the simple linear model:

$y = \mu + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma)$

There are two parameters, $\mu,\sigma$, so we need priors on these. 
We expand on this simple model next.

Recall from the foundations chapter that 
the way we will conduct data analysis is as follows. 

  - Given data, specify a *likelihood function*.
  - Specify *prior distributions* for model parameters.
  - Evaluate whether model makes sense, using fake-data simulation, *prior predictive* and *posterior predictive* checks, and (if you want to claim a discovery) calibrating true and false discovery rates.
  - Using software, derive *marginal posterior distributions* for parameters given likelihood function and prior density. I.e., simulate parameters to get *samples from posterior distributions* of parameters using some *Markov Chain Monte Carlo (MCMC) sampling algorithm*.
  - Check that the model converged using *model convergence* diagnostics,
  - Summarize *posterior distributions* of parameter samples and make your scientific decision.

We will now work through some specific examples to illustrate how the data analysis process works.

## Example 1: A single subject pressing a button repeatedly \label{sec:first}

As a first example, we will fit a simple linear model to some reaction time data.


The file \verb|button_press.dat| contains data of a subject pressing the space bar without reading in a self-paced reading experiment.

### Preprocessing of the data

```{r, reading_noreading}
noreading_data <- read.table(header = FALSE,"data/button_press.dat",
                             encoding="latin1")
noreading_data <- noreading_data[c("V2","V3","V5","V6","V8")]
colnames(noreading_data) <- c("type","item","wordn","word","y")
tail(noreading_data)
summary(noreading_data$y)
class(noreading_data)
```

### Visualizing the data

It is a good idea to look at the distribution of the data before doing anything else. See Figure \ref{fig:m1visualize}.

```{r fig.cap="\\label{fig:m1visualize}Visualizing the data."}
plot(density(noreading_data$y),
     main="Button-press data",xlab="RT")
```

The data looks a bit skewed, but we ignore this for the moment.

### Define the likelihood function

Let's model the data with the following assumptions:

- There is a true underlying time, $\mu$, that the participant needs to press the space-bar.
- There is some noise in this process.
- The noise is normally distributed (this assumption is questionable given the skew but; we fix this assumption later).

This means that the likelihood for each observation $i$ will be:

\begin{equation}
\begin{aligned}
y_i \sim Normal(\mu, \sigma)
\end{aligned}
\end{equation}

where $i =1 \ldots N$.

This is just the simple linear model:

\begin{equation}
y = \mu + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma)
\end{equation}

### Define the priors for the parameters

We are going to use the following priors for the two parameters in this model:
\begin{equation}
\begin{aligned}
\mu &\sim Normal(0, 2000) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0
\end{aligned}
\end{equation}

In order to decide on a prior for the parameters, always visualize them first. See Figure \ref{fig:m1priors}.

```{r fig.cap="\\label{fig:m1priors}Visualizing the priors for example 1."}
op<-par(mfrow=c(1,2),pty="s")
x<-seq(-4000,4000,by=1)
plot(x,dnorm(x,mean=0,sd=2000),type="l",
     main=expression(paste("Prior for ",mu)),xlab=expression(mu))
x<-seq(0,4000,by=1)
plot(x,dnorm(x,mean=0,sd=500),type="l",
     main=expression(paste("Prior for ",sigma)),xlab=expression(sigma))

```

The prior for $\mu$ expresses the belief that the underlying mean button-pressing time could be both positive and negative, and given that
the scale of the prior (in this case the standard deviation of the normal
distribution) is 2000, we are $\approx 68\%$ certain that the true value would
be between 2000 ms and -2000 ms and $\approx 95\%$ certain that it would be
between -4000 ms and 4000 ms (two standard deviations away from zero). 
We know this because:

```{r}
pnorm(2000,mean=0,sd=2000)-pnorm(-2000,mean=0,sd=2000)
pnorm(4000,mean=0,sd=2000)-pnorm(-4000,mean=0,sd=2000)
```

But we
obviously know that button-pressing time can't be negative! So we have more prior
information than what we are using for informing the model. We'll discuss this
below. 

Regarding the prior for $\sigma$: The values must be positive, and we are
$\approx 68\%$ certain that the true value would be between 0 ms and 500 ms
and $\approx 95\%$ certain that it would be between 0 ms and 1000 ms. **How would you check this using pnorm?**

```{r echo=FALSE, eval=FALSE}
2*(pnorm(500,mean=0,sd=500)-pnorm(0,mean=0,sd=500))
2*(pnorm(1000,mean=0,sd=500)-pnorm(0,mean=0,sd=500))
```

### Prior predictive checks

With these priors, we are going to generate something called the **prior predictive distribution**. This helps us check whether the priors make sense.

Formally, we want to know the density $f(\cdot)$ of data points $y_1,\dots,n$, given a vector of priors $\Theta$. In our example, $\Theta=\langle\mu,\sigma \rangle$. The prior predictive density is:

\begin{equation}
f(y_1,\dots,y_n)= \int f(y_1)\cdot f(y_2)\cdots f(y_n) f(\Theta) \, d\Theta 
\end{equation}

In essence, we integrate out the parameters. Here is one way to do it in R. For the following for *nsim* iterations:

  - Take one sample from each of the priors
  - Generate *nobs* data points using those samples

This would give us a matrix containing nsim * nobs generated data. 
We can then plot the prior predictive densities generated.

```{r cache=TRUE,fig.cap="\\label{fig:m1priorpred}First attempt at prior predictive distribution of the data, model m1."}
library(extraDistr) ## needed for half-normal distribution
## number of simulations
nsim<-1000
## number of observations generated each time:
nobs<-100
y<-matrix(rep(NA,nsim*nobs),ncol = nobs)
mu<-rnorm(nsim,mean=0,sd=2000)
## truncated normal, cut off at 0:
sigma<-rtnorm(nsim,mean=0,sd=500,a=0)

for(i in 1:nsim){
y[i,]<-rnorm(nobs,mean=mu[i],sd=sigma[i])
}

op<-par(mfrow=c(3,5),pty="s")
random_sample<-sample(1:nsim,15)
for(i in random_sample){
hist(y[i,],main="",freq=FALSE)
}
```

The prior predictive distributions in Figure \ref{fig:m1priorpred} show that the prior for $\mu$ is not realistic: how can button press time have negative values?

We can try to redefine the prior for $\mu$  to have only positive values, and then check again (Figure \ref{fig:m1priorpred2}). We still get some negative values, but that is because we are assuming that 

$y \sim Normal(\mu,\sigma)$

which will have negative values for small $\mu$ and large $\sigma$. 

```{r warning=FALSE,cache=TRUE,fig.cap="\\label{fig:m1priorpred2}Second attempt at prior predictive distribution of the data, model m1."}
y<-matrix(rep(NA,nsim*nobs),ncol = nobs)
## positive values only:
mu<-rtnorm(nsim,mean=0,sd=2000)
## truncated normal, cut off at 0:
sigma<-rtnorm(nsim,mean=0,sd=500,a=0)

for(i in 1:nsim){
y[i,]<-rnorm(nobs,mean=mu[i],sd=sigma[i])
}

op<-par(mfrow=c(3,5),pty="s")
random_sample<-sample(1:nsim,15)
for(i in random_sample){
hist(y[i,],main="",freq=FALSE)
}
```

This prior predictive distribution in Figure \ref{fig:m1priorpred2} is good enough for now; we will improve on this later. 

#### Generating prior predictive data using Stan

Incidentally, we can also generate a prior predictive distribution using Stan as follows.

First, we define a Stan model that defines the priors and defines how the data are to be generated. The details of the code below will be explained in class. Documentation on Stan is available at mc-stan.org.

```{r}
priorpred<-"data {
  int N;
}
parameters {
real<lower=0> mu; //enforce positive mu
real<lower=0> sigma;
}
model {
  // priors
	mu ~ normal(0,2000);
	sigma ~ normal(0,500);
}
generated quantities {
  vector[N] y_sim;
  // prior predictive
  for(i in 1:N) {
    y_sim[i] = normal_rng(mu,sigma);
  }
}
"
```

Then we generate the data:

```{r warning=FALSE,message=FALSE,results="hide",cache=TRUE}
## load rstan
library(rstan)
options(mc.cores = parallel::detectCores())

## generate 100 data points
dat<-list(N=100)

## fit model:
m1priorpred<-stan(model_code=priorpred,
                  data=dat,
                  chains = 4, 
                iter = 2000)

## extract and plot one of the data-sets:
y_sim<-extract(m1priorpred,pars="y_sim")
str(y_sim)

op<-par(mfrow=c(3,5),pty="s")
random_sample<-sample(1:nsim,15)
for(i in random_sample){
hist(y_sim$y_sim[i,],main="",freq=FALSE,xlab="y_sim")
}
```


Now that we have a sense of what these priors give us, we now fit the model to fake data. The goal here is to ensure that the model recovers the true underlying parameters.

###  Fake-data simulation and modeling

Next, we write the Stan model, adding a likelihood in the model block:

```{r}
m1<-"data {
  int N;
  real y[N]; // data
}
parameters {
real<lower=0> mu;
real<lower=0> sigma;
}
model {
// priors
mu ~ normal(0,2000);
sigma ~ normal(0,500);
// likelihood
y ~ normal(mu,sigma);
}
generated quantities {
  vector[N] y_sim;
 // posterior predictive
  for(i in 1:N) {
    y_sim[i] = normal_rng(mu,sigma);
  }
}
"
```

Then generate fake data with known parameter values (we decide what these are):

```{r fake_data}
set.seed(123)
N <- 500
true_mu <- 400
true_sigma <- 125
y <- rnorm(N, true_mu, true_sigma)

y <- round(y) 
fake_data <- data.frame(y=y)
dat<-list(y=y,N=N)
```

Finally, we fit the model:

```{r results="hide",cache=TRUE}
## fit model:
m1rstan<-stan(model_code=m1,
                  data=dat,
                  chains = 4, 
                iter = 2000)

## extract posteriors:
posteriors<-extract(m1rstan,pars=c("mu","sigma"))
```

Figure \ref{fig:m1rstanpost} shows that the true parameters that we defined when generating the fake data fall within the posterior distributions. This shows that the model can in principle recover the parameters. (One should do several fake data simulations to check that the model consistently recovers the true parameters.)

```{r fig.cap="\\label{fig:m1rstanpost}Posteriors from fake data, model m1. Vertical lines show the true values of the parameters."}
op<-par(mfrow=c(1,2),pty="s")
hist(posteriors$mu,
     main=expression(paste("posterior for ",mu)),freq=FALSE,xlab="")
abline(v=400)
hist(posteriors$sigma,
     main=expression(paste("posterior for ",sigma)),freq=FALSE,xlab="")
abline(v=125)
```

### Posterior predictive checks

Once we have the posterior distribution $f(\Theta\mid y)$, we can derive the predictions based on this posterior distribution:

\begin{equation}
p(y_{pred}\mid y ) = \int p(y_{pred}, \Theta\mid y)\, d\Theta= \int 
p(y_{pred}\mid \Theta,y)p(\Theta\mid y)\, d\Theta
\end{equation}

Assuming that past and future observations are conditionally independent given $\Theta$, i.e., $p(y_{pred}\mid \Theta,y)= p(y_{pred}\mid \Theta)$, we can write:

\begin{equation}
p(y_{pred}\mid y )=\int p(y_{pred}\mid \Theta) p(\Theta\mid y)\, d\Theta
\end{equation}

Note that we are conditioning $y_{pred}$ only on $y$, we do not condition on what we don't know ($\Theta$); **we integrate out the unknown parameters**.

This posterior predictive distribution is different from  the frequentist approach, which gives only a predictive distribution of $y_{pred}$ given our estimate of $\theta$ (a point value).

In the Stan code above, we have already generated the posterior predictive distribution, in the generated quantities block:

\begin{verbatim}
generated quantities {
  vector[N] y_sim;
 // posterior predictive
  for(i in 1:N) {
    y_sim[i] = normal_rng(mu,sigma);
  }
}
\end{verbatim}

### Implementing model in brms

An alternative to using rstan as we did above is the package brms. The advantage with using brms is that many of the details of model-specification are hidden from the user; the price paid is loss of transparency, and reduced flexibility in modeling. brms is a good software for fitting canned models, but for customized models you will always need Stan, so it is good to know both syntaxes. I personally do all my research using Stan, unless I am fitting a canned model, and in cases where I want the special brms functions as quick short-cuts for modeling. So, both rstan and brms are valuable tools and you should aim to be comfortable with both.

First, load the brms package; this packge runs Stan [@Stan2017] in the background. For an introduction to this package, see @brms, and https://github.com/paul-buerkner/brms. 

```{r}
library(brms)
```


This model is expressed in brms in the following way. First, define the priors:

```{r}
priors <- c(set_prior("normal(0, 2000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

Then, define the generative process assumed:

```{r cache=TRUE,warning=FALSE,message=FALSE,results="hide"}
m1brms<-brm(y~1,noreading_data,prior = priors,
       iter = 2000,
       warmup = 1000,
       chains = 4,
       family = gaussian(), 
       control = list(adapt_delta = 0.99))
```


\begin{enumerate}
\item
The term \texttt{family~=~gaussian()} makes explicit the underlying likelihood function that is implicit in \texttt{lme4}. Other linking functions are possible, exactly as in the \texttt{glmer} function in \texttt{lme4}.
\item
The term \texttt{prior} takes as argument the list of priors we defined earlier. Although this specification of priors is optional, the  researcher should always explicitly specify each prior. Otherwise, \texttt{brms} will define a prior by default, which may or may not be appropriate. 
\item 
The term \texttt{iter} refers to the number of iterations  that the sampler makes to sample from the posterior distribution of each parameter (by default 2000). See the discussion on HMC earlier.
\item 
The term \texttt{warmup} refers to the number of iterations from the start of sampling that are eventually discarded (by default half of \texttt{iter}).
\item 
The term 
\texttt{chains} refers to the number of independent runs for sampling (by default four).
\item
The term \texttt{control} refers to some optional control parameters for the sampler, such as \texttt{adapt\_delta}. Briefly, increasing \texttt{adapt\_delta} tells the sampler to take smaller steps when moving in the parameter space; it slows down sampling but that is a price one should be willing to pay if one is having convergence warnings. 

\end{enumerate}


### Summarizing the posteriors, and convergence diagnostics

The summary displayed below show summary statistics over the marginal posterior distributions of the parameters in the model. The summary shows posterior means,  standard  deviations  (`sd`),  quantiles,  Monte
Carlo standard errors (`se_mean`), Rhats, and effective sample sizes (`n_eff`).  

The summaries
are computed after removing the warmup  and merging together all chains. Notice that the `se_mean` is unrelated to the `se` of an estimate in the frequentist model, and we will ignore it in this course.  


```{r}
summary(m1brms)
```

A graphical summary of posterior distributions of model m1 is shown in Figure \ref{fig:m1stanplot}:

```{r message=FALSE,fig.cap="\\label{fig:m1stanplot}Posterior distributions of the parameters in model m1."}
stanplot(m1brms,type="hist")
```

The trace plots in Figure \ref{fig:m1traceplot} show how well the four chains are mixing:

```{r message=FALSE,fig.cap="\\label{fig:m1traceplot}Trace plots in model m1."}
stanplot(m1brms,type="trace")
```

An alternative way to plot is shown in Figure \ref{fig:m1plot}.

```{r message=FALSE,fig.cap="\\label{fig:m1plot}Posterior distributions and trace plots in model m1."}
plot(m1brms)
```



```{r fit_fake, message=FALSE,warning=FALSE,include=FALSE,eval=FALSE,cache=TRUE}

m1_fakebrms<-brm(y~1,fake_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
```


### MCMC diagnostics: Convergence problems and Stan warnings

Because we are using MCMC methods to sample from the posterior distributions, we need to make sure that the model has converged.

The most important checks or MCMC diagnostics are the following:


* The chains should look like a straight "fat hairy caterpillar": the chains should
  bounce around the same values and with the same variance.
* The R-hat statistic, $\hat{R}$s, of the parameters should
  be close to one (as a rule of thumb less than $1.1$).  This indicates that
  the chains have mixed (recall discussion on \pageref{mixing}) and they are traversing the same distribution.
  $\hat{R}$s are printed in the summary in the column `Rhat` [see
  section 11.4 of @GelmanEtAl2014]. These are the ratio of between to within chain variance.
* The effective sample size, $n_{eff}$ is an estimate of the number of independent draws from the
  posterior distribution. Since the samples are not independent, $n_{eff}$
  will generally be smaller than the total number of samples, $N$.  How large
  $n_{eff}$ should be depends on the summary statistics that we want to use.
  But as a rule of thumb, $n_{eff}/N > 0.1$.
* Check that the software does not  warnings such as divergent transitions, Bayesian
  fraction of missing information (BFMI) that was too low, etc. These warning may
  indicate that the sampler is not adequately exploring  the parameter space. If you see these warnings, 
 consult http://mc-stan.org/misc/warnings.html

For useful graphical visualizations see https://mc-stan.org/users/interfaces/shinystan

Any warnings **should not be ignored**! See the Appendix \ref{sec:Appendix} for some troubleshooting ideas to solve them.

### Summarizing the posterior distribution: posterior probabilities and the credible interval

We are assuming that there's a true underlying time it takes to press the space bar, $\mu$, and there is normally distributed noise with distribution Normal(0,$\sigma$) that generates the different RTs. 
All this is encoded in our likelihood by assuming that RTs are distributed with an unknown true mean $\mu$ (and an unknown standard deviation $\sigma$). 

The objective of the Bayesian model is to learn about the plausible values of $\mu$, or in other words, to get a distribution that encodes what we know about the true mean of the distribution of RTs, and about the true standard deviation, $\sigma$, of the distribution of RTs.

Our model allows us to answer questions such as:

**What is the probability that the underlying value of the mindless press of
the space bar would be over, say 170 ms?**

As an example, consider this model that we ran above:

```{r cache=TRUE}
priors <- c(set_prior("normal(0, 2000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))

m1brms<-brm(y~1,noreading_data,prior = priors,
       iter = 2000,
       warmup = 1000,
       chains = 4,
       family = gaussian(), 
       control = list(adapt_delta = 0.99))
```

We now compute the posterior probability  $Prob(\mu>170)$:

```{r}
mu_post<-posterior_samples(m1brms,pars=c("b_Intercept"))$b_Intercept
mean(mu_post>170)
```

**The credible interval**

The 95% credible interval can be extracted for $\mu$ as follows:

```{r}
posterior_interval(m1brms,pars=c("b_Intercept"))
```


This type of interval is also known as a *credible interval*. 
A credible interval demarcates the range within which we can be certain with a certain probability that the "true value" of a parameter lies given the data and the model.
This is very different from the frequentist confidence interval! See for discussion, @HoekstraEtAl2014 and @MoreyEtAl2015.

The percentile interval is a type of credible interval (the most common one), where we assign equal probability mass to each tail. We generally report 95% credible intervals. But we can extract any interval, an 80% interval, for example, leaves `r (1.00-.20)/2*100`% of the probability mass on each tail, and we can calculate it like this:

```{r}
quantile(mu_post,prob=c(0.10,0.90))
```



### Influence of priors and sensitivity analysis

Everything was normally distributed in our example (or truncated normal), but the fact that we assumed that RTs were normally distributed is completely unrelated to our (truncated) normally distributed priors. Let's try a  uniform prior on $\mu$  with a low boundary of 0 and a high boundary of 5000. Here, we assume that every value between 0 and 5000 is equally likely. In general, this is a bad idea for two reasons: (i) it is computationally expensive (the sampler has a larger parameter space to search), and (ii) it is providing information that we know is not sensible (every value between 0 and 5000 cannot be equally likely). But in our very simple example these priors will give use the same posterior as with the normal priors.


\begin{equation}
\begin{aligned}
\mu &\sim Uniform(0,5000) \\
\sigma &\sim Uniform(0, 5000) 
\end{aligned}
\end{equation}

```{r}
priors <- c(set_prior("uniform(0, 5000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m2<-brm(y~1,noreading_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
summary(m2)
```

In general, we don't want our priors to have too much influence on our
posterior. This is unless we have *very* good reasons for having informative
priors, such as a very small sample and a lot of prior information; an example
would be if we have data from an impaired population, which makes it hard to
increase our sample size. 

We usually center the priors on 0 and we let the
likelihood dominate in determining the posterior. This type of
prior is called *weakly informative prior*. Notice that a uniform prior is
not a weakly informative prior, it assumes that every value is equally
likely, zero is as likely as 5000. 

You should always do a 
*sensitivity analysis* to check how influential the prior is: try different priors and verify that the posterior
doesn't change drastically [for a published  example, see
@VasishthetalPLoSOne2013]. 

## Example 2: Investigating adaptation effects \label{sec:moreinter}

More realistically, we might have run the small experiment to find out whether the
participant tended to speedup (practice effect) or slowdown (fatigue effect) while pressing the space bar. 

### Preprocessing the data

We need to have data about the number of times the space bar was
pressed for each observation, and add it to our list. It's a good idea to
center the number of presses (a covariate) to have a
clearer interpretation of the intercept. In general, centering predictors is always a good idea, for interpretability and for computational reasons. See @SchadEtAlcontrasts for details on this point.

```{r, reading_noreading_sb}
# Create the new column in the data frame
noreading_data$presses <- 1:nrow(noreading_data)
# Center the column
noreading_data$c_presses <- noreading_data$presses - mean(noreading_data$presses)
```

### Probability model


Our model changes, because we have a new parameter. 

\begin{equation}
y_i \sim Normal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


And we are going to use the following priors:

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 2000) \\
\beta &\sim Normal(0, 500) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0 \\
\end{aligned}
\end{equation}


We are basically fitting a linear model, $\alpha$ represents the intercept (namely, the grand mean of the RTs), and $\beta$ represents the slope. What information are the priors encoding? Do the priors make sense?



We'll write this in brms as follows.

```{r results="hide"}
priors <- c(set_prior("normal(0, 2000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "b",
                      coef="presses"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m2<-brm(y~1+presses,noreading_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
summary(m2)
```

### Summarizing the posterior and inference

How can we answer our research question? What is the effect of pressing the
bar on the participant's reaction time?


We'll need to examine what happens with $\beta$. The summary gives us the relevant information:

```{r}
m2_post_samp_b <- posterior_samples(m2, "^b")
beta_samples <- m2_post_samp_b$b_presses 
beta_mean<-mean(beta_samples)
quantiles_beta <- quantile(beta_samples,prob=c(0.025,0.975))
beta_low<-quantiles_beta[1]
beta_high<-quantiles_beta[2]
```

We learn that the most likely values of $\beta$ will be around the mean of the posterior `r beta_mean`, and we can be 95% certain that the true value of $\beta$ *given the model and the data* lies between `r beta_low` and `r beta_high`. 

We see that as the number of times the space bar is pressed increases, the participant becomes slower. If we want to determine how  likely it is that the participant was slower rather than faster, we can examine the proportion of samples above zero:

```{r beta_samples}
mean(beta_samples > 0)
```

We could report this in a paper as $\hat\beta = `r beta_mean`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$, $P(\beta >0)  \approx `r mean(beta_samples > 0)`$. Generally, I don't report the last probability shown, because it confuses people who are used to seeing p-values (they tend to misinterpret this posterior probability as a p-value; so it's better not to show it at all). Plotting the posterior as a histogram is always a good idea. Example papers from my lab where we only display the posterior with a credible interval are @VasishthMertzenJaegerGelman2018, @ALV2019, and @JaegerMertzenVanDykeVasishth2019.

Can we really conclude that there is a fatigue effect? It depends on how much
we expect the fatigue to affect the RTs. Here we see that only after 100
button presses do we see a slowdown of 9 ms on average ($0.09 \times 100$). 

We need to consider whether the size of this effect has any scientific relevance by considering the
previous literature. Sometimes this requires a meta-analysis. See @JaegerEtAl2017, @NicenboimRoettgeretal for examples. 

### Posterior predictive checks

Let's say we know that our model is working as expected, since we already used fake data to test the recovery of the parameters (this will be a homework assignment). 

We will now examine the *descriptive adequacy* of the models [@ShiffrinEtAl2008;
@GelmanEtAl2014, Chapter 6]: the observed data should look plausible under the
*posterior predictive distribution*, as discussed above. The posterior predictive distribution is
composed of one dataset for each sample from the posterior. (So it will
generate as many datasets as iterations we have after the warm-up.)  Achieving
descriptive adequacy means that the current data could have been predicted
by the model. Passing a test of descriptive adequacy is not strong evidence
in favor of a model, but a major failure in descriptive adequacy can be
interpreted as strong evidence against a model [@ShiffrinEtAl2008].


To do posterior predictive checks for our last example, using brms, we need to do:

```{r fig.cap="\\label{fig:m2ppc}Posterior predictive check of model m2."}
pp_check(m2, nsamples = 100)+
  theme(text = element_text(size=16),
        legend.text=element_text(size=16))
```

We'll use the values generated by our model to verify whether the general
shape of the actual distribution matches the distributions from some of the
generated datasets. Let's compare the real data against 100 of the predicted 4000
datasets.

*Are the posterior predicted data similar to the real data?*

Figure \ref{fig:m2ppc} shows that 
our dataset seems to be more skewed to the right than  our predicted
datasets. This is not too surprising, we assumed that the likelihood was a
normal distribution, but latencies are not very normal-like, they can't be
negative and can be arbitrarily long.

### Using the log-normal likelihood

Since we know that the latencies shouldn't be normally distributed, we can choose a more realistic distribution for the likelihood. A good candidate is the log-normal distribution since a variable (such as time) that is log-normally distributed takes only positive real values. 

If $Y$ is log-normally distributed, this means that $log(Y)$ is normally distributed.^[In fact, $log_e(Y)$ or $ln(Y)$, but  we'll write it as just $log()$] Something important to notice is that the log-normal distribution is defined using again $\mu$ and $\sigma$, but this corresponds to the mean and standard deviation of the normally distributed logarithm $log(Y)$.  Thus $\mu$ and $\sigma$ are on a different scale than the variable that is log-normally distributed. 

This also means that you can create a log-normal distribution by exponentiating the samples of a normal distribution. See Figure \ref{fig:logndemo}.



```{r lognormal,fig.height=5,fig.width=5,fig.cap="\\label{fig:logndemo}The log-normal distribution.", message=FALSE, warning=FALSE}
mu <- 6
sigma <- 0.5
N <- 100000
# Generate N random samples from a log-normal distribution
sl <- rlnorm(N, mu, sigma)
lognormal_plot <- ggplot(data.frame(samples=sl), aes(sl)) + geom_histogram() + 
      ggtitle("Log-normal distribution\n") +
  ylim(0,25000) + xlim(0,2000)
# Generate N random samples from a normal distribution, 
# and then exponentiate them
sn <- exp(rnorm(N, mu, sigma))
normalplot <- ggplot(data.frame(samples=sn), aes(sn)) + 
  geom_histogram() + 
      ggtitle("Exponentiated samples of\na normal distribution") + 
  ylim(0,25000) + xlim(0,2000)

source("R/multiplot.R")
multiplot(lognormal_plot,normalplot,cols=)
```



### Re-fit the model assuming a log-normal likelihood

If we assume that RTs are log-normally distributed, we'll need to change our model:

\begin{equation}
Y_i \sim LogNormal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


But now the scale of our priors needs to change! They are no longer in milliseconds.

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 10) \\
\beta &\sim Normal(0, 1) \\
\sigma &\sim Normal(0, 2) \text{ truncated so that } \sigma > 0 \\
\end{aligned}
\end{equation}

The interpretation of the parameters changes and it is more
complex than if we were dealing with a linear model that assumes  a normal likelihood:

* $\alpha$. In our previous linear model, $\alpha$ represented the grand mean (or the grand median since in a normal distribution both coincide), and was equivalent to our previous $\mu$ (since $\beta$ was multiplied by 0). But now, the grand mean needs to be calculated in the following way,  $\exp(\alpha +\sigma ^{2}/2)$. Interestingly, the grand median will  just be $exp(\alpha)$,^[You can check in Wikipedia (https://en.wikipedia.org/wiki/Log-normal_distribution) why.] and we could assume that this represents the underlying time it takes to press the space bar if there would be no noise, that is, if $\sigma$ had no effect. This also means that the prior of $\alpha$ is not in milliseconds, but in log(milliseconds).

*  $\beta$. In a linear model, $\beta$ represents the slowdown for each time the space bar is pressed. Now $\beta$ is the effect on the log-scale, and the effect in milliseconds depends on the intercept $\alpha$: $\exp(\alpha + \beta) - \exp(\alpha)$. Notice that the log is not linear and the effect of  $\beta$ will have more impact on milliseconds as the intercept grows. For example, if we start with (i) $\exp(5) = 148$, and we add $0.1$ in log-scale, $\exp(5 + 0.1) = 164$, we end up with a difference of 15 ms; if we start with (ii) $\exp(6) = 400$, and we add $0.1$, $\exp(6 + 0.1) = 445$, we end up with a difference of 45 ms. You can also see this graphically below, in Figure \ref{fig:logexp}.

*  $\sigma$. This is the standard deviation of the normal distribution of $log(y)$.

<!-- ($exp(10+.1) - exp(10) >  exp(1+.1) - exp(1)$, that is, $`r exp(10+.1) - exp(10)` > `r exp(1+.1) - exp(1)`$). -->

```{r fig.cap="\\label{fig:logexp}Back-transforming an effect of 0.1 log milliseconds to milliseconds.",echo=TRUE, warning=FALSE}

ms_diff <- function(Intercept){
  exp(Intercept + .1) - exp(Intercept)
}
df <- tibble::data_frame(Intercept=seq(.1,15,.01), 
                         ms= ms_diff(Intercept))
ggplot(df, aes(x=Intercept,y=ms)) + geom_point() +
  scale_y_continuous("Effect in milliseconds \n  (log-scale effect: 0.1)")
```

### What kind of information are the priors encoding?


* For $\alpha$: We are 95% certain that the grand median of the RTs will be between $\approx `r exp(-10*2)`$ and $`r round(exp(10*2))`$ milliseconds. This is a (very) weakly regularizing prior  because it won't affect our results, but it will down-weight values for the grand median of the RTs that are extremely large, and won't allow the grand median to be negative. We calculate the previous range by back-transforming the values that lie between two standard deviations of the prior ($2 \times 10$) to millisecond scale: $exp(-10 \times 2)$ and $exp(10 \times 2)$).

* For $\beta$: This is more complicated, because the effect on milliseconds will depend on the estimate of $\alpha$. However, we can assume some value for $\alpha$ and it will be enough to be in the right order of magnitude. So let's assume 500 ms. That will mean that we are 95% certain that the effect of pressing the space bar will be between $`r round(exp(log(500) - 4)) - 500`$ and $`r round(exp(log(500) + 4)) - 500`$ milliseconds. It is asymmetric because the log-scale is asymmetric. But the prior is weak enough so that if we assume 1000 or 100 instead of 500, the possible estimates of $\beta$ will still be contained in the prior distribution.  We calculate this by first finding out the value in milliseconds when we are two standard deviations away in both directions: ($2\times 2$), that is $\exp(\log(500) - 2 \times 2)$ and  $\exp(\log(500) + 2 \times 2)$, and we subtract from that the value of $\alpha$ that we assumed, $500$:   $\exp(\log(500) - 2 \times 2) - 500$ and  $\exp(\log(500) + 2 \times 2) - 500$.

*  For $\sigma$. This indicates that we are 95% certain that the standard deviation of $log(y)$ will be between 0 and 2. So 95% of the RTs  will be between $\exp(\log(500) - 1 \times 2) = `r round(exp(log(500) - 1 * 2))`$ and $exp(log(500) + 1 \times 2) = `r round(exp(log(500) + 1 * 2))`$.

What happens if we replace 500 by 100, and by 1000? What happens if it is 10 instead? Does it still makes sense?

We'll code the model as follows. 

```{r results="hide",cache=TRUE}
priors_log <- c(set_prior("normal(0, 10)", 
                      class = "Intercept"),
            set_prior("normal(0, 1)", 
                      class = "b",
                      coef="presses"),
            set_prior("normal(0, 2)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m2_logn<-brm(y~1+presses,noreading_data,prior = priors_log,
       iter = 2000, chains = 4,family = lognormal(), 
       control = list(adapt_delta = 0.99,max_treedepth=15))
```

```{r}
summary(m2_logn)
```

We fit the model, and check its convergence as usual (this will be a homework assignment).



### Summarizing the posterior and inference

Next, we turn to the question of what we can report as our results, and what we can conclude from the data.

```{r, echo=F, results="hide"}
options(scipen=999, digits=6)

alpha_samples<-posterior_samples(m2_logn,"^b")$b_Intercept

beta_samples<-posterior_samples(m2_logn,"^b")$b_presses

beta_ms<-exp(alpha_samples+beta_samples)-exp(alpha_samples)

beta_msmean <- round(mean(beta_ms),5)
beta_mslow <- round(quantile(beta_ms,prob=0.025),5)
beta_mshigh <- round(quantile(beta_ms,prob=0.975),5)

beta_mean <- round(mean(beta_samples),5)
beta_low <- round(quantile(beta_samples,prob=0.025),5)
beta_high <- round(quantile(beta_samples,prob=0.975),5)
```


We can summarize the posterior and do inference as discussed in Example 1. If we want to talk about the effect estimated by the model, we summarize the posterior of $\beta$ in the following way: $\hat\beta = `r mean(beta_samples)`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$.

But in most cases, the effect is easier to interpret in milliseconds. We generated the 
effect of 1 press in the generated quantities block, which is not the same as
the linear model's $\beta$. Our generated estimate will tell us the estimate
of the slowdown produced by pressing the space bar in the middle of the
experiment once, assuming that the RTs are log-normally distributed:
$`r beta_msmean`$ ms, 95% CrI = $[ `r beta_mslow` , `r beta_mshigh` ]$. 
Coincidentally, it is close to the
same value as before, but this is not always the case, and since it's not
linear the effect won't be the same across the whole experiment.

### Posterior predictive checks and distribution of summary statistics 

We can now verify whether our predicted datasets look more similar to the real dataset. See Figure \ref{fig:lognppc}.

```{r, message=FALSE, warning=FALSE, fig.height=4,fig.cap="\\label{fig:lognppc}Posterior predictive check."}
pp_check(m2_logn, nsamples = 100)+
  theme(text = element_text(size=16),legend.text=element_text(size=16))
```

*Are the posterior predicted data now more similar to the real data?*

It seems so, but it's not easy to tell. Another way to examine this would be
to look at the  *distribution of summary statistics*. The idea is to compare the
distribution of representative summary statistics for the datasets generated
by different models and compare them to the observed statistics. Since we
suspect that the log-normal distribution may capture the long tail,  we could
use the maximum as a summary statistics.
We could generate 100 posterior predictive data-sets, and then compute the maximum each time and plot the distribution of the maximum, comparing it with the maximum value (409) in the data.

```{r results="hide",cache=TRUE}
m2_logn<-brm(y~1+presses,noreading_data,prior = priors_log,
       iter = 2000, chains = 4,
       family = lognormal(),
       control = list(adapt_delta = 0.99,max_treedepth=15))
postsamp<-posterior_samples(m2_logn,"^b",add_chain=TRUE)

nsim<-100
maximum<-rep(NA,nsim)
for(i in 1:nsim){
ppm2_logn<-predict(m2_logn)
maximum[i]<-max(as.vector(ppm2_logn))
}
```

```{r fig.cap="\\label{fig:ppcheckmax}Distribution of maximum values in a posterior predictive check. The maximum in the data is 409 ms."}
hist(maximum,main="Distribution of maximum values",freq=FALSE)
```

Figure \ref{fig:ppcheckmax} shows that  we are unable to capture the maximum value of the observed data, so there's still room for improving the model.
We will return to this question later in the course.

### General workflow

This is the general workflow that we suggest when fitting a Bayesian model.

1. Define the full probability model:
    a. Decide on the likelihood.
    b. Decide on the priors.
    c. Write the brms or Stan model.
2. Do prior predictive checks to determine if priors make sense.    
3. Check model using fake data simulations:
    a. Simulate data with known values for the parameters.
    b. Fit the model and do MCMC diagnostics.
    c. Verify that it recovers the parameters from simulated data.
4. Fit the model with real data and do MCMC diagnostics.
5. Evaluate the model's fit (e.g., posterior predictive checks, distribution of summary statistics). This may send you back to 1.
6. Inference/prediction/decisions.
7. Conduct model comparison if there's an alternative model (to be discussed later).





\newpage


\newpage

## Appendix - Troubleshooting problems of convergence \label{sec:Appendix}


1. Rhat > 1.1 First of all check that there are no mistakes in the model. Forgetting to put parenthesis, multiplying the wrong parameters, using the wrong operation, etc. can create a model that can't converge. As our models grow in complexity there are more places where to make mistakes. Start simple, see if the model works, add complexity slowly, checking if the model converges at every step. In very rare occasions, when Rhat >1.1 and the model is correct, it may help to increase the number of iterations, but then it's usually a better idea to re-parametrize the model, see 3.

2. Stan gives a warning. The solution may also be point 1. But if the model is correctly specified, you should check  Stan's website, there is a very good guide to solve problems in: http://mc-stan.org/misc/warnings.html. If this doesn't work, you may need to re-parametrize the model, see 3.

3. Some models have convergence issues because the sampler struggles to explore the parameters space. This is specially relevant in complex hierarchical models. In this case, the solution might be to re-parametrize the model. This is by no means trivial. However, the simplest parametrization trick to try is to have all the priors on the same rough scale, that is priors shouldn't have different orders of magnitude. You can find some suggestions in the chapter 21 of Stan manual [@Stan2017], and the following case study: http://mc-stan.org/users/documentation/case-studies/qr_regression.html.


<!-- https://cran.r-project.org/web/packages/bayesplot/vignettes/PPC.html -->


    

 